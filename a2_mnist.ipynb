{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f66a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import kornia\n",
    "import kornia.filters as kf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23502126",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PIN_MEMORY = True if DEVICE.type == \"cuda\" else False\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "print(f\"Running on: {DEVICE}\")\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "CHECKPOINT_DIR = \"./checkpoints_joint_5k\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ee866",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SIZE = 5000        \n",
    "COORD_ITERS = 20            \n",
    "PHI_EPOCHS_PER_ITER = 5    \n",
    "THETA_STEPS_PER_ITER = 50  \n",
    "\n",
    "PRETRAIN_EPOCHS = 3\n",
    "PRETRAIN_BATCH = 64\n",
    "\n",
    "HOAG_MAX_INNER_STEPS = 100\n",
    "INNER_LR = 0.02\n",
    "OUTER_LR = 0.01\n",
    "CG_MAX_ITERS = 50           \n",
    "CG_TOL = 1e-6\n",
    "\n",
    "THETA_INIT = torch.tensor([0.0, -3.0], dtype=torch.float32)\n",
    "BLUR_KERNEL_SIZE = (11, 11)\n",
    "BLUR_SIGMA = (3.0, 3.0)\n",
    "NOISE_STD = 0.12\n",
    "\n",
    "GRAD_CLIP_VALUE = 2.0\n",
    "THETA_CLAMP = (-12.0, 12.0)\n",
    "EPSILON_MIN = 1e-8\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "THETA_CLAMP_MIN = torch.tensor([-12.0, -12.0])\n",
    "THETA_CLAMP_MAX = torch.tensor([12.0, -2.0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eac197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_theta(theta):\n",
    "    return torch.max(torch.min(theta, THETA_CLAMP_MAX.to(theta.device)), THETA_CLAMP_MIN.to(theta.device))\n",
    "\n",
    "def ensure_bchw(x):\n",
    "    if isinstance(x, np.ndarray): x = torch.from_numpy(x)\n",
    "    if not torch.is_tensor(x): x = torch.as_tensor(x)\n",
    "    d = x.dim()\n",
    "    if d == 4: return x\n",
    "    if d == 3: return x.unsqueeze(0) if x.size(0) in [1,3] else x.unsqueeze(1)\n",
    "    if d == 2: return x.unsqueeze(0).unsqueeze(0)\n",
    "    raise ValueError(f\"Unsupported shape {x.shape}\")\n",
    "\n",
    "def clamp_theta(theta):\n",
    "    return torch.clamp(theta, THETA_CLAMP[0], THETA_CLAMP[1])\n",
    "\n",
    "raw_k = kornia.filters.get_gaussian_kernel2d(BLUR_KERNEL_SIZE, BLUR_SIGMA)\n",
    "kernel = torch.as_tensor(raw_k, dtype=torch.float32, device=DEVICE)\n",
    "while kernel.dim() > 2: kernel = kernel.squeeze(0)\n",
    "kernel = kernel / kernel.sum()\n",
    "BLUR_KERNEL = kernel.unsqueeze(0).unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedBlurredDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        d = torch.load(path, map_location='cpu')\n",
    "        self.y = d['y']\n",
    "        self.x = d.get('x', None)\n",
    "        self.labels = d['labels']\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, i): \n",
    "        return self.y[i], (self.x[i] if self.x is not None else torch.zeros_like(self.y[i])), self.labels[i]\n",
    "\n",
    "def precompute_blur_to_disk(sharp_dataset, out_path):\n",
    "    loader = DataLoader(sharp_dataset, batch_size=512, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    Ys, Xs, Ls = [], [], []\n",
    "    print(f\"Precomputing blur for {len(sharp_dataset)} samples -> {out_path}...\")\n",
    "    t0 = time.time()\n",
    "    for xb, labs in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        # y = A(x) + noise\n",
    "        yb = kf.gaussian_blur2d(xb, kernel_size=BLUR_KERNEL_SIZE, sigma=BLUR_SIGMA)\n",
    "        if NOISE_STD > 0: yb += torch.randn_like(yb) * NOISE_STD\n",
    "        Ys.append(yb.clamp(0,1).cpu())\n",
    "        Xs.append(xb.cpu())\n",
    "        Ls.append(labs)\n",
    "    torch.save({'y': torch.cat(Ys), 'x': torch.cat(Xs), 'labels': torch.cat(Ls)}, out_path)\n",
    "    print(f\"Done in {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 48, 3, padding=1), nn.BatchNorm2d(48), nn.ReLU(),\n",
    "            nn.Conv2d(48, 48, 3, padding=1), nn.BatchNorm2d(48), nn.ReLU(), nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(48, 96, 3, padding=1), nn.BatchNorm2d(96), nn.ReLU(),\n",
    "            nn.Conv2d(96, 96, 3, padding=1), nn.BatchNorm2d(96), nn.ReLU(), nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(96, 192, 3, padding=1), nn.BatchNorm2d(192), nn.ReLU(), nn.AdaptiveAvgPool2d((3,3))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(nn.Flatten(), nn.Linear(192*9, 256), nn.ReLU(), nn.Dropout(0.25), nn.Linear(256, n_classes))\n",
    "    def forward(self, x): return self.classifier(self.features(x))\n",
    "\n",
    "def total_variation_regularizer(x, theta):\n",
    "    x = ensure_bchw(x)\n",
    "    theta = clamp_theta(theta)\n",
    "    scale = torch.exp(theta[0])           \n",
    "    eps = torch.exp(theta[1])**2 + EPSILON_MIN  \n",
    "    grad = kornia.filters.spatial_gradient(x, mode='diff')\n",
    "    magsq = grad[:,:,0,:,:]**2 + grad[:,:,1,:,:]**2 + eps\n",
    "    return scale * torch.sum(torch.sqrt(magsq))\n",
    "\n",
    "def inner_loss_func(x, theta, fixed_params, y):\n",
    "    x = ensure_bchw(x)\n",
    "    y = ensure_bchw(y)\n",
    "    pad_h, pad_w = BLUR_KERNEL.shape[-2]//2, BLUR_KERNEL.shape[-1]//2\n",
    "    pred = F.conv2d(x, BLUR_KERNEL, padding=(pad_h, pad_w))\n",
    "    recon = 0.5 * torch.sum((y - pred)**2)\n",
    "    reg = total_variation_regularizer(x, theta)\n",
    "    return recon + reg\n",
    "\n",
    "def outer_loss_func(x_hat, fixed_params, label):\n",
    "    classifier_net = Classifier(10).to(DEVICE)\n",
    "    classifier_net.load_state_dict(fixed_params['phi'])\n",
    "    classifier_net.eval()\n",
    "    logits = classifier_net(x_hat.clamp(0,1))\n",
    "    return nn.CrossEntropyLoss()(logits, label.long().to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_vector_product(h_scalar, w, v):\n",
    "    grad_h = autograd.grad(h_scalar, w, create_graph=True, retain_graph=True)[0]\n",
    "    grad_h_v = torch.dot(grad_h.reshape(-1), v.reshape(-1))\n",
    "    return autograd.grad(grad_h_v, w, retain_graph=True)[0]\n",
    "\n",
    "def conjugate_gradient(A_func, b, max_iters=CG_MAX_ITERS, tol=CG_TOL):\n",
    "    x = torch.zeros_like(b)\n",
    "    r = b.clone(); p = r.clone()\n",
    "    rsold = torch.dot(r.view(-1), r.view(-1))\n",
    "    for i in range(max_iters):\n",
    "        if rsold < tol: break\n",
    "        Ap = A_func(p)\n",
    "        denom = torch.dot(p.view(-1), Ap.view(-1)) + 1e-12\n",
    "        alpha = rsold / denom\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        rsnew = torch.dot(r.view(-1), r.view(-1))\n",
    "        p = r + (rsnew/rsold) * p\n",
    "        rsold = rsnew\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d33316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE A\n",
    "def hoag_step_for_theta(theta, fixed_params, loader, steps=10):\n",
    "    theta = theta.detach().requires_grad_(True)\n",
    "    history = []\n",
    "    iterator = iter(loader)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        try:\n",
    "            batch = next(iterator)\n",
    "        except StopIteration:\n",
    "            iterator = iter(loader)\n",
    "            batch = next(iterator)\n",
    "            \n",
    "        y_cpu, _, label_cpu = batch\n",
    "        y = ensure_bchw(y_cpu).to(DEVICE)\n",
    "        label = label_cpu.to(DEVICE)\n",
    "        w = y.clone().detach().requires_grad_(True)\n",
    "        v_mom = torch.zeros_like(w)\n",
    "        beta = 0.9\n",
    "        \n",
    "        for _ in range(HOAG_MAX_INNER_STEPS):\n",
    "            loss = inner_loss_func(w, theta, fixed_params, y)\n",
    "            gw = autograd.grad(loss, w, create_graph=True)[0]\n",
    "            with torch.no_grad():\n",
    "                v_mom = beta * v_mom + (1-beta) * gw\n",
    "                w = w - INNER_LR * v_mom\n",
    "                w.clamp_(0, 1)\n",
    "            w.requires_grad_(True)\n",
    "            if torch.norm(gw) < 1e-4: break\n",
    "            \n",
    "        outer_loss = outer_loss_func(w, fixed_params, label)\n",
    "        b = autograd.grad(outer_loss, w, retain_graph=True)[0]\n",
    "        \n",
    "        # Inverse Hessian (v = H^-1 g) via CG\n",
    "        w_cg = w.detach().requires_grad_(True)\n",
    "        theta_cg = theta.detach()\n",
    "        def A_hvp(v):\n",
    "            h = inner_loss_func(w_cg, theta_cg, fixed_params, y)\n",
    "            return hessian_vector_product(h, w_cg, v)\n",
    "        \n",
    "        q = conjugate_gradient(A_hvp, b)\n",
    "        \n",
    "        # Hypergradient\n",
    "        w_clean = w.detach().requires_grad_(True)\n",
    "        theta_clean = theta.detach().requires_grad_(True)\n",
    "        scalar = inner_loss_func(w_clean, theta_clean, fixed_params, y)\n",
    "        gw_h = autograd.grad(scalar, w_clean, create_graph=True)[0]\n",
    "        \n",
    "        # Cross derivative vector product\n",
    "        cross = autograd.grad(gw_h, theta_clean, grad_outputs=q, allow_unused=True)[0]\n",
    "        \n",
    "        hypergrad = -cross if cross is not None else torch.zeros_like(theta)\n",
    "        \n",
    "        # Gradient Descent on Theta\n",
    "        with torch.no_grad():\n",
    "            theta = theta - OUTER_LR * hypergrad\n",
    "            theta = clamp_theta(theta)\n",
    "            theta.requires_grad_(True)\n",
    "            \n",
    "        history.append(outer_loss.item())\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"    [Theta-HOAG] Step {i+1}/{steps} | Loss: {outer_loss.item():.4f} | |Grad|: {torch.norm(hypergrad):.4e}\")\n",
    "\n",
    "    return theta.detach(), np.mean(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6995594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE B\n",
    "def update_phi_on_reconstructions(classifier, theta, loader, epochs=1):\n",
    "    print(f\"  [Phi-SGD] Training classifier on reconstructions...\")\n",
    "    classifier.train()\n",
    "    optimizer = torch.optim.AdamW(classifier.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    theta_fixed = theta.detach().to(DEVICE)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        count = 0\n",
    "        for y_cpu, _, labels in loader:\n",
    "            y = ensure_bchw(y_cpu).to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            w_hat = y.clone().detach().requires_grad_(True)\n",
    "            for _ in range(50): \n",
    "                loss = inner_loss_func(w_hat, theta_fixed, None, y)\n",
    "                gw = autograd.grad(loss, w_hat, create_graph=False)[0]\n",
    "                with torch.no_grad():\n",
    "                    w_hat = w_hat - 0.02 * gw\n",
    "                    w_hat.clamp_(0, 1)\n",
    "                w_hat.requires_grad_(True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = classifier(w_hat)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item() * y.size(0)\n",
    "            count += y.size(0)\n",
    "            \n",
    "        print(f\"    [Phi-SGD] Epoch {epoch+1} Avg Loss: {avg_loss/count:.4f}\")\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(theta, classifier, test_loader):\n",
    "    classifier.eval()\n",
    "    theta_eval = clamp_theta(theta)\n",
    "    correct = 0; total = 0\n",
    "    \n",
    "    # Process in chunks\n",
    "    for y_cpu, _, labels in test_loader:\n",
    "        y = ensure_bchw(y_cpu).to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        # High quality reconstruction for evaluation (100 steps)\n",
    "        w_hat = y.clone().detach().requires_grad_(True)\n",
    "        for _ in range(100):\n",
    "            loss = inner_loss_func(w_hat, theta_eval, None, y)\n",
    "            gw = autograd.grad(loss, w_hat, create_graph=False)[0]\n",
    "            with torch.no_grad():\n",
    "                w_hat = (w_hat - 0.01 * gw).clamp(0,1)\n",
    "            w_hat.requires_grad_(True)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            preds = classifier(w_hat).argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2327c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51545e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "\n",
      "=== Starting Approach 2: Joint Optimization (Subset: 5000) ===\n",
      "Selecting 5000 random samples from training set...\n",
      "Found cached training data: ./train_blur_5000.pt\n",
      "Found cached test data: ./test_blur_full.pt\n",
      "Pretraining Classifier on raw blurred images (warmup)...\n",
      "\n",
      ">>> CYCLE 1 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0002 | |Grad|: 4.0098e-04\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 16.5209 | |Grad|: 8.6301e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 8.0150 | |Grad|: 1.5566e+01\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0027 | |Grad|: 1.0438e-02\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0483 | |Grad|: 3.5922e-01\n",
      "   => New Theta: [-1.7385604 -1.4856956]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.3245\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.2818\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.2539\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.2426\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.2382\n",
      "  [Result] Cycle 1 Test Acc: 91.26%\n",
      "   * New Best Model Saved *\n",
      "\n",
      ">>> CYCLE 2 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0638 | |Grad|: 3.3646e-01\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0006 | |Grad|: 2.6683e-03\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.5364 | |Grad|: 4.6326e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0614 | |Grad|: 2.4743e-01\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0011 | |Grad|: 8.6355e-03\n",
      "   => New Theta: [-1.8752279 -1.3624604]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.2211\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.2070\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.1905\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.1692\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.1572\n",
      "  [Result] Cycle 2 Test Acc: 91.25%\n",
      "\n",
      ">>> CYCLE 3 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0006 | |Grad|: 9.3282e-04\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0124 | |Grad|: 2.5956e-02\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0302 | |Grad|: 3.7264e-03\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0934 | |Grad|: 8.6364e-02\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0138 | |Grad|: 1.5869e-01\n",
      "   => New Theta: [-2.0484617 -1.2060368]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.1537\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.1389\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.1127\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.1037\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0950\n",
      "  [Result] Cycle 3 Test Acc: 91.17%\n",
      "\n",
      ">>> CYCLE 4 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0001 | |Grad|: 9.9890e-04\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0003 | |Grad|: 1.7557e-03\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.1183 | |Grad|: 4.8857e-01\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0001 | |Grad|: 3.0062e-04\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0008 | |Grad|: 1.2199e-03\n",
      "   => New Theta: [-2.2345898 -1.0365489]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0961\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0826\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0598\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0728\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0677\n",
      "  [Result] Cycle 4 Test Acc: 90.66%\n",
      "\n",
      ">>> CYCLE 5 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0001 | |Grad|: 9.4261e-05\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.1730 | |Grad|: 6.3968e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.1299 | |Grad|: 2.3759e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0001 | |Grad|: 2.4645e-03\n",
      "   => New Theta: [-2.391517  -0.8931573]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0519\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0386\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0480\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0277\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0490\n",
      "  [Result] Cycle 5 Test Acc: 90.22%\n",
      "\n",
      ">>> CYCLE 6 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0004 | |Grad|: 8.2709e-03\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0017 | |Grad|: 2.2770e-02\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0001 | |Grad|: 4.5640e-04\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0006 | |Grad|: 2.2299e-02\n",
      "   => New Theta: [-2.420679  -0.8668496]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0300\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0206\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0363\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0141\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0163\n",
      "  [Result] Cycle 6 Test Acc: 90.39%\n",
      "\n",
      ">>> CYCLE 7 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0016 | |Grad|: 2.7434e-02\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0011 | |Grad|: 8.2666e-03\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 4.9178e-04\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0033 | |Grad|: 7.3598e-02\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.479284 -0.811813]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0198\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0141\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0151\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0473\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0099\n",
      "  [Result] Cycle 7 Test Acc: 89.73%\n",
      "\n",
      ">>> CYCLE 8 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0008 | |Grad|: 2.0961e-03\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0003 | |Grad|: 7.9315e-04\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0002 | |Grad|: 3.4139e-03\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0003 | |Grad|: 8.7865e-03\n",
      "   => New Theta: [-2.4997644  -0.79327905]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0165\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0091\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0087\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0141\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0338\n",
      "  [Result] Cycle 8 Test Acc: 90.68%\n",
      "\n",
      ">>> CYCLE 9 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0121 | |Grad|: 4.7641e-02\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0034 | |Grad|: 1.0344e-01\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0022 | |Grad|: 1.8497e-02\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.5188315 -0.7753348]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0153\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0079\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0140\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0039\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0022\n",
      "  [Result] Cycle 9 Test Acc: 90.52%\n",
      "\n",
      ">>> CYCLE 10 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0001 | |Grad|: 2.6470e-03\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 6.7755e-04\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.519455  -0.7747688]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0084\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0057\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0218\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0045\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0043\n",
      "  [Result] Cycle 10 Test Acc: 90.33%\n",
      "\n",
      ">>> CYCLE 11 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0008 | |Grad|: 2.3130e-02\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 7.5761e-05\n",
      "   => New Theta: [-2.529081   -0.76598847]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0125\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0074\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0049\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0199\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0122\n",
      "  [Result] Cycle 11 Test Acc: 90.38%\n",
      "\n",
      ">>> CYCLE 12 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0001 | |Grad|: 2.8732e-03\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.5772176 -0.7203014]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0078\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0132\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0137\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0152\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0137\n",
      "  [Result] Cycle 12 Test Acc: 89.97%\n",
      "\n",
      ">>> CYCLE 13 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.6394477  -0.66394866]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0144\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0037\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0033\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0069\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0118\n",
      "  [Result] Cycle 13 Test Acc: 89.16%\n",
      "\n",
      ">>> CYCLE 14 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0002 | |Grad|: 1.2314e-03\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.6634123 -0.6417451]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0153\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0038\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0111\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0366\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0036\n",
      "  [Result] Cycle 14 Test Acc: 90.57%\n",
      "\n",
      ">>> CYCLE 15 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.663422 -0.641736]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0124\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0115\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0251\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0115\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0018\n",
      "  [Result] Cycle 15 Test Acc: 90.82%\n",
      "\n",
      ">>> CYCLE 16 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.664387   -0.64081717]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0094\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0108\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0048\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0007\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0033\n",
      "  [Result] Cycle 16 Test Acc: 89.88%\n",
      "\n",
      ">>> CYCLE 17 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 5.9539e-04\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0014 | |Grad|: 2.0213e-02\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.6326373  -0.67053586]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0056\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0116\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0114\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0044\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0070\n",
      "  [Result] Cycle 17 Test Acc: 89.98%\n",
      "\n",
      ">>> CYCLE 18 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0001 | |Grad|: 5.6604e-04\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.641888  -0.6617982]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0086\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0067\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0039\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0353\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0036\n",
      "  [Result] Cycle 18 Test Acc: 90.52%\n",
      "\n",
      ">>> CYCLE 19 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 6.7064e-04\n",
      "   => New Theta: [-2.6442635  -0.65948725]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0064\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0081\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0088\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0022\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0008\n",
      "  [Result] Cycle 19 Test Acc: 90.80%\n",
      "\n",
      ">>> CYCLE 20 / 20 <<<\n",
      "  [Phase A] Optimizing Reconstruction Parameters (Theta)...\n",
      "    [Theta-HOAG] Step 10/50 | Loss: 0.0001 | |Grad|: 1.4166e-04\n",
      "    [Theta-HOAG] Step 20/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 30/50 | Loss: 0.0001 | |Grad|: 6.4824e-04\n",
      "    [Theta-HOAG] Step 40/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "    [Theta-HOAG] Step 50/50 | Loss: 0.0000 | |Grad|: 0.0000e+00\n",
      "   => New Theta: [-2.6444237 -0.6593374]\n",
      "  [Phase B] Adapting Classifier to New Reconstructions...\n",
      "  [Phi-SGD] Training classifier on reconstructions...\n",
      "    [Phi-SGD] Epoch 1 Avg Loss: 0.0076\n",
      "    [Phi-SGD] Epoch 2 Avg Loss: 0.0059\n",
      "    [Phi-SGD] Epoch 3 Avg Loss: 0.0072\n",
      "    [Phi-SGD] Epoch 4 Avg Loss: 0.0124\n",
      "    [Phi-SGD] Epoch 5 Avg Loss: 0.0012\n",
      "  [Result] Cycle 20 Test Acc: 90.80%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(f\"\\n (Subset: {SUBSET_SIZE})\")\n",
    "    \n",
    "    train_sharp = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transforms.ToTensor())\n",
    "    test_sharp = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor())\n",
    "    \n",
    "    indices = torch.randperm(len(train_sharp))[:SUBSET_SIZE]\n",
    "    train_subset = Subset(train_sharp, indices)\n",
    "\n",
    "    train_blur_path = f\"./train_blur_{SUBSET_SIZE}.pt\"\n",
    "    test_blur_path = \"./test_blur_full.pt\"\n",
    "    \n",
    "    if not os.path.exists(train_blur_path):\n",
    "        precompute_blur_to_disk(train_subset, train_blur_path)\n",
    "    else:\n",
    "        print(f\"Found cached training data: {train_blur_path}\")\n",
    "        \n",
    "    if not os.path.exists(test_blur_path):\n",
    "        precompute_blur_to_disk(test_sharp, test_blur_path)\n",
    "    else:\n",
    "        print(f\"Found cached test data: {test_blur_path}\")\n",
    "        \n",
    "    train_ds = PrecomputedBlurredDataset(train_blur_path)\n",
    "    test_ds = PrecomputedBlurredDataset(test_blur_path)\n",
    "    \n",
    "\n",
    "    theta_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0)\n",
    "    phi_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=100, shuffle=False)\n",
    "\n",
    "    classifier = Classifier(10).to(DEVICE)\n",
    "    theta = THETA_INIT.clone().to(DEVICE).requires_grad_(True)\n",
    "    \n",
    "    optim = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "    for ep in range(PRETRAIN_EPOCHS):\n",
    "        for y, _, l in phi_loader:\n",
    "            y, l = y.to(DEVICE), l.to(DEVICE)\n",
    "            loss = nn.CrossEntropyLoss()(classifier(y), l)\n",
    "            optim.zero_grad(); loss.backward(); optim.step()\n",
    "    \n",
    "    # Coordinate Descent Loop\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for cycle in range(COORD_ITERS):\n",
    "        print(f\"\\n>>> CYCLE {cycle+1} / {COORD_ITERS} <<<\")\n",
    "        \n",
    "        # PHASE A: Update Regularizer (Theta)\n",
    "        #print(\"  [Phase A] Optimizing Reconstruction Parameters (Theta)...\")\n",
    "        fixed_phi = {'phi': {k: v.cpu().clone() for k, v in classifier.state_dict().items()}}\n",
    "        theta, _ = hoag_step_for_theta(theta, fixed_phi, theta_loader, steps=THETA_STEPS_PER_ITER)\n",
    "        print(f\"   => New Theta: {theta.cpu().numpy()}\")\n",
    "\n",
    "        # PHASE B: Update Classifier (Phi)\n",
    "        #print(\"  [Phase B] Adapting Classifier to New Reconstructions...\")\n",
    "        classifier = update_phi_on_reconstructions(classifier, theta, phi_loader, epochs=PHI_EPOCHS_PER_ITER)\n",
    "        \n",
    "        # Eval\n",
    "        acc = evaluate_pipeline(theta, classifier, test_loader)\n",
    "        print(f\"  [Result] Cycle {cycle+1} Test Acc: {acc*100:.2f}%\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save({'theta': theta, 'phi': classifier.state_dict()}, os.path.join(CHECKPOINT_DIR, \"best_model.pt\"))\n",
    "            print(\"   * New Best Model Saved *\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0463a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
