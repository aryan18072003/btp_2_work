{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting HOAG Optimization ---\n",
      "Iteration 01: Hyperparameter (lambda) = 9.9909, Validation Loss = 6.0137\n",
      "Iteration 02: Hyperparameter (lambda) = 9.9818, Validation Loss = 6.0129\n",
      "Iteration 03: Hyperparameter (lambda) = 9.9727, Validation Loss = 6.0121\n",
      "Iteration 04: Hyperparameter (lambda) = 9.9635, Validation Loss = 6.0113\n",
      "Iteration 05: Hyperparameter (lambda) = 9.9544, Validation Loss = 6.0104\n",
      "Iteration 06: Hyperparameter (lambda) = 9.9452, Validation Loss = 6.0096\n",
      "Iteration 07: Hyperparameter (lambda) = 9.9360, Validation Loss = 6.0087\n",
      "Iteration 08: Hyperparameter (lambda) = 9.9268, Validation Loss = 6.0079\n",
      "Iteration 09: Hyperparameter (lambda) = 9.9176, Validation Loss = 6.0071\n",
      "Iteration 10: Hyperparameter (lambda) = 9.9084, Validation Loss = 6.0062\n",
      "Iteration 11: Hyperparameter (lambda) = 9.8991, Validation Loss = 6.0054\n",
      "Iteration 12: Hyperparameter (lambda) = 9.8899, Validation Loss = 6.0045\n",
      "Iteration 13: Hyperparameter (lambda) = 9.8806, Validation Loss = 6.0036\n",
      "Iteration 14: Hyperparameter (lambda) = 9.8713, Validation Loss = 6.0028\n",
      "Iteration 15: Hyperparameter (lambda) = 9.8620, Validation Loss = 6.0019\n",
      "Iteration 16: Hyperparameter (lambda) = 9.8527, Validation Loss = 6.0010\n",
      "Iteration 17: Hyperparameter (lambda) = 9.8433, Validation Loss = 6.0002\n",
      "Iteration 18: Hyperparameter (lambda) = 9.8340, Validation Loss = 5.9993\n",
      "Iteration 19: Hyperparameter (lambda) = 9.8246, Validation Loss = 5.9984\n",
      "Iteration 20: Hyperparameter (lambda) = 9.8152, Validation Loss = 5.9976\n",
      "\n",
      "--- Final Result ---\n",
      "Optimal hyperparameter found: lambda = 9.8152\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "def hessian_vector_product(h, w, v):\n",
    "    grad_h = autograd.grad(h, w, create_graph=True, retain_graph=True)[0]\n",
    "    grad_h_v = torch.dot(grad_h, v)\n",
    "    hvp = autograd.grad(grad_h_v, w, retain_graph=True)[0]\n",
    "    return hvp\n",
    "\n",
    "def conjugate_gradient(A_hvp_func, b, n_iter=10, epsilon=1e-8):\n",
    "    x = torch.zeros_like(b)\n",
    "    r = b.clone()\n",
    "    p = r.clone()\n",
    "    rs_old = torch.dot(r, r)\n",
    "    for _ in range(n_iter):\n",
    "        Ap = A_hvp_func(p)\n",
    "        alpha = rs_old / torch.dot(p, Ap)\n",
    "        x += alpha * p\n",
    "        r -= alpha * Ap\n",
    "        rs_new = torch.dot(r, r)\n",
    "        if torch.sqrt(rs_new) < epsilon:\n",
    "            break\n",
    "        p = r + (rs_new / rs_old) * p\n",
    "        rs_old = rs_new\n",
    "    return x\n",
    "\n",
    "#hoag implementation\n",
    "def hoag_optimize(\n",
    "    inner_loss_func,      \n",
    "    outer_loss_func,      \n",
    "    w,                    \n",
    "    lambda_,              \n",
    "    train_data,           \n",
    "    val_data,             \n",
    "    n_outer_steps=20,     \n",
    "    inner_lr=0.01,        \n",
    "    outer_lr=0.1,\n",
    "    max_inner_steps=50,\n",
    "    epsilon=1e-5\n",
    "):\n",
    "    history = []\n",
    "\n",
    "    print(\"--- Starting HOAG Optimization ---\")\n",
    "    for k in range(n_outer_steps):\n",
    "        \n",
    "        #Step (i): Solve Inner Problem\n",
    "        for _ in range(max_inner_steps):\n",
    "            inner_loss = inner_loss_func(w, lambda_, train_data)\n",
    "            grad_w_inner = autograd.grad(inner_loss, w, create_graph=True)[0]\n",
    "            if torch.norm(grad_w_inner) < epsilon:\n",
    "                break\n",
    "            w = w - inner_lr * grad_w_inner\n",
    "\n",
    "        #Step (ii): Solve Linear System\n",
    "        outer_loss = outer_loss_func(w, val_data)\n",
    "        b = autograd.grad(outer_loss, w, retain_graph=True)[0]\n",
    "        \n",
    "        w_cg = w.detach().requires_grad_()\n",
    "        lambda_cg = lambda_.detach()\n",
    "        def hvp_func(v):\n",
    "            h = inner_loss_func(w_cg, lambda_cg, train_data)\n",
    "            return hessian_vector_product(h, w_cg, v)\n",
    "        q_k = conjugate_gradient(hvp_func, b, epsilon=epsilon)\n",
    "\n",
    "        #Step (iii): Compute Approximate Gradient\n",
    "        w_clean = w.detach().requires_grad_()\n",
    "        lambda_clean = lambda_.detach().requires_grad_()\n",
    "\n",
    "        inner_loss_grads = inner_loss_func(w_clean, lambda_clean, train_data)\n",
    "        outer_loss_grads = outer_loss_func(w_clean, val_data)\n",
    "\n",
    "        # First term: d g/dλ\n",
    "        grad_g_lambda = autograd.grad(outer_loss_grads, lambda_clean, allow_unused=True)[0]\n",
    "        if grad_g_lambda is None: grad_g_lambda = 0.0\n",
    "            \n",
    "        # Second term: (d^2 h / (dλ dw))^T * q_k\n",
    "        grad_h_lambda = autograd.grad(inner_loss_grads, lambda_clean, create_graph=True)[0]\n",
    "        cross_gradient_term = autograd.grad(grad_h_lambda, w_clean, retain_graph=True)[0]\n",
    "        \n",
    "        p_k = grad_g_lambda - torch.dot(cross_gradient_term, q_k)\n",
    "\n",
    "        #Step (iv): Update Hyperparameter\n",
    "        lambda_ = lambda_ - outer_lr * p_k\n",
    "        lambda_ = torch.clamp(lambda_, min=0.0)\n",
    "\n",
    "        w = w.detach().requires_grad_()\n",
    "        lambda_ = lambda_.detach().requires_grad_()\n",
    "        \n",
    "        current_val_loss = outer_loss_func(w, val_data).item()\n",
    "        history.append({'lambda': lambda_.item(), 'val_loss': current_val_loss})\n",
    "        print(f\"Iteration {k+1:02d}: Hyperparameter (lambda) = {lambda_.item():.4f}, Validation Loss = {current_val_loss:.4f}\")\n",
    "        \n",
    "    print(f\"\\n--- Final Result ---\\nOptimal hyperparameter found: lambda = {lambda_.item():.4f}\")\n",
    "    \n",
    "    return lambda_, w, history\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     def ridge_inner_loss(w, lambda_, data):\n",
    "#         X, y = data\n",
    "#         return torch.mean((X @ w - y)**2) + lambda_ * torch.sum(w**2)\n",
    "\n",
    "#     def ridge_outer_loss(w, data):\n",
    "#         X, y = data\n",
    "#         return torch.mean((X @ w - y)**2)\n",
    "\n",
    "#     n_features, n_samples = 10, 100\n",
    "#     X_train = torch.randn(n_samples, n_features)\n",
    "#     w_true = torch.randn(n_features)\n",
    "#     y_train = X_train @ w_true + torch.randn(n_samples) * 0.1\n",
    "#     X_val = torch.randn(n_samples, n_features)\n",
    "#     y_val = X_val @ w_true + torch.randn(n_samples) * 0.1\n",
    "\n",
    "#     w_init = torch.randn(n_features, requires_grad=True)\n",
    "#     lambda_init = torch.tensor(10.0, requires_grad=True)\n",
    "\n",
    "#     final_lambda, final_w, history = hoag_optimize(\n",
    "#         inner_loss_func=ridge_inner_loss,\n",
    "#         outer_loss_func=ridge_outer_loss,\n",
    "#         w=w_init,\n",
    "#         lambda_=lambda_init,\n",
    "#         train_data=(X_train, y_train),\n",
    "#         val_data=(X_val, y_val),\n",
    "#         max_inner_steps=50,\n",
    "#         epsilon=1e-4\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7640c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63453491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb343c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553eb7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
