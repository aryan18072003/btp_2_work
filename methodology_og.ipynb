{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46b333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "BLUR_KERNEL shape: torch.Size([1, 1, 7, 7])\n",
      "[Pretrain] Epoch 1/15 avg_loss=2.28707 lr=0.00100\n",
      "[Pretrain] Epoch 2/15 avg_loss=2.21096 lr=0.00100\n",
      "[Pretrain] Epoch 3/15 avg_loss=2.09962 lr=0.00100\n",
      "[Pretrain] Epoch 4/15 avg_loss=1.92443 lr=0.00100\n",
      "[Pretrain] Epoch 5/15 avg_loss=1.65605 lr=0.00100\n",
      "[Pretrain] Epoch 6/15 avg_loss=1.30965 lr=0.00050\n",
      "[Pretrain] Epoch 7/15 avg_loss=1.02212 lr=0.00050\n",
      "[Pretrain] Epoch 8/15 avg_loss=0.86783 lr=0.00050\n",
      "[Pretrain] Epoch 9/15 avg_loss=0.75883 lr=0.00050\n",
      "[Pretrain] Epoch 10/15 avg_loss=0.65782 lr=0.00050\n",
      "[Pretrain] Epoch 11/15 avg_loss=0.58664 lr=0.00050\n",
      "[Pretrain] Epoch 12/15 avg_loss=0.53293 lr=0.00025\n",
      "[Pretrain] Epoch 13/15 avg_loss=0.48449 lr=0.00025\n",
      "[Pretrain] Epoch 14/15 avg_loss=0.44795 lr=0.00025\n",
      "[Pretrain] Epoch 15/15 avg_loss=0.43420 lr=0.00025\n",
      "Initial theta: [ 0. -3.]\n",
      "=== HOAG epoch 1/2 ===\n",
      "\n",
      "[HOAG] Sample 1: label=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukta\\AppData\\Local\\Temp\\ipykernel_19036\\4281910160.py:438: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"\\n[HOAG] Sample {sample_count}: label={int(label.cpu().numpy())}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HOAG] theta now: [-0.08821027 -2.9546678 ]\n",
      "\n",
      "[HOAG] Sample 2: label=7\n",
      "[HOAG] theta now: [-0.23573846 -2.8987136 ]\n",
      "\n",
      "[HOAG] Sample 3: label=3\n",
      "[HOAG] theta now: [-0.24542657 -2.891942  ]\n",
      "\n",
      "[HOAG] Sample 4: label=8\n",
      "[HOAG] theta now: [-0.31893522 -2.863602  ]\n",
      "\n",
      "[HOAG] Sample 5: label=6\n",
      "[HOAG] theta now: [-0.35815194 -2.8290792 ]\n",
      "\n",
      "[HOAG] Sample 6: label=3\n",
      "[HOAG] theta now: [-0.36984995 -2.822663  ]\n",
      "\n",
      "[HOAG] Sample 7: label=7\n",
      "[HOAG] theta now: [-0.46575233 -2.7808633 ]\n",
      "\n",
      "[HOAG] Sample 8: label=7\n",
      "[HOAG] theta now: [-0.569393  -2.7310307]\n",
      "\n",
      "[HOAG] Sample 9: label=5\n",
      "[HOAG] theta now: [-0.5811418 -2.7212892]\n",
      "\n",
      "[HOAG] Sample 10: label=2\n",
      "[HOAG] theta now: [-0.57884216 -2.7255516 ]\n",
      "\n",
      "[HOAG] Sample 11: label=3\n",
      "[HOAG] theta now: [-0.63306695 -2.6921232 ]\n",
      "\n",
      "[HOAG] Sample 12: label=7\n",
      "[HOAG] theta now: [-0.7448721 -2.627145 ]\n",
      "\n",
      "[HOAG] Sample 13: label=0\n",
      "[HOAG] theta now: [-0.85722387 -2.5895827 ]\n",
      "\n",
      "[HOAG] Sample 14: label=1\n",
      "[HOAG] theta now: [-0.96172523 -2.5281978 ]\n",
      "\n",
      "[HOAG] Sample 15: label=6\n",
      "[HOAG] theta now: [-1.0162833 -2.4965854]\n",
      "\n",
      "[HOAG] Sample 16: label=2\n",
      "[HOAG] theta now: [-1.052869  -2.4781315]\n",
      "\n",
      "[HOAG] Sample 17: label=4\n",
      "[HOAG] theta now: [-1.1037576 -2.4584186]\n",
      "\n",
      "[HOAG] Sample 18: label=3\n",
      "[HOAG] theta now: [-1.1495227 -2.4356873]\n",
      "\n",
      "[HOAG] Sample 19: label=4\n",
      "[HOAG] theta now: [-1.1801987 -2.4220195]\n",
      "\n",
      "[HOAG] Sample 20: label=6\n",
      "[HOAG] theta now: [-1.2353462 -2.397305 ]\n",
      "\n",
      "[HOAG] Sample 21: label=6\n",
      "[HOAG] theta now: [-1.2933013 -2.3543925]\n",
      "\n",
      "[HOAG] Sample 22: label=1\n",
      "[HOAG] theta now: [-1.4196885 -2.3027022]\n",
      "\n",
      "[HOAG] Sample 23: label=3\n",
      "[HOAG] theta now: [-1.4508431 -2.2766485]\n",
      "\n",
      "[HOAG] Sample 24: label=2\n",
      "[HOAG] theta now: [-1.4696149 -2.2677696]\n",
      "\n",
      "[HOAG] Sample 25: label=1\n",
      "[HOAG] theta now: [-1.5840163 -2.216873 ]\n",
      "\n",
      "[HOAG] Sample 26: label=1\n",
      "[HOAG] theta now: [-1.6934433 -2.1614802]\n",
      "\n",
      "[HOAG] Sample 27: label=8\n",
      "[HOAG] theta now: [-1.7032454 -2.1589172]\n",
      "\n",
      "[HOAG] Sample 28: label=0\n",
      "[HOAG] theta now: [-1.7533896 -2.136838 ]\n",
      "\n",
      "[HOAG] Sample 29: label=9\n",
      "[HOAG] theta now: [-1.77769   -2.1262455]\n",
      "\n",
      "[HOAG] Sample 30: label=7\n",
      "[HOAG] theta now: [-1.8263998 -2.0961976]\n",
      "\n",
      "[HOAG] Sample 31: label=2\n",
      "[HOAG] theta now: [-1.8446195 -2.086427 ]\n",
      "\n",
      "[HOAG] Sample 32: label=0\n",
      "[HOAG] theta now: [-1.8678833 -2.078048 ]\n",
      "\n",
      "[HOAG] Sample 33: label=1\n",
      "[HOAG] theta now: [-1.9584743 -2.042753 ]\n",
      "\n",
      "[HOAG] Sample 34: label=3\n",
      "[HOAG] theta now: [-1.9759003 -2.034275 ]\n",
      "\n",
      "[HOAG] Sample 35: label=4\n",
      "[HOAG] theta now: [-2.0045986 -2.0204325]\n",
      "\n",
      "[HOAG] Sample 36: label=3\n",
      "[HOAG] theta now: [-2.0266776 -2.0083086]\n",
      "\n",
      "[HOAG] Sample 37: label=1\n",
      "[HOAG] theta now: [-2.1053426 -1.9822564]\n",
      "\n",
      "[HOAG] Sample 38: label=5\n",
      "[HOAG] theta now: [-2.1262903 -1.9738954]\n",
      "\n",
      "[HOAG] Sample 39: label=7\n",
      "[HOAG] theta now: [-2.162178  -1.9549257]\n",
      "\n",
      "[HOAG] Sample 40: label=4\n",
      "[HOAG] theta now: [-2.184114 -1.933129]\n",
      "\n",
      "[HOAG] Sample 41: label=9\n",
      "[HOAG] theta now: [-2.2021313 -1.9216273]\n",
      "\n",
      "[HOAG] Sample 42: label=3\n",
      "[HOAG] theta now: [-2.1934717 -1.9270344]\n",
      "\n",
      "[HOAG] Sample 43: label=0\n",
      "[HOAG] theta now: [-2.228578  -1.8987594]\n",
      "\n",
      "[HOAG] Sample 44: label=2\n",
      "[HOAG] theta now: [-2.2187347 -1.9010165]\n",
      "\n",
      "[HOAG] Sample 45: label=1\n",
      "[HOAG] theta now: [-2.2581053 -1.8896277]\n",
      "\n",
      "[HOAG] Sample 46: label=4\n",
      "[HOAG] theta now: [-2.2668118 -1.8871945]\n",
      "\n",
      "[HOAG] Sample 47: label=2\n",
      "[HOAG] theta now: [-2.2672048 -1.8870316]\n",
      "\n",
      "[HOAG] Sample 48: label=4\n",
      "[HOAG] theta now: [-2.2832284 -1.8775275]\n",
      "\n",
      "[HOAG] Sample 49: label=4\n",
      "[HOAG] theta now: [-2.2994256 -1.8709327]\n",
      "\n",
      "[HOAG] Sample 50: label=1\n",
      "[HOAG] theta now: [-2.3532727 -1.8402289]\n",
      "\n",
      "[HOAG] Sample 51: label=7\n",
      "[HOAG] theta now: [-2.356275  -1.8321916]\n",
      "\n",
      "[HOAG] Sample 52: label=7\n",
      "[HOAG] theta now: [-2.3725262 -1.8255109]\n",
      "\n",
      "[HOAG] Sample 53: label=7\n",
      "[HOAG] theta now: [-2.4041579 -1.8111719]\n",
      "\n",
      "[HOAG] Sample 54: label=9\n",
      "[HOAG] theta now: [-2.424929  -1.8001933]\n",
      "\n",
      "[HOAG] Sample 55: label=8\n",
      "[HOAG] theta now: [-2.4480731 -1.7877545]\n",
      "\n",
      "[HOAG] Sample 56: label=4\n",
      "[HOAG] theta now: [-2.4515493 -1.7851663]\n",
      "\n",
      "[HOAG] Sample 57: label=8\n",
      "[HOAG] theta now: [-2.467044  -1.7773366]\n",
      "\n",
      "[HOAG] Sample 58: label=4\n",
      "[HOAG] theta now: [-2.4876173 -1.7639662]\n",
      "\n",
      "[HOAG] Sample 59: label=1\n",
      "[HOAG] theta now: [-2.545268  -1.7303149]\n",
      "\n",
      "[HOAG] Sample 60: label=6\n",
      "[HOAG] theta now: [-2.552548 -1.729144]\n",
      "\n",
      "[HOAG] Sample 61: label=8\n",
      "[HOAG] theta now: [-2.5698605 -1.7171414]\n",
      "\n",
      "[HOAG] Sample 62: label=9\n",
      "[HOAG] theta now: [-2.6036184 -1.699511 ]\n",
      "\n",
      "[HOAG] Sample 63: label=8\n",
      "[HOAG] theta now: [-2.6104484 -1.6958811]\n",
      "\n",
      "[HOAG] Sample 64: label=9\n",
      "[HOAG] theta now: [-2.6337757 -1.681257 ]\n",
      "\n",
      "[HOAG] Sample 65: label=6\n",
      "[HOAG] theta now: [-2.6542478 -1.6709943]\n",
      "\n",
      "[HOAG] Sample 66: label=8\n",
      "[HOAG] theta now: [-2.6657786 -1.6664897]\n",
      "\n",
      "[HOAG] Sample 67: label=2\n",
      "[HOAG] theta now: [-2.667639  -1.6655908]\n",
      "\n",
      "[HOAG] Sample 68: label=3\n",
      "[HOAG] theta now: [-2.6632962 -1.6701659]\n",
      "\n",
      "[HOAG] Sample 69: label=7\n",
      "[HOAG] theta now: [-2.6570156 -1.6661143]\n",
      "\n",
      "[HOAG] Sample 70: label=0\n",
      "[HOAG] theta now: [-2.680567  -1.6511369]\n",
      "\n",
      "[HOAG] Sample 71: label=7\n",
      "[HOAG] theta now: [-2.6970356 -1.6436192]\n",
      "\n",
      "[HOAG] Sample 72: label=4\n",
      "[HOAG] theta now: [-2.698599  -1.6429845]\n",
      "\n",
      "[HOAG] Sample 73: label=3\n",
      "[HOAG] theta now: [-2.7071762 -1.6384416]\n",
      "\n",
      "[HOAG] Sample 74: label=3\n",
      "[HOAG] theta now: [-2.7245672 -1.6397812]\n",
      "\n",
      "[HOAG] Sample 75: label=4\n",
      "[HOAG] theta now: [-2.743713  -1.6331617]\n",
      "\n",
      "[HOAG] Sample 76: label=7\n",
      "[HOAG] theta now: [-2.7483423 -1.6313542]\n",
      "\n",
      "[HOAG] Sample 77: label=1\n",
      "[HOAG] theta now: [-2.788749  -1.6114075]\n",
      "\n",
      "[HOAG] Sample 78: label=8\n",
      "[HOAG] theta now: [-2.806165  -1.6010926]\n",
      "\n",
      "[HOAG] Sample 79: label=3\n",
      "[HOAG] theta now: [-2.8115833 -1.598071 ]\n",
      "\n",
      "[HOAG] Sample 80: label=9\n",
      "[HOAG] theta now: [-2.8467453 -1.5796273]\n",
      "\n",
      "[HOAG] Sample 81: label=0\n",
      "[HOAG] theta now: [-2.8607445 -1.5727671]\n",
      "\n",
      "[HOAG] Sample 82: label=8\n",
      "[HOAG] theta now: [-2.875214  -1.5638223]\n",
      "\n",
      "[HOAG] Sample 83: label=2\n",
      "[HOAG] theta now: [-2.8762145 -1.5632635]\n",
      "\n",
      "[HOAG] Sample 84: label=0\n",
      "[HOAG] theta now: [-2.900186  -1.5511996]\n",
      "\n",
      "[HOAG] Sample 85: label=7\n",
      "[HOAG] theta now: [-2.9206796 -1.5423841]\n",
      "\n",
      "[HOAG] Sample 86: label=5\n",
      "[HOAG] theta now: [-2.9317327 -1.5352854]\n",
      "\n",
      "[HOAG] Sample 87: label=0\n",
      "[HOAG] theta now: [-2.9419446 -1.5302461]\n",
      "\n",
      "[HOAG] Sample 88: label=2\n",
      "[HOAG] theta now: [-2.9589913 -1.5238148]\n",
      "\n",
      "[HOAG] Sample 89: label=2\n",
      "[HOAG] theta now: [-2.9602866 -1.5227622]\n",
      "\n",
      "[HOAG] Sample 90: label=4\n",
      "[HOAG] theta now: [-2.9667192 -1.5199517]\n",
      "\n",
      "[HOAG] Sample 91: label=4\n",
      "[HOAG] theta now: [-2.9746373 -1.512331 ]\n",
      "\n",
      "[HOAG] Sample 92: label=3\n",
      "[HOAG] theta now: [-2.9872234 -1.505564 ]\n",
      "\n",
      "[HOAG] Sample 93: label=5\n",
      "[HOAG] theta now: [-2.982066  -1.5092559]\n",
      "\n",
      "[HOAG] Sample 94: label=0\n",
      "[HOAG] theta now: [-2.987302 -1.506728]\n",
      "\n",
      "[HOAG] Sample 95: label=2\n",
      "[HOAG] theta now: [-2.9931116 -1.5039921]\n",
      "\n",
      "[HOAG] Sample 96: label=2\n",
      "[HOAG] theta now: [-3.0023952 -1.4964477]\n",
      "\n",
      "[HOAG] Sample 97: label=8\n",
      "[HOAG] theta now: [-3.019498  -1.4830699]\n",
      "\n",
      "[HOAG] Sample 98: label=8\n",
      "[HOAG] theta now: [-3.022505  -1.4811468]\n",
      "\n",
      "[HOAG] Sample 99: label=7\n",
      "[HOAG] theta now: [-3.0429277 -1.4688236]\n",
      "\n",
      "[HOAG] Sample 100: label=6\n",
      "[HOAG] theta now: [-3.0325947 -1.4764818]\n",
      "\n",
      "[HOAG] Sample 101: label=5\n",
      "[HOAG] theta now: [-3.0463555 -1.4647276]\n",
      "\n",
      "[HOAG] Sample 102: label=0\n",
      "[HOAG] theta now: [-3.0547705 -1.4598632]\n",
      "\n",
      "[HOAG] Sample 103: label=4\n",
      "[HOAG] theta now: [-3.059224 -1.457497]\n",
      "\n",
      "[HOAG] Sample 104: label=4\n",
      "[HOAG] theta now: [-3.0531514 -1.4601616]\n",
      "\n",
      "[HOAG] Sample 105: label=1\n",
      "[HOAG] theta now: [-3.0827217 -1.4445714]\n",
      "\n",
      "[HOAG] Sample 106: label=7\n",
      "[HOAG] theta now: [-3.092991  -1.4395089]\n",
      "\n",
      "[HOAG] Sample 107: label=7\n",
      "[HOAG] theta now: [-3.1012316 -1.4339538]\n",
      "\n",
      "[HOAG] Sample 108: label=0\n",
      "[HOAG] theta now: [-3.105383  -1.4316626]\n",
      "\n",
      "[HOAG] Sample 109: label=3\n",
      "[HOAG] theta now: [-3.1264806 -1.4172946]\n",
      "\n",
      "[HOAG] Sample 110: label=1\n",
      "[HOAG] theta now: [-3.1406837 -1.4102091]\n",
      "\n",
      "[HOAG] Sample 111: label=6\n",
      "[HOAG] theta now: [-3.156074  -1.4012709]\n",
      "\n",
      "[HOAG] Sample 112: label=5\n",
      "[HOAG] theta now: [-3.1571918 -1.4026034]\n",
      "\n",
      "[HOAG] Sample 113: label=6\n",
      "[HOAG] theta now: [-3.1768312 -1.392934 ]\n",
      "\n",
      "[HOAG] Sample 114: label=6\n",
      "[HOAG] theta now: [-3.1892548 -1.3853338]\n",
      "\n",
      "[HOAG] Sample 115: label=6\n",
      "[HOAG] theta now: [-3.1978743 -1.3795387]\n",
      "\n",
      "[HOAG] Sample 116: label=3\n",
      "[HOAG] theta now: [-3.207257 -1.37313 ]\n",
      "\n",
      "[HOAG] Sample 117: label=2\n",
      "[HOAG] theta now: [-3.2103996 -1.3726827]\n",
      "\n",
      "[HOAG] Sample 118: label=6\n",
      "[HOAG] theta now: [-3.2175665 -1.3680062]\n",
      "\n",
      "[HOAG] Sample 119: label=1\n",
      "[HOAG] theta now: [-3.2439086 -1.3529772]\n",
      "\n",
      "[HOAG] Sample 120: label=0\n",
      "[HOAG] theta now: [-3.2469914 -1.3512714]\n",
      "\n",
      "[HOAG] Sample 121: label=9\n",
      "[HOAG] theta now: [-3.2579691 -1.3442523]\n",
      "\n",
      "[HOAG] Sample 122: label=9\n",
      "[HOAG] theta now: [-3.2501452 -1.3498954]\n",
      "\n",
      "[HOAG] Sample 123: label=3\n",
      "[HOAG] theta now: [-3.2572923 -1.3452855]\n",
      "\n",
      "[HOAG] Sample 124: label=6\n",
      "[HOAG] theta now: [-3.2685037 -1.3396188]\n",
      "\n",
      "[HOAG] Sample 125: label=7\n",
      "[HOAG] theta now: [-3.2715657 -1.3376603]\n",
      "\n",
      "[HOAG] Sample 126: label=9\n",
      "[HOAG] theta now: [-3.2966924 -1.3226837]\n",
      "\n",
      "[HOAG] Sample 127: label=2\n",
      "[HOAG] theta now: [-3.2993085 -1.3197224]\n",
      "\n",
      "[HOAG] Sample 128: label=0\n",
      "[HOAG] theta now: [-3.3076477 -1.3145808]\n",
      "\n",
      "[HOAG] Sample 129: label=4\n",
      "[HOAG] theta now: [-3.3111544 -1.312721 ]\n",
      "\n",
      "[HOAG] Sample 130: label=3\n",
      "[HOAG] theta now: [-3.3308184 -1.3040355]\n",
      "\n",
      "[HOAG] Sample 131: label=0\n",
      "[HOAG] theta now: [-3.3423095 -1.296312 ]\n",
      "\n",
      "[HOAG] Sample 132: label=1\n",
      "[HOAG] theta now: [-3.3671598 -1.2845893]\n",
      "\n",
      "[HOAG] Sample 133: label=9\n",
      "[HOAG] theta now: [-3.3826723 -1.2744743]\n",
      "\n",
      "[HOAG] Sample 134: label=5\n",
      "[HOAG] theta now: [-3.3912408 -1.2698584]\n",
      "\n",
      "[HOAG] Sample 135: label=8\n",
      "[HOAG] theta now: [-3.3854725 -1.2730738]\n",
      "\n",
      "[HOAG] Sample 136: label=4\n",
      "[HOAG] theta now: [-3.3874826 -1.2709513]\n",
      "\n",
      "[HOAG] Sample 137: label=0\n",
      "[HOAG] theta now: [-3.394313  -1.2665619]\n",
      "\n",
      "[HOAG] Sample 138: label=2\n",
      "[HOAG] theta now: [-3.4029558 -1.2606024]\n",
      "\n",
      "[HOAG] Sample 139: label=3\n",
      "[HOAG] theta now: [-3.413056  -1.2525691]\n",
      "\n",
      "[HOAG] Sample 140: label=4\n",
      "[HOAG] theta now: [-3.4148996 -1.2514777]\n",
      "\n",
      "[HOAG] Sample 141: label=1\n",
      "[HOAG] theta now: [-3.4321868 -1.2399771]\n",
      "\n",
      "[HOAG] Sample 142: label=4\n",
      "[HOAG] theta now: [-3.4400613 -1.2336277]\n",
      "\n",
      "[HOAG] Sample 143: label=3\n",
      "[HOAG] theta now: [-3.4516191 -1.222465 ]\n",
      "\n",
      "[HOAG] Sample 144: label=1\n",
      "[HOAG] theta now: [-3.4604115 -1.2171961]\n",
      "\n",
      "[HOAG] Sample 145: label=2\n",
      "[HOAG] theta now: [-3.4678698 -1.2122282]\n",
      "\n",
      "[HOAG] Sample 146: label=3\n",
      "[HOAG] theta now: [-3.468932  -1.2097346]\n",
      "\n",
      "[HOAG] Sample 147: label=9\n",
      "[HOAG] theta now: [-3.4742026 -1.2068228]\n",
      "\n",
      "[HOAG] Sample 148: label=9\n",
      "[HOAG] theta now: [-3.4671502 -1.2096653]\n",
      "\n",
      "[HOAG] Sample 149: label=3\n",
      "[HOAG] theta now: [-3.4739163 -1.2052113]\n",
      "\n",
      "[HOAG] Sample 150: label=3\n",
      "[HOAG] theta now: [-3.4779656 -1.2028698]\n",
      "\n",
      "[HOAG] Sample 151: label=7\n",
      "[HOAG] theta now: [-3.4894867 -1.1982968]\n",
      "\n",
      "[HOAG] Sample 152: label=5\n",
      "[HOAG] theta now: [-3.4951346 -1.1953859]\n",
      "\n",
      "[HOAG] Sample 153: label=1\n",
      "[HOAG] theta now: [-3.5144198 -1.1815089]\n",
      "\n",
      "[HOAG] Sample 154: label=3\n",
      "[HOAG] theta now: [-3.523879  -1.1739157]\n",
      "\n",
      "[HOAG] Sample 155: label=6\n",
      "[HOAG] theta now: [-3.52708  -1.171741]\n",
      "\n",
      "[HOAG] Sample 156: label=3\n",
      "[HOAG] theta now: [-3.5313764 -1.1686189]\n",
      "\n",
      "[HOAG] Sample 157: label=9\n",
      "[HOAG] theta now: [-3.5487254 -1.1568167]\n",
      "\n",
      "[HOAG] Sample 158: label=1\n",
      "[HOAG] theta now: [-3.5588381 -1.1506306]\n",
      "\n",
      "[HOAG] Sample 159: label=9\n",
      "[HOAG] theta now: [-3.5688357 -1.1433556]\n",
      "\n",
      "[HOAG] Sample 160: label=6\n",
      "[HOAG] theta now: [-3.585717  -1.1312728]\n",
      "\n",
      "[HOAG] Sample 161: label=1\n",
      "[HOAG] theta now: [-3.6091897 -1.1112529]\n",
      "\n",
      "[HOAG] Sample 162: label=1\n",
      "[HOAG] theta now: [-3.62857  -1.098984]\n",
      "\n",
      "[HOAG] Sample 163: label=8\n",
      "[HOAG] theta now: [-3.6337233 -1.0955665]\n",
      "\n",
      "[HOAG] Sample 164: label=2\n",
      "[HOAG] theta now: [-3.6352952 -1.0944322]\n",
      "\n",
      "[HOAG] Sample 165: label=9\n",
      "[HOAG] theta now: [-3.639908  -1.0907179]\n",
      "\n",
      "[HOAG] Sample 166: label=8\n",
      "[HOAG] theta now: [-3.650275  -1.0814525]\n",
      "\n",
      "[HOAG] Sample 167: label=3\n",
      "[HOAG] theta now: [-3.655674 -1.07811 ]\n",
      "\n",
      "[HOAG] Sample 168: label=7\n",
      "[HOAG] theta now: [-3.6620843 -1.0723381]\n",
      "\n",
      "[HOAG] Sample 169: label=6\n",
      "[HOAG] theta now: [-3.6781325 -1.058697 ]\n",
      "\n",
      "[HOAG] Sample 170: label=9\n",
      "[HOAG] theta now: [-3.6850502 -1.0538007]\n",
      "\n",
      "[HOAG] Sample 171: label=6\n",
      "[HOAG] theta now: [-3.6914878 -1.0487324]\n",
      "\n",
      "[HOAG] Sample 172: label=2\n",
      "[HOAG] theta now: [-3.6915352 -1.0486478]\n",
      "\n",
      "[HOAG] Sample 173: label=9\n",
      "[HOAG] theta now: [-3.7004955 -1.0414693]\n",
      "\n",
      "[HOAG] Sample 174: label=0\n",
      "[HOAG] theta now: [-3.70405   -1.0390012]\n",
      "\n",
      "[HOAG] Sample 175: label=9\n",
      "[HOAG] theta now: [-3.7131002 -1.0321456]\n",
      "\n",
      "[HOAG] Sample 176: label=6\n",
      "[HOAG] theta now: [-3.7296858 -1.0184736]\n",
      "\n",
      "[HOAG] Sample 177: label=8\n",
      "[HOAG] theta now: [-3.7363555 -1.01254  ]\n",
      "\n",
      "[HOAG] Sample 178: label=9\n",
      "[HOAG] theta now: [-3.739871  -1.0099125]\n",
      "\n",
      "[HOAG] Sample 179: label=3\n",
      "[HOAG] theta now: [-3.743765  -1.0072238]\n",
      "\n",
      "[HOAG] Sample 180: label=4\n",
      "[HOAG] theta now: [-3.7468839 -1.0048746]\n",
      "\n",
      "[HOAG] Sample 181: label=8\n",
      "[HOAG] theta now: [-3.7547338  -0.99886584]\n",
      "\n",
      "[HOAG] Sample 182: label=4\n",
      "[HOAG] theta now: [-3.760927   -0.99391645]\n",
      "\n",
      "[HOAG] Sample 183: label=4\n",
      "[HOAG] theta now: [-3.7637436 -0.9917211]\n",
      "\n",
      "[HOAG] Sample 184: label=7\n",
      "[HOAG] theta now: [-3.7724538  -0.98457766]\n",
      "\n",
      "[HOAG] Sample 185: label=5\n",
      "[HOAG] theta now: [-3.781283  -0.9779885]\n",
      "\n",
      "[HOAG] Sample 186: label=8\n",
      "[HOAG] theta now: [-3.7864342 -0.9738075]\n",
      "\n",
      "[HOAG] Sample 187: label=5\n",
      "[HOAG] theta now: [-3.7911701 -0.9713557]\n",
      "\n",
      "[HOAG] Sample 188: label=8\n",
      "[HOAG] theta now: [-3.791099   -0.97267985]\n",
      "\n",
      "[HOAG] Sample 189: label=8\n",
      "[HOAG] theta now: [-3.795264  -0.9689223]\n",
      "\n",
      "[HOAG] Sample 190: label=5\n",
      "[HOAG] theta now: [-3.8031812 -0.9613294]\n",
      "\n",
      "[HOAG] Sample 191: label=9\n",
      "[HOAG] theta now: [-3.812717  -0.9539381]\n",
      "\n",
      "[HOAG] Sample 192: label=7\n",
      "[HOAG] theta now: [-3.8186958  -0.94925004]\n",
      "\n",
      "[HOAG] Sample 193: label=1\n",
      "[HOAG] theta now: [-3.826752  -0.9433854]\n",
      "\n",
      "[HOAG] Sample 194: label=7\n",
      "[HOAG] theta now: [-3.828191   -0.94287276]\n",
      "\n",
      "[HOAG] Sample 195: label=5\n",
      "[HOAG] theta now: [-3.8344367  -0.93758386]\n",
      "\n",
      "[HOAG] Sample 196: label=8\n",
      "[HOAG] theta now: [-3.8383834  -0.93428975]\n",
      "\n",
      "[HOAG] Sample 197: label=6\n",
      "[HOAG] theta now: [-3.840182   -0.93293965]\n",
      "\n",
      "[HOAG] Sample 198: label=4\n",
      "[HOAG] theta now: [-3.847976  -0.9260672]\n",
      "\n",
      "[HOAG] Sample 199: label=0\n",
      "[HOAG] theta now: [-3.8560786 -0.9194   ]\n",
      "\n",
      "[HOAG] Sample 200: label=8\n",
      "[HOAG] theta now: [-3.858434  -0.9176799]\n",
      "[HOAG] quick eval after sample 200\n",
      "\n",
      "--- Evaluating Model on Test Set (Iterative Deblurring) ---\n",
      "[eval] sample 0 iter 0 grad_norm=5.9404e-01, inner_loss=7.3527e+00\n",
      "[eval] sample 0 iter 10 grad_norm=5.6514e-01, inner_loss=7.3191e+00\n",
      "[eval] sample 0 iter 20 grad_norm=5.3805e-01, inner_loss=7.2886e+00\n",
      "[eval] sample 0 iter 30 grad_norm=5.1264e-01, inner_loss=7.2609e+00\n",
      "[eval] sample 0 iter 40 grad_norm=4.8880e-01, inner_loss=7.2358e+00\n",
      "[eval] sample 1 iter 0 grad_norm=7.2811e-01, inner_loss=7.7893e+00\n",
      "[eval] sample 1 iter 10 grad_norm=6.9305e-01, inner_loss=7.7387e+00\n",
      "[eval] sample 1 iter 20 grad_norm=6.6026e-01, inner_loss=7.6928e+00\n",
      "[eval] sample 1 iter 30 grad_norm=6.2957e-01, inner_loss=7.6512e+00\n",
      "[eval] sample 1 iter 40 grad_norm=6.0082e-01, inner_loss=7.6132e+00\n",
      "[eval] sample 2 iter 0 grad_norm=4.1705e-01, inner_loss=6.9998e+00\n",
      "[eval] sample 2 iter 10 grad_norm=3.9724e-01, inner_loss=6.9831e+00\n",
      "[eval] sample 2 iter 20 grad_norm=3.7866e-01, inner_loss=6.9681e+00\n",
      "[eval] sample 2 iter 30 grad_norm=3.6124e-01, inner_loss=6.9543e+00\n",
      "[eval] sample 2 iter 40 grad_norm=3.4489e-01, inner_loss=6.9419e+00\n",
      "[eval] sample 3 iter 0 grad_norm=9.0013e-01, inner_loss=8.0684e+00\n",
      "[eval] sample 3 iter 10 grad_norm=8.5644e-01, inner_loss=7.9911e+00\n",
      "[eval] sample 3 iter 20 grad_norm=8.1527e-01, inner_loss=7.9210e+00\n",
      "[eval] sample 3 iter 30 grad_norm=7.7646e-01, inner_loss=7.8576e+00\n",
      "[eval] sample 3 iter 40 grad_norm=7.3985e-01, inner_loss=7.8000e+00\n",
      "[eval] sample 4 iter 0 grad_norm=5.7406e-01, inner_loss=7.3360e+00\n",
      "[eval] sample 4 iter 10 grad_norm=5.4786e-01, inner_loss=7.3044e+00\n",
      "[eval] sample 4 iter 20 grad_norm=5.2316e-01, inner_loss=7.2757e+00\n",
      "[eval] sample 4 iter 30 grad_norm=4.9988e-01, inner_loss=7.2495e+00\n",
      "[eval] sample 4 iter 40 grad_norm=4.7791e-01, inner_loss=7.2255e+00\n",
      "[eval] sample 5 iter 0 grad_norm=5.5719e-01, inner_loss=7.2434e+00\n",
      "[eval] sample 5 iter 10 grad_norm=5.2989e-01, inner_loss=7.2138e+00\n",
      "[eval] sample 5 iter 20 grad_norm=5.0433e-01, inner_loss=7.1870e+00\n",
      "[eval] sample 5 iter 30 grad_norm=4.8037e-01, inner_loss=7.1627e+00\n",
      "[eval] sample 5 iter 40 grad_norm=4.5791e-01, inner_loss=7.1406e+00\n",
      "[eval] sample 6 iter 0 grad_norm=5.9758e-01, inner_loss=7.3907e+00\n",
      "[eval] sample 6 iter 10 grad_norm=5.6964e-01, inner_loss=7.3566e+00\n",
      "[eval] sample 6 iter 20 grad_norm=5.4339e-01, inner_loss=7.3255e+00\n",
      "[eval] sample 6 iter 30 grad_norm=5.1870e-01, inner_loss=7.2973e+00\n",
      "[eval] sample 6 iter 40 grad_norm=4.9546e-01, inner_loss=7.2715e+00\n",
      "[eval] sample 7 iter 0 grad_norm=5.4869e-01, inner_loss=7.2991e+00\n",
      "[eval] sample 7 iter 10 grad_norm=5.1950e-01, inner_loss=7.2705e+00\n",
      "[eval] sample 7 iter 20 grad_norm=4.9243e-01, inner_loss=7.2449e+00\n",
      "[eval] sample 7 iter 30 grad_norm=4.6731e-01, inner_loss=7.2218e+00\n",
      "[eval] sample 7 iter 40 grad_norm=4.4398e-01, inner_loss=7.2010e+00\n",
      "[eval] sample 8 iter 0 grad_norm=8.1716e-01, inner_loss=7.7871e+00\n",
      "[eval] sample 8 iter 10 grad_norm=7.7440e-01, inner_loss=7.7236e+00\n",
      "[eval] sample 8 iter 20 grad_norm=7.3429e-01, inner_loss=7.6666e+00\n",
      "[eval] sample 8 iter 30 grad_norm=6.9666e-01, inner_loss=7.6153e+00\n",
      "[eval] sample 8 iter 40 grad_norm=6.6134e-01, inner_loss=7.5691e+00\n",
      "[eval] sample 9 iter 0 grad_norm=6.9756e-01, inner_loss=7.6369e+00\n",
      "[eval] sample 9 iter 10 grad_norm=6.6283e-01, inner_loss=7.5905e+00\n",
      "[eval] sample 9 iter 20 grad_norm=6.3043e-01, inner_loss=7.5486e+00\n",
      "[eval] sample 9 iter 30 grad_norm=6.0017e-01, inner_loss=7.5107e+00\n",
      "[eval] sample 9 iter 40 grad_norm=5.7189e-01, inner_loss=7.4763e+00\n",
      "[eval] sample 10 iter 0 grad_norm=7.6180e-01, inner_loss=7.8543e+00\n",
      "[eval] sample 10 iter 10 grad_norm=7.2566e-01, inner_loss=7.7988e+00\n",
      "[eval] sample 10 iter 20 grad_norm=6.9170e-01, inner_loss=7.7485e+00\n",
      "[eval] sample 10 iter 30 grad_norm=6.5977e-01, inner_loss=7.7027e+00\n",
      "[eval] sample 10 iter 40 grad_norm=6.2973e-01, inner_loss=7.6611e+00\n",
      "[eval] sample 11 iter 0 grad_norm=5.6075e-01, inner_loss=7.4885e+00\n",
      "[eval] sample 11 iter 10 grad_norm=5.3691e-01, inner_loss=7.4584e+00\n",
      "[eval] sample 11 iter 20 grad_norm=5.1455e-01, inner_loss=7.4307e+00\n",
      "[eval] sample 11 iter 30 grad_norm=4.9356e-01, inner_loss=7.4052e+00\n",
      "[eval] sample 11 iter 40 grad_norm=4.7384e-01, inner_loss=7.3818e+00\n",
      "[eval] sample 12 iter 0 grad_norm=5.7194e-01, inner_loss=7.4315e+00\n",
      "[eval] sample 12 iter 10 grad_norm=5.4628e-01, inner_loss=7.4002e+00\n",
      "[eval] sample 12 iter 20 grad_norm=5.2233e-01, inner_loss=7.3716e+00\n",
      "[eval] sample 12 iter 30 grad_norm=4.9993e-01, inner_loss=7.3454e+00\n",
      "[eval] sample 12 iter 40 grad_norm=4.7897e-01, inner_loss=7.3215e+00\n",
      "[eval] sample 13 iter 0 grad_norm=7.7924e-01, inner_loss=7.8446e+00\n",
      "[eval] sample 13 iter 10 grad_norm=7.4197e-01, inner_loss=7.7866e+00\n",
      "[eval] sample 13 iter 20 grad_norm=7.0687e-01, inner_loss=7.7340e+00\n",
      "[eval] sample 13 iter 30 grad_norm=6.7381e-01, inner_loss=7.6863e+00\n",
      "[eval] sample 13 iter 40 grad_norm=6.4265e-01, inner_loss=7.6428e+00\n",
      "[eval] sample 14 iter 0 grad_norm=6.2960e-01, inner_loss=7.3820e+00\n",
      "[eval] sample 14 iter 10 grad_norm=5.9842e-01, inner_loss=7.3442e+00\n",
      "[eval] sample 14 iter 20 grad_norm=5.6923e-01, inner_loss=7.3100e+00\n",
      "[eval] sample 14 iter 30 grad_norm=5.4189e-01, inner_loss=7.2791e+00\n",
      "[eval] sample 14 iter 40 grad_norm=5.1626e-01, inner_loss=7.2510e+00\n",
      "[eval] sample 15 iter 0 grad_norm=5.5812e-01, inner_loss=7.3742e+00\n",
      "[eval] sample 15 iter 10 grad_norm=5.3308e-01, inner_loss=7.3444e+00\n",
      "[eval] sample 15 iter 20 grad_norm=5.0957e-01, inner_loss=7.3172e+00\n",
      "[eval] sample 15 iter 30 grad_norm=4.8746e-01, inner_loss=7.2923e+00\n",
      "[eval] sample 15 iter 40 grad_norm=4.6666e-01, inner_loss=7.2695e+00\n",
      "[eval] sample 16 iter 0 grad_norm=6.2858e-01, inner_loss=7.5014e+00\n",
      "[eval] sample 16 iter 10 grad_norm=6.0096e-01, inner_loss=7.4636e+00\n",
      "[eval] sample 16 iter 20 grad_norm=5.7492e-01, inner_loss=7.4289e+00\n",
      "[eval] sample 16 iter 30 grad_norm=5.5035e-01, inner_loss=7.3972e+00\n",
      "[eval] sample 16 iter 40 grad_norm=5.2714e-01, inner_loss=7.3681e+00\n",
      "[eval] sample 17 iter 0 grad_norm=7.0114e-01, inner_loss=7.6104e+00\n",
      "[eval] sample 17 iter 10 grad_norm=6.6674e-01, inner_loss=7.5635e+00\n",
      "[eval] sample 17 iter 20 grad_norm=6.3451e-01, inner_loss=7.5211e+00\n",
      "[eval] sample 17 iter 30 grad_norm=6.0428e-01, inner_loss=7.4827e+00\n",
      "[eval] sample 17 iter 40 grad_norm=5.7593e-01, inner_loss=7.4478e+00\n",
      "[eval] sample 18 iter 0 grad_norm=7.3494e-01, inner_loss=7.9138e+00\n",
      "[eval] sample 18 iter 10 grad_norm=7.0288e-01, inner_loss=7.8620e+00\n",
      "[eval] sample 18 iter 20 grad_norm=6.7276e-01, inner_loss=7.8146e+00\n",
      "[eval] sample 18 iter 30 grad_norm=6.4443e-01, inner_loss=7.7711e+00\n",
      "[eval] sample 18 iter 40 grad_norm=6.1776e-01, inner_loss=7.7312e+00\n",
      "[eval] sample 19 iter 0 grad_norm=4.8748e-01, inner_loss=7.1739e+00\n",
      "[eval] sample 19 iter 10 grad_norm=4.6595e-01, inner_loss=7.1511e+00\n",
      "[eval] sample 19 iter 20 grad_norm=4.4576e-01, inner_loss=7.1303e+00\n",
      "[eval] sample 19 iter 30 grad_norm=4.2680e-01, inner_loss=7.1113e+00\n",
      "[eval] sample 19 iter 40 grad_norm=4.0897e-01, inner_loss=7.0938e+00\n",
      "[eval] sample 20 iter 0 grad_norm=6.7286e-01, inner_loss=7.4617e+00\n",
      "[eval] sample 20 iter 10 grad_norm=6.3609e-01, inner_loss=7.4187e+00\n",
      "[eval] sample 20 iter 20 grad_norm=6.0190e-01, inner_loss=7.3803e+00\n",
      "[eval] sample 20 iter 30 grad_norm=5.7011e-01, inner_loss=7.3459e+00\n",
      "[eval] sample 20 iter 40 grad_norm=5.4051e-01, inner_loss=7.3150e+00\n",
      "[eval] sample 21 iter 0 grad_norm=5.8055e-01, inner_loss=7.4544e+00\n",
      "[eval] sample 21 iter 10 grad_norm=5.5462e-01, inner_loss=7.4221e+00\n",
      "[eval] sample 21 iter 20 grad_norm=5.3039e-01, inner_loss=7.3927e+00\n",
      "[eval] sample 21 iter 30 grad_norm=5.0771e-01, inner_loss=7.3657e+00\n",
      "[eval] sample 21 iter 40 grad_norm=4.8646e-01, inner_loss=7.3409e+00\n",
      "[eval] sample 22 iter 0 grad_norm=5.6689e-01, inner_loss=7.4078e+00\n",
      "[eval] sample 22 iter 10 grad_norm=5.4074e-01, inner_loss=7.3771e+00\n",
      "[eval] sample 22 iter 20 grad_norm=5.1626e-01, inner_loss=7.3491e+00\n",
      "[eval] sample 22 iter 30 grad_norm=4.9331e-01, inner_loss=7.3236e+00\n",
      "[eval] sample 22 iter 40 grad_norm=4.7179e-01, inner_loss=7.3003e+00\n",
      "[eval] sample 23 iter 0 grad_norm=5.6365e-01, inner_loss=7.3816e+00\n",
      "[eval] sample 23 iter 10 grad_norm=5.3838e-01, inner_loss=7.3512e+00\n",
      "[eval] sample 23 iter 20 grad_norm=5.1466e-01, inner_loss=7.3234e+00\n",
      "[eval] sample 23 iter 30 grad_norm=4.9238e-01, inner_loss=7.2980e+00\n",
      "[eval] sample 23 iter 40 grad_norm=4.7142e-01, inner_loss=7.2748e+00\n",
      "[eval] sample 24 iter 0 grad_norm=3.8674e-01, inner_loss=6.9915e+00\n",
      "[eval] sample 24 iter 10 grad_norm=3.7016e-01, inner_loss=6.9771e+00\n",
      "[eval] sample 24 iter 20 grad_norm=3.5458e-01, inner_loss=6.9640e+00\n",
      "[eval] sample 24 iter 30 grad_norm=3.3992e-01, inner_loss=6.9519e+00\n",
      "[eval] sample 24 iter 40 grad_norm=3.2612e-01, inner_loss=6.9408e+00\n",
      "[eval] sample 25 iter 0 grad_norm=1.0369e+00, inner_loss=8.3600e+00\n",
      "[eval] sample 25 iter 10 grad_norm=9.8365e-01, inner_loss=8.2577e+00\n",
      "[eval] sample 25 iter 20 grad_norm=9.3361e-01, inner_loss=8.1656e+00\n",
      "[eval] sample 25 iter 30 grad_norm=8.8652e-01, inner_loss=8.0826e+00\n",
      "[eval] sample 25 iter 40 grad_norm=8.4219e-01, inner_loss=8.0077e+00\n",
      "[eval] sample 26 iter 0 grad_norm=5.2711e-01, inner_loss=7.2358e+00\n",
      "[eval] sample 26 iter 10 grad_norm=5.0237e-01, inner_loss=7.2092e+00\n",
      "[eval] sample 26 iter 20 grad_norm=4.7915e-01, inner_loss=7.1851e+00\n",
      "[eval] sample 26 iter 30 grad_norm=4.5735e-01, inner_loss=7.1631e+00\n",
      "[eval] sample 26 iter 40 grad_norm=4.3688e-01, inner_loss=7.1431e+00\n",
      "[eval] sample 27 iter 0 grad_norm=6.1661e-01, inner_loss=7.4943e+00\n",
      "[eval] sample 27 iter 10 grad_norm=5.8964e-01, inner_loss=7.4578e+00\n",
      "[eval] sample 27 iter 20 grad_norm=5.6429e-01, inner_loss=7.4245e+00\n",
      "[eval] sample 27 iter 30 grad_norm=5.4043e-01, inner_loss=7.3939e+00\n",
      "[eval] sample 27 iter 40 grad_norm=5.1795e-01, inner_loss=7.3659e+00\n",
      "[eval] sample 28 iter 0 grad_norm=8.5572e-01, inner_loss=8.0179e+00\n",
      "[eval] sample 28 iter 10 grad_norm=8.1442e-01, inner_loss=7.9480e+00\n",
      "[eval] sample 28 iter 20 grad_norm=7.7559e-01, inner_loss=7.8846e+00\n",
      "[eval] sample 28 iter 30 grad_norm=7.3907e-01, inner_loss=7.8272e+00\n",
      "[eval] sample 28 iter 40 grad_norm=7.0471e-01, inner_loss=7.7750e+00\n",
      "[eval] sample 29 iter 0 grad_norm=5.2055e-01, inner_loss=7.1802e+00\n",
      "[eval] sample 29 iter 10 grad_norm=4.9542e-01, inner_loss=7.1544e+00\n",
      "[eval] sample 29 iter 20 grad_norm=4.7189e-01, inner_loss=7.1309e+00\n",
      "[eval] sample 29 iter 30 grad_norm=4.4982e-01, inner_loss=7.1096e+00\n",
      "[eval] sample 29 iter 40 grad_norm=4.2913e-01, inner_loss=7.0903e+00\n",
      "[eval] sample 30 iter 0 grad_norm=6.6021e-01, inner_loss=7.5411e+00\n",
      "[eval] sample 30 iter 10 grad_norm=6.2650e-01, inner_loss=7.4996e+00\n",
      "[eval] sample 30 iter 20 grad_norm=5.9506e-01, inner_loss=7.4622e+00\n",
      "[eval] sample 30 iter 30 grad_norm=5.6572e-01, inner_loss=7.4285e+00\n",
      "[eval] sample 30 iter 40 grad_norm=5.3833e-01, inner_loss=7.3979e+00\n",
      "[eval] sample 31 iter 0 grad_norm=4.5729e-01, inner_loss=7.0696e+00\n",
      "[eval] sample 31 iter 10 grad_norm=4.3542e-01, inner_loss=7.0497e+00\n",
      "[eval] sample 31 iter 20 grad_norm=4.1494e-01, inner_loss=7.0316e+00\n",
      "[eval] sample 31 iter 30 grad_norm=3.9573e-01, inner_loss=7.0151e+00\n",
      "[eval] sample 31 iter 40 grad_norm=3.7770e-01, inner_loss=7.0001e+00\n",
      "[eval] sample 32 iter 0 grad_norm=6.7972e-01, inner_loss=7.6737e+00\n",
      "[eval] sample 32 iter 10 grad_norm=6.4876e-01, inner_loss=7.6295e+00\n",
      "[eval] sample 32 iter 20 grad_norm=6.1976e-01, inner_loss=7.5892e+00\n",
      "[eval] sample 32 iter 30 grad_norm=5.9258e-01, inner_loss=7.5524e+00\n",
      "[eval] sample 32 iter 40 grad_norm=5.6708e-01, inner_loss=7.5187e+00\n",
      "[eval] sample 33 iter 0 grad_norm=6.6835e-01, inner_loss=7.5649e+00\n",
      "[eval] sample 33 iter 10 grad_norm=6.3556e-01, inner_loss=7.5223e+00\n",
      "[eval] sample 33 iter 20 grad_norm=6.0480e-01, inner_loss=7.4838e+00\n",
      "[eval] sample 33 iter 30 grad_norm=5.7596e-01, inner_loss=7.4489e+00\n",
      "[eval] sample 33 iter 40 grad_norm=5.4888e-01, inner_loss=7.4172e+00\n",
      "[eval] sample 34 iter 0 grad_norm=7.4819e-01, inner_loss=7.6991e+00\n",
      "[eval] sample 34 iter 10 grad_norm=7.1093e-01, inner_loss=7.6458e+00\n",
      "[eval] sample 34 iter 20 grad_norm=6.7603e-01, inner_loss=7.5976e+00\n",
      "[eval] sample 34 iter 30 grad_norm=6.4333e-01, inner_loss=7.5540e+00\n",
      "[eval] sample 34 iter 40 grad_norm=6.1267e-01, inner_loss=7.5145e+00\n",
      "[eval] sample 35 iter 0 grad_norm=7.4623e-01, inner_loss=7.7754e+00\n",
      "[eval] sample 35 iter 10 grad_norm=7.1055e-01, inner_loss=7.7222e+00\n",
      "[eval] sample 35 iter 20 grad_norm=6.7700e-01, inner_loss=7.6740e+00\n",
      "[eval] sample 35 iter 30 grad_norm=6.4545e-01, inner_loss=7.6302e+00\n",
      "[eval] sample 35 iter 40 grad_norm=6.1575e-01, inner_loss=7.5903e+00\n",
      "[eval] sample 36 iter 0 grad_norm=6.4324e-01, inner_loss=7.4527e+00\n",
      "[eval] sample 36 iter 10 grad_norm=6.1099e-01, inner_loss=7.4132e+00\n",
      "[eval] sample 36 iter 20 grad_norm=5.8075e-01, inner_loss=7.3777e+00\n",
      "[eval] sample 36 iter 30 grad_norm=5.5239e-01, inner_loss=7.3455e+00\n",
      "[eval] sample 36 iter 40 grad_norm=5.2577e-01, inner_loss=7.3164e+00\n",
      "[eval] sample 37 iter 0 grad_norm=5.6003e-01, inner_loss=7.2474e+00\n",
      "[eval] sample 37 iter 10 grad_norm=5.3263e-01, inner_loss=7.2175e+00\n",
      "[eval] sample 37 iter 20 grad_norm=5.0698e-01, inner_loss=7.1904e+00\n",
      "[eval] sample 37 iter 30 grad_norm=4.8294e-01, inner_loss=7.1659e+00\n",
      "[eval] sample 37 iter 40 grad_norm=4.6040e-01, inner_loss=7.1436e+00\n",
      "[eval] sample 38 iter 0 grad_norm=5.2536e-01, inner_loss=7.2408e+00\n",
      "[eval] sample 38 iter 10 grad_norm=4.9996e-01, inner_loss=7.2145e+00\n",
      "[eval] sample 38 iter 20 grad_norm=4.7619e-01, inner_loss=7.1906e+00\n",
      "[eval] sample 38 iter 30 grad_norm=4.5393e-01, inner_loss=7.1689e+00\n",
      "[eval] sample 38 iter 40 grad_norm=4.3306e-01, inner_loss=7.1492e+00\n",
      "[eval] sample 39 iter 0 grad_norm=6.7331e-01, inner_loss=7.4227e+00\n",
      "[eval] sample 39 iter 10 grad_norm=6.3889e-01, inner_loss=7.3795e+00\n",
      "[eval] sample 39 iter 20 grad_norm=6.0670e-01, inner_loss=7.3407e+00\n",
      "[eval] sample 39 iter 30 grad_norm=5.7656e-01, inner_loss=7.3056e+00\n",
      "[eval] sample 39 iter 40 grad_norm=5.4834e-01, inner_loss=7.2739e+00\n",
      "[eval] sample 40 iter 0 grad_norm=3.0437e-01, inner_loss=6.8310e+00\n",
      "[eval] sample 40 iter 10 grad_norm=2.9021e-01, inner_loss=6.8222e+00\n",
      "[eval] sample 40 iter 20 grad_norm=2.7692e-01, inner_loss=6.8141e+00\n",
      "[eval] sample 40 iter 30 grad_norm=2.6445e-01, inner_loss=6.8068e+00\n",
      "[eval] sample 40 iter 40 grad_norm=2.5273e-01, inner_loss=6.8001e+00\n",
      "[eval] sample 41 iter 0 grad_norm=5.3978e-01, inner_loss=7.2363e+00\n",
      "[eval] sample 41 iter 10 grad_norm=5.1427e-01, inner_loss=7.2085e+00\n",
      "[eval] sample 41 iter 20 grad_norm=4.9032e-01, inner_loss=7.1832e+00\n",
      "[eval] sample 41 iter 30 grad_norm=4.6782e-01, inner_loss=7.1602e+00\n",
      "[eval] sample 41 iter 40 grad_norm=4.4667e-01, inner_loss=7.1393e+00\n",
      "[eval] sample 42 iter 0 grad_norm=6.6063e-01, inner_loss=7.5321e+00\n",
      "[eval] sample 42 iter 10 grad_norm=6.2975e-01, inner_loss=7.4904e+00\n",
      "[eval] sample 42 iter 20 grad_norm=6.0082e-01, inner_loss=7.4525e+00\n",
      "[eval] sample 42 iter 30 grad_norm=5.7370e-01, inner_loss=7.4179e+00\n",
      "[eval] sample 42 iter 40 grad_norm=5.4824e-01, inner_loss=7.3864e+00\n",
      "[eval] sample 43 iter 0 grad_norm=5.0378e-01, inner_loss=7.1807e+00\n",
      "[eval] sample 43 iter 10 grad_norm=4.8001e-01, inner_loss=7.1565e+00\n",
      "[eval] sample 43 iter 20 grad_norm=4.5771e-01, inner_loss=7.1344e+00\n",
      "[eval] sample 43 iter 30 grad_norm=4.3678e-01, inner_loss=7.1144e+00\n",
      "[eval] sample 43 iter 40 grad_norm=4.1711e-01, inner_loss=7.0961e+00\n",
      "[eval] sample 44 iter 0 grad_norm=5.9820e-01, inner_loss=7.3874e+00\n",
      "[eval] sample 44 iter 10 grad_norm=5.6926e-01, inner_loss=7.3533e+00\n",
      "[eval] sample 44 iter 20 grad_norm=5.4221e-01, inner_loss=7.3223e+00\n",
      "[eval] sample 44 iter 30 grad_norm=5.1691e-01, inner_loss=7.2942e+00\n",
      "[eval] sample 44 iter 40 grad_norm=4.9322e-01, inner_loss=7.2687e+00\n",
      "[eval] sample 45 iter 0 grad_norm=5.1524e-01, inner_loss=7.3024e+00\n",
      "[eval] sample 45 iter 10 grad_norm=4.9405e-01, inner_loss=7.2769e+00\n",
      "[eval] sample 45 iter 20 grad_norm=4.7411e-01, inner_loss=7.2534e+00\n",
      "[eval] sample 45 iter 30 grad_norm=4.5533e-01, inner_loss=7.2318e+00\n",
      "[eval] sample 45 iter 40 grad_norm=4.3761e-01, inner_loss=7.2118e+00\n",
      "[eval] sample 46 iter 0 grad_norm=6.8721e-01, inner_loss=7.4570e+00\n",
      "[eval] sample 46 iter 10 grad_norm=6.5198e-01, inner_loss=7.4121e+00\n",
      "[eval] sample 46 iter 20 grad_norm=6.1902e-01, inner_loss=7.3716e+00\n",
      "[eval] sample 46 iter 30 grad_norm=5.8817e-01, inner_loss=7.3351e+00\n",
      "[eval] sample 46 iter 40 grad_norm=5.5926e-01, inner_loss=7.3021e+00\n",
      "[eval] sample 47 iter 0 grad_norm=4.2102e-01, inner_loss=7.0256e+00\n",
      "[eval] sample 47 iter 10 grad_norm=3.9930e-01, inner_loss=7.0088e+00\n",
      "[eval] sample 47 iter 20 grad_norm=3.7903e-01, inner_loss=6.9936e+00\n",
      "[eval] sample 47 iter 30 grad_norm=3.6011e-01, inner_loss=6.9799e+00\n",
      "[eval] sample 47 iter 40 grad_norm=3.4244e-01, inner_loss=6.9675e+00\n",
      "[eval] sample 48 iter 0 grad_norm=8.4738e-01, inner_loss=7.9070e+00\n",
      "[eval] sample 48 iter 10 grad_norm=8.0439e-01, inner_loss=7.8386e+00\n",
      "[eval] sample 48 iter 20 grad_norm=7.6424e-01, inner_loss=7.7770e+00\n",
      "[eval] sample 48 iter 30 grad_norm=7.2668e-01, inner_loss=7.7213e+00\n",
      "[eval] sample 48 iter 40 grad_norm=6.9154e-01, inner_loss=7.6709e+00\n",
      "[eval] sample 49 iter 0 grad_norm=6.7140e-01, inner_loss=7.5467e+00\n",
      "[eval] sample 49 iter 10 grad_norm=6.3926e-01, inner_loss=7.5037e+00\n",
      "[eval] sample 49 iter 20 grad_norm=6.0904e-01, inner_loss=7.4646e+00\n",
      "[eval] sample 49 iter 30 grad_norm=5.8062e-01, inner_loss=7.4292e+00\n",
      "[eval] sample 49 iter 40 grad_norm=5.5387e-01, inner_loss=7.3969e+00\n",
      "[eval] sample 50 iter 0 grad_norm=5.5808e-01, inner_loss=7.4102e+00\n",
      "[eval] sample 50 iter 10 grad_norm=5.3374e-01, inner_loss=7.3804e+00\n",
      "[eval] sample 50 iter 20 grad_norm=5.1095e-01, inner_loss=7.3530e+00\n",
      "[eval] sample 50 iter 30 grad_norm=4.8960e-01, inner_loss=7.3280e+00\n",
      "[eval] sample 50 iter 40 grad_norm=4.6958e-01, inner_loss=7.3049e+00\n",
      "[eval] sample 51 iter 0 grad_norm=7.7472e-01, inner_loss=8.0723e+00\n",
      "[eval] sample 51 iter 10 grad_norm=7.4119e-01, inner_loss=8.0148e+00\n",
      "[eval] sample 51 iter 20 grad_norm=7.0973e-01, inner_loss=7.9620e+00\n",
      "[eval] sample 51 iter 30 grad_norm=6.8019e-01, inner_loss=7.9136e+00\n",
      "[eval] sample 51 iter 40 grad_norm=6.5242e-01, inner_loss=7.8692e+00\n",
      "[eval] sample 52 iter 0 grad_norm=5.8697e-01, inner_loss=7.4758e+00\n",
      "[eval] sample 52 iter 10 grad_norm=5.6149e-01, inner_loss=7.4428e+00\n",
      "[eval] sample 52 iter 20 grad_norm=5.3755e-01, inner_loss=7.4125e+00\n",
      "[eval] sample 52 iter 30 grad_norm=5.1502e-01, inner_loss=7.3848e+00\n",
      "[eval] sample 52 iter 40 grad_norm=4.9380e-01, inner_loss=7.3593e+00\n",
      "[eval] sample 53 iter 0 grad_norm=4.4593e-01, inner_loss=7.1147e+00\n",
      "[eval] sample 53 iter 10 grad_norm=4.2660e-01, inner_loss=7.0956e+00\n",
      "[eval] sample 53 iter 20 grad_norm=4.0839e-01, inner_loss=7.0781e+00\n",
      "[eval] sample 53 iter 30 grad_norm=3.9121e-01, inner_loss=7.0621e+00\n",
      "[eval] sample 53 iter 40 grad_norm=3.7500e-01, inner_loss=7.0474e+00\n",
      "[eval] sample 54 iter 0 grad_norm=7.3689e-01, inner_loss=7.9325e+00\n",
      "[eval] sample 54 iter 10 grad_norm=7.0590e-01, inner_loss=7.8803e+00\n",
      "[eval] sample 54 iter 20 grad_norm=6.7673e-01, inner_loss=7.8324e+00\n",
      "[eval] sample 54 iter 30 grad_norm=6.4923e-01, inner_loss=7.7884e+00\n",
      "[eval] sample 54 iter 40 grad_norm=6.2329e-01, inner_loss=7.7478e+00\n",
      "[eval] sample 55 iter 0 grad_norm=7.1795e-01, inner_loss=7.7753e+00\n",
      "[eval] sample 55 iter 10 grad_norm=6.8696e-01, inner_loss=7.7258e+00\n",
      "[eval] sample 55 iter 20 grad_norm=6.5776e-01, inner_loss=7.6805e+00\n",
      "[eval] sample 55 iter 30 grad_norm=6.3023e-01, inner_loss=7.6390e+00\n",
      "[eval] sample 55 iter 40 grad_norm=6.0426e-01, inner_loss=7.6008e+00\n",
      "[eval] sample 56 iter 0 grad_norm=7.6625e-01, inner_loss=7.7771e+00\n",
      "[eval] sample 56 iter 10 grad_norm=7.2938e-01, inner_loss=7.7211e+00\n",
      "[eval] sample 56 iter 20 grad_norm=6.9472e-01, inner_loss=7.6703e+00\n",
      "[eval] sample 56 iter 30 grad_norm=6.6211e-01, inner_loss=7.6242e+00\n",
      "[eval] sample 56 iter 40 grad_norm=6.3142e-01, inner_loss=7.5822e+00\n",
      "[eval] sample 57 iter 0 grad_norm=4.3740e-01, inner_loss=7.0391e+00\n",
      "[eval] sample 57 iter 10 grad_norm=4.1661e-01, inner_loss=7.0209e+00\n",
      "[eval] sample 57 iter 20 grad_norm=3.9711e-01, inner_loss=7.0043e+00\n",
      "[eval] sample 57 iter 30 grad_norm=3.7883e-01, inner_loss=6.9892e+00\n",
      "[eval] sample 57 iter 40 grad_norm=3.6167e-01, inner_loss=6.9755e+00\n",
      "[eval] sample 58 iter 0 grad_norm=6.3413e-01, inner_loss=7.5691e+00\n",
      "[eval] sample 58 iter 10 grad_norm=6.0479e-01, inner_loss=7.5307e+00\n",
      "[eval] sample 58 iter 20 grad_norm=5.7732e-01, inner_loss=7.4957e+00\n",
      "[eval] sample 58 iter 30 grad_norm=5.5159e-01, inner_loss=7.4637e+00\n",
      "[eval] sample 58 iter 40 grad_norm=5.2746e-01, inner_loss=7.4346e+00\n",
      "[eval] sample 59 iter 0 grad_norm=4.0820e-01, inner_loss=7.0120e+00\n",
      "[eval] sample 59 iter 10 grad_norm=3.8894e-01, inner_loss=6.9961e+00\n",
      "[eval] sample 59 iter 20 grad_norm=3.7090e-01, inner_loss=6.9816e+00\n",
      "[eval] sample 59 iter 30 grad_norm=3.5399e-01, inner_loss=6.9685e+00\n",
      "[eval] sample 59 iter 40 grad_norm=3.3814e-01, inner_loss=6.9565e+00\n",
      "[eval] sample 60 iter 0 grad_norm=7.9019e-01, inner_loss=7.7890e+00\n",
      "[eval] sample 60 iter 10 grad_norm=7.5048e-01, inner_loss=7.7295e+00\n",
      "[eval] sample 60 iter 20 grad_norm=7.1331e-01, inner_loss=7.6758e+00\n",
      "[eval] sample 60 iter 30 grad_norm=6.7851e-01, inner_loss=7.6273e+00\n",
      "[eval] sample 60 iter 40 grad_norm=6.4589e-01, inner_loss=7.5833e+00\n",
      "[eval] sample 61 iter 0 grad_norm=6.9237e-01, inner_loss=7.6724e+00\n",
      "[eval] sample 61 iter 10 grad_norm=6.5936e-01, inner_loss=7.6267e+00\n",
      "[eval] sample 61 iter 20 grad_norm=6.2848e-01, inner_loss=7.5851e+00\n",
      "[eval] sample 61 iter 30 grad_norm=5.9956e-01, inner_loss=7.5473e+00\n",
      "[eval] sample 61 iter 40 grad_norm=5.7247e-01, inner_loss=7.5129e+00\n",
      "[eval] sample 62 iter 0 grad_norm=4.0148e-01, inner_loss=7.0242e+00\n",
      "[eval] sample 62 iter 10 grad_norm=3.8325e-01, inner_loss=7.0088e+00\n",
      "[eval] sample 62 iter 20 grad_norm=3.6618e-01, inner_loss=6.9948e+00\n",
      "[eval] sample 62 iter 30 grad_norm=3.5018e-01, inner_loss=6.9819e+00\n",
      "[eval] sample 62 iter 40 grad_norm=3.3517e-01, inner_loss=6.9701e+00\n",
      "[eval] sample 63 iter 0 grad_norm=5.9543e-01, inner_loss=7.4786e+00\n",
      "[eval] sample 63 iter 10 grad_norm=5.6917e-01, inner_loss=7.4446e+00\n",
      "[eval] sample 63 iter 20 grad_norm=5.4459e-01, inner_loss=7.4135e+00\n",
      "[eval] sample 63 iter 30 grad_norm=5.2155e-01, inner_loss=7.3851e+00\n",
      "[eval] sample 63 iter 40 grad_norm=4.9993e-01, inner_loss=7.3589e+00\n",
      "[eval] sample 64 iter 0 grad_norm=7.1348e-01, inner_loss=7.7755e+00\n",
      "[eval] sample 64 iter 10 grad_norm=6.8032e-01, inner_loss=7.7269e+00\n",
      "[eval] sample 64 iter 20 grad_norm=6.4929e-01, inner_loss=7.6826e+00\n",
      "[eval] sample 64 iter 30 grad_norm=6.2024e-01, inner_loss=7.6422e+00\n",
      "[eval] sample 64 iter 40 grad_norm=5.9301e-01, inner_loss=7.6053e+00\n",
      "[eval] sample 65 iter 0 grad_norm=3.9680e-01, inner_loss=6.9961e+00\n",
      "[eval] sample 65 iter 10 grad_norm=3.7618e-01, inner_loss=6.9811e+00\n",
      "[eval] sample 65 iter 20 grad_norm=3.5702e-01, inner_loss=6.9676e+00\n",
      "[eval] sample 65 iter 30 grad_norm=3.3923e-01, inner_loss=6.9555e+00\n",
      "[eval] sample 65 iter 40 grad_norm=3.2267e-01, inner_loss=6.9445e+00\n",
      "[eval] sample 66 iter 0 grad_norm=5.4316e-01, inner_loss=7.3686e+00\n",
      "[eval] sample 66 iter 10 grad_norm=5.1844e-01, inner_loss=7.3404e+00\n",
      "[eval] sample 66 iter 20 grad_norm=4.9526e-01, inner_loss=7.3147e+00\n",
      "[eval] sample 66 iter 30 grad_norm=4.7350e-01, inner_loss=7.2912e+00\n",
      "[eval] sample 66 iter 40 grad_norm=4.5307e-01, inner_loss=7.2697e+00\n",
      "[eval] sample 67 iter 0 grad_norm=6.2544e-01, inner_loss=7.4529e+00\n",
      "[eval] sample 67 iter 10 grad_norm=5.9558e-01, inner_loss=7.4155e+00\n",
      "[eval] sample 67 iter 20 grad_norm=5.6756e-01, inner_loss=7.3816e+00\n",
      "[eval] sample 67 iter 30 grad_norm=5.4123e-01, inner_loss=7.3508e+00\n",
      "[eval] sample 67 iter 40 grad_norm=5.1650e-01, inner_loss=7.3228e+00\n",
      "[eval] sample 68 iter 0 grad_norm=7.5637e-01, inner_loss=7.8528e+00\n",
      "[eval] sample 68 iter 10 grad_norm=7.1954e-01, inner_loss=7.7982e+00\n",
      "[eval] sample 68 iter 20 grad_norm=6.8533e-01, inner_loss=7.7488e+00\n",
      "[eval] sample 68 iter 30 grad_norm=6.5349e-01, inner_loss=7.7039e+00\n",
      "[eval] sample 68 iter 40 grad_norm=6.2385e-01, inner_loss=7.6630e+00\n",
      "[eval] sample 69 iter 0 grad_norm=7.5335e-01, inner_loss=7.8190e+00\n",
      "[eval] sample 69 iter 10 grad_norm=7.1717e-01, inner_loss=7.7648e+00\n",
      "[eval] sample 69 iter 20 grad_norm=6.8316e-01, inner_loss=7.7157e+00\n",
      "[eval] sample 69 iter 30 grad_norm=6.5117e-01, inner_loss=7.6711e+00\n",
      "[eval] sample 69 iter 40 grad_norm=6.2106e-01, inner_loss=7.6305e+00\n",
      "[eval] sample 70 iter 0 grad_norm=6.7558e-01, inner_loss=7.5771e+00\n",
      "[eval] sample 70 iter 10 grad_norm=6.4285e-01, inner_loss=7.5336e+00\n",
      "[eval] sample 70 iter 20 grad_norm=6.1216e-01, inner_loss=7.4941e+00\n",
      "[eval] sample 70 iter 30 grad_norm=5.8339e-01, inner_loss=7.4583e+00\n",
      "[eval] sample 70 iter 40 grad_norm=5.5640e-01, inner_loss=7.4258e+00\n",
      "[eval] sample 71 iter 0 grad_norm=9.7556e-01, inner_loss=8.3811e+00\n",
      "[eval] sample 71 iter 10 grad_norm=9.2703e-01, inner_loss=8.2904e+00\n",
      "[eval] sample 71 iter 20 grad_norm=8.8143e-01, inner_loss=8.2085e+00\n",
      "[eval] sample 71 iter 30 grad_norm=8.3857e-01, inner_loss=8.1344e+00\n",
      "[eval] sample 71 iter 40 grad_norm=7.9827e-01, inner_loss=8.0672e+00\n",
      "[eval] sample 72 iter 0 grad_norm=6.7890e-01, inner_loss=7.5593e+00\n",
      "[eval] sample 72 iter 10 grad_norm=6.4429e-01, inner_loss=7.5154e+00\n",
      "[eval] sample 72 iter 20 grad_norm=6.1194e-01, inner_loss=7.4758e+00\n",
      "[eval] sample 72 iter 30 grad_norm=5.8168e-01, inner_loss=7.4401e+00\n",
      "[eval] sample 72 iter 40 grad_norm=5.5336e-01, inner_loss=7.4079e+00\n",
      "[eval] sample 73 iter 0 grad_norm=7.2432e-01, inner_loss=7.5989e+00\n",
      "[eval] sample 73 iter 10 grad_norm=6.8415e-01, inner_loss=7.5492e+00\n",
      "[eval] sample 73 iter 20 grad_norm=6.4682e-01, inner_loss=7.5048e+00\n",
      "[eval] sample 73 iter 30 grad_norm=6.1210e-01, inner_loss=7.4651e+00\n",
      "[eval] sample 73 iter 40 grad_norm=5.7981e-01, inner_loss=7.4295e+00\n",
      "[eval] sample 74 iter 0 grad_norm=5.9103e-01, inner_loss=7.2935e+00\n",
      "[eval] sample 74 iter 10 grad_norm=5.6169e-01, inner_loss=7.2602e+00\n",
      "[eval] sample 74 iter 20 grad_norm=5.3424e-01, inner_loss=7.2302e+00\n",
      "[eval] sample 74 iter 30 grad_norm=5.0853e-01, inner_loss=7.2029e+00\n",
      "[eval] sample 74 iter 40 grad_norm=4.8444e-01, inner_loss=7.1782e+00\n",
      "[eval] sample 75 iter 0 grad_norm=5.8397e-01, inner_loss=7.4065e+00\n",
      "[eval] sample 75 iter 10 grad_norm=5.5560e-01, inner_loss=7.3740e+00\n",
      "[eval] sample 75 iter 20 grad_norm=5.2916e-01, inner_loss=7.3445e+00\n",
      "[eval] sample 75 iter 30 grad_norm=5.0451e-01, inner_loss=7.3177e+00\n",
      "[eval] sample 75 iter 40 grad_norm=4.8150e-01, inner_loss=7.2934e+00\n",
      "[eval] sample 76 iter 0 grad_norm=5.0964e-01, inner_loss=7.2423e+00\n",
      "[eval] sample 76 iter 10 grad_norm=4.8676e-01, inner_loss=7.2174e+00\n",
      "[eval] sample 76 iter 20 grad_norm=4.6531e-01, inner_loss=7.1947e+00\n",
      "[eval] sample 76 iter 30 grad_norm=4.4518e-01, inner_loss=7.1740e+00\n",
      "[eval] sample 76 iter 40 grad_norm=4.2626e-01, inner_loss=7.1550e+00\n",
      "[eval] sample 77 iter 0 grad_norm=5.3893e-01, inner_loss=7.2651e+00\n",
      "[eval] sample 77 iter 10 grad_norm=5.1462e-01, inner_loss=7.2373e+00\n",
      "[eval] sample 77 iter 20 grad_norm=4.9166e-01, inner_loss=7.2120e+00\n",
      "[eval] sample 77 iter 30 grad_norm=4.6998e-01, inner_loss=7.1888e+00\n",
      "[eval] sample 77 iter 40 grad_norm=4.4948e-01, inner_loss=7.1676e+00\n",
      "[eval] sample 78 iter 0 grad_norm=5.6263e-01, inner_loss=7.2221e+00\n",
      "[eval] sample 78 iter 10 grad_norm=5.3114e-01, inner_loss=7.1921e+00\n",
      "[eval] sample 78 iter 20 grad_norm=5.0184e-01, inner_loss=7.1653e+00\n",
      "[eval] sample 78 iter 30 grad_norm=4.7458e-01, inner_loss=7.1415e+00\n",
      "[eval] sample 78 iter 40 grad_norm=4.4920e-01, inner_loss=7.1201e+00\n",
      "[eval] sample 79 iter 0 grad_norm=9.0142e-01, inner_loss=7.9730e+00\n",
      "[eval] sample 79 iter 10 grad_norm=8.5393e-01, inner_loss=7.8958e+00\n",
      "[eval] sample 79 iter 20 grad_norm=8.0953e-01, inner_loss=7.8264e+00\n",
      "[eval] sample 79 iter 30 grad_norm=7.6799e-01, inner_loss=7.7641e+00\n",
      "[eval] sample 79 iter 40 grad_norm=7.2911e-01, inner_loss=7.7079e+00\n",
      "[eval] sample 80 iter 0 grad_norm=6.3804e-01, inner_loss=7.5130e+00\n",
      "[eval] sample 80 iter 10 grad_norm=6.0813e-01, inner_loss=7.4741e+00\n",
      "[eval] sample 80 iter 20 grad_norm=5.8016e-01, inner_loss=7.4388e+00\n",
      "[eval] sample 80 iter 30 grad_norm=5.5398e-01, inner_loss=7.4065e+00\n",
      "[eval] sample 80 iter 40 grad_norm=5.2945e-01, inner_loss=7.3771e+00\n",
      "[eval] sample 81 iter 0 grad_norm=6.9722e-01, inner_loss=7.5924e+00\n",
      "[eval] sample 81 iter 10 grad_norm=6.6317e-01, inner_loss=7.5461e+00\n",
      "[eval] sample 81 iter 20 grad_norm=6.3127e-01, inner_loss=7.5041e+00\n",
      "[eval] sample 81 iter 30 grad_norm=6.0135e-01, inner_loss=7.4660e+00\n",
      "[eval] sample 81 iter 40 grad_norm=5.7327e-01, inner_loss=7.4315e+00\n",
      "[eval] sample 82 iter 0 grad_norm=8.4449e-01, inner_loss=7.9221e+00\n",
      "[eval] sample 82 iter 10 grad_norm=8.0153e-01, inner_loss=7.8542e+00\n",
      "[eval] sample 82 iter 20 grad_norm=7.6128e-01, inner_loss=7.7930e+00\n",
      "[eval] sample 82 iter 30 grad_norm=7.2354e-01, inner_loss=7.7378e+00\n",
      "[eval] sample 82 iter 40 grad_norm=6.8814e-01, inner_loss=7.6879e+00\n",
      "[eval] sample 83 iter 0 grad_norm=5.9510e-01, inner_loss=7.3586e+00\n",
      "[eval] sample 83 iter 10 grad_norm=5.6700e-01, inner_loss=7.3248e+00\n",
      "[eval] sample 83 iter 20 grad_norm=5.4063e-01, inner_loss=7.2941e+00\n",
      "[eval] sample 83 iter 30 grad_norm=5.1585e-01, inner_loss=7.2661e+00\n",
      "[eval] sample 83 iter 40 grad_norm=4.9257e-01, inner_loss=7.2406e+00\n",
      "[eval] sample 84 iter 0 grad_norm=5.5914e-01, inner_loss=7.4766e+00\n",
      "[eval] sample 84 iter 10 grad_norm=5.3330e-01, inner_loss=7.4467e+00\n",
      "[eval] sample 84 iter 20 grad_norm=5.0924e-01, inner_loss=7.4195e+00\n",
      "[eval] sample 84 iter 30 grad_norm=4.8682e-01, inner_loss=7.3946e+00\n",
      "[eval] sample 84 iter 40 grad_norm=4.6591e-01, inner_loss=7.3719e+00\n",
      "[eval] sample 85 iter 0 grad_norm=8.3599e-01, inner_loss=7.8776e+00\n",
      "[eval] sample 85 iter 10 grad_norm=7.9364e-01, inner_loss=7.8111e+00\n",
      "[eval] sample 85 iter 20 grad_norm=7.5399e-01, inner_loss=7.7511e+00\n",
      "[eval] sample 85 iter 30 grad_norm=7.1684e-01, inner_loss=7.6969e+00\n",
      "[eval] sample 85 iter 40 grad_norm=6.8202e-01, inner_loss=7.6478e+00\n",
      "[eval] sample 86 iter 0 grad_norm=7.5995e-01, inner_loss=7.6593e+00\n",
      "[eval] sample 86 iter 10 grad_norm=7.2109e-01, inner_loss=7.6044e+00\n",
      "[eval] sample 86 iter 20 grad_norm=6.8477e-01, inner_loss=7.5549e+00\n",
      "[eval] sample 86 iter 30 grad_norm=6.5080e-01, inner_loss=7.5102e+00\n",
      "[eval] sample 86 iter 40 grad_norm=6.1901e-01, inner_loss=7.4698e+00\n",
      "[eval] sample 87 iter 0 grad_norm=6.7249e-01, inner_loss=7.5770e+00\n",
      "[eval] sample 87 iter 10 grad_norm=6.3960e-01, inner_loss=7.5338e+00\n",
      "[eval] sample 87 iter 20 grad_norm=6.0885e-01, inner_loss=7.4948e+00\n",
      "[eval] sample 87 iter 30 grad_norm=5.8006e-01, inner_loss=7.4594e+00\n",
      "[eval] sample 87 iter 40 grad_norm=5.5310e-01, inner_loss=7.4272e+00\n",
      "[eval] sample 88 iter 0 grad_norm=6.6823e-01, inner_loss=7.5920e+00\n",
      "[eval] sample 88 iter 10 grad_norm=6.3539e-01, inner_loss=7.5494e+00\n",
      "[eval] sample 88 iter 20 grad_norm=6.0467e-01, inner_loss=7.5109e+00\n",
      "[eval] sample 88 iter 30 grad_norm=5.7589e-01, inner_loss=7.4760e+00\n",
      "[eval] sample 88 iter 40 grad_norm=5.4893e-01, inner_loss=7.4443e+00\n",
      "[eval] sample 89 iter 0 grad_norm=6.2739e-01, inner_loss=7.3470e+00\n",
      "[eval] sample 89 iter 10 grad_norm=5.9560e-01, inner_loss=7.3095e+00\n",
      "[eval] sample 89 iter 20 grad_norm=5.6585e-01, inner_loss=7.2757e+00\n",
      "[eval] sample 89 iter 30 grad_norm=5.3800e-01, inner_loss=7.2452e+00\n",
      "[eval] sample 89 iter 40 grad_norm=5.1192e-01, inner_loss=7.2176e+00\n",
      "[eval] sample 90 iter 0 grad_norm=6.1340e-01, inner_loss=7.4995e+00\n",
      "[eval] sample 90 iter 10 grad_norm=5.8339e-01, inner_loss=7.4636e+00\n",
      "[eval] sample 90 iter 20 grad_norm=5.5536e-01, inner_loss=7.4311e+00\n",
      "[eval] sample 90 iter 30 grad_norm=5.2916e-01, inner_loss=7.4016e+00\n",
      "[eval] sample 90 iter 40 grad_norm=5.0467e-01, inner_loss=7.3749e+00\n",
      "[eval] sample 91 iter 0 grad_norm=5.3198e-01, inner_loss=7.2993e+00\n",
      "[eval] sample 91 iter 10 grad_norm=5.0527e-01, inner_loss=7.2723e+00\n",
      "[eval] sample 91 iter 20 grad_norm=4.8042e-01, inner_loss=7.2480e+00\n",
      "[eval] sample 91 iter 30 grad_norm=4.5730e-01, inner_loss=7.2259e+00\n",
      "[eval] sample 91 iter 40 grad_norm=4.3577e-01, inner_loss=7.2060e+00\n",
      "[eval] sample 92 iter 0 grad_norm=3.7420e-01, inner_loss=6.9686e+00\n",
      "[eval] sample 92 iter 10 grad_norm=3.5549e-01, inner_loss=6.9553e+00\n",
      "[eval] sample 92 iter 20 grad_norm=3.3807e-01, inner_loss=6.9432e+00\n",
      "[eval] sample 92 iter 30 grad_norm=3.2183e-01, inner_loss=6.9323e+00\n",
      "[eval] sample 92 iter 40 grad_norm=3.0670e-01, inner_loss=6.9224e+00\n",
      "[eval] sample 93 iter 0 grad_norm=8.2758e-01, inner_loss=7.7688e+00\n",
      "[eval] sample 93 iter 10 grad_norm=7.8195e-01, inner_loss=7.7039e+00\n",
      "[eval] sample 93 iter 20 grad_norm=7.3945e-01, inner_loss=7.6459e+00\n",
      "[eval] sample 93 iter 30 grad_norm=6.9984e-01, inner_loss=7.5940e+00\n",
      "[eval] sample 93 iter 40 grad_norm=6.6291e-01, inner_loss=7.5475e+00\n",
      "[eval] sample 94 iter 0 grad_norm=7.3964e-01, inner_loss=7.5027e+00\n",
      "[eval] sample 94 iter 10 grad_norm=6.9973e-01, inner_loss=7.4508e+00\n",
      "[eval] sample 94 iter 20 grad_norm=6.6246e-01, inner_loss=7.4043e+00\n",
      "[eval] sample 94 iter 30 grad_norm=6.2764e-01, inner_loss=7.3625e+00\n",
      "[eval] sample 94 iter 40 grad_norm=5.9508e-01, inner_loss=7.3251e+00\n",
      "[eval] sample 95 iter 0 grad_norm=7.1829e-01, inner_loss=7.6753e+00\n",
      "[eval] sample 95 iter 10 grad_norm=6.8060e-01, inner_loss=7.6263e+00\n",
      "[eval] sample 95 iter 20 grad_norm=6.4560e-01, inner_loss=7.5822e+00\n",
      "[eval] sample 95 iter 30 grad_norm=6.1308e-01, inner_loss=7.5425e+00\n",
      "[eval] sample 95 iter 40 grad_norm=5.8283e-01, inner_loss=7.5067e+00\n",
      "[eval] sample 96 iter 0 grad_norm=4.8512e-01, inner_loss=7.1080e+00\n",
      "[eval] sample 96 iter 10 grad_norm=4.6155e-01, inner_loss=7.0856e+00\n",
      "[eval] sample 96 iter 20 grad_norm=4.3947e-01, inner_loss=7.0653e+00\n",
      "[eval] sample 96 iter 30 grad_norm=4.1878e-01, inner_loss=7.0468e+00\n",
      "[eval] sample 96 iter 40 grad_norm=3.9937e-01, inner_loss=7.0300e+00\n",
      "[eval] sample 97 iter 0 grad_norm=7.1223e-01, inner_loss=7.6625e+00\n",
      "[eval] sample 97 iter 10 grad_norm=6.7646e-01, inner_loss=7.6141e+00\n",
      "[eval] sample 97 iter 20 grad_norm=6.4313e-01, inner_loss=7.5705e+00\n",
      "[eval] sample 97 iter 30 grad_norm=6.1204e-01, inner_loss=7.5310e+00\n",
      "[eval] sample 97 iter 40 grad_norm=5.8303e-01, inner_loss=7.4953e+00\n",
      "[eval] sample 98 iter 0 grad_norm=6.1534e-01, inner_loss=7.3884e+00\n",
      "[eval] sample 98 iter 10 grad_norm=5.8397e-01, inner_loss=7.3524e+00\n",
      "[eval] sample 98 iter 20 grad_norm=5.5463e-01, inner_loss=7.3199e+00\n",
      "[eval] sample 98 iter 30 grad_norm=5.2715e-01, inner_loss=7.2906e+00\n",
      "[eval] sample 98 iter 40 grad_norm=5.0143e-01, inner_loss=7.2641e+00\n",
      "[eval] sample 99 iter 0 grad_norm=7.4251e-01, inner_loss=7.8231e+00\n",
      "[eval] sample 99 iter 10 grad_norm=7.0952e-01, inner_loss=7.7703e+00\n",
      "[eval] sample 99 iter 20 grad_norm=6.7856e-01, inner_loss=7.7221e+00\n",
      "[eval] sample 99 iter 30 grad_norm=6.4947e-01, inner_loss=7.6779e+00\n",
      "[eval] sample 99 iter 40 grad_norm=6.2210e-01, inner_loss=7.6374e+00\n",
      "[eval] sample 100 iter 0 grad_norm=4.1392e-01, inner_loss=7.0814e+00\n",
      "[eval] sample 100 iter 10 grad_norm=3.9597e-01, inner_loss=7.0650e+00\n",
      "[eval] sample 100 iter 20 grad_norm=3.7915e-01, inner_loss=7.0500e+00\n",
      "[eval] sample 100 iter 30 grad_norm=3.6336e-01, inner_loss=7.0362e+00\n",
      "[eval] sample 100 iter 40 grad_norm=3.4852e-01, inner_loss=7.0235e+00\n",
      "[eval] sample 101 iter 0 grad_norm=7.0774e-01, inner_loss=7.6757e+00\n",
      "[eval] sample 101 iter 10 grad_norm=6.7370e-01, inner_loss=7.6279e+00\n",
      "[eval] sample 101 iter 20 grad_norm=6.4171e-01, inner_loss=7.5846e+00\n",
      "[eval] sample 101 iter 30 grad_norm=6.1163e-01, inner_loss=7.5452e+00\n",
      "[eval] sample 101 iter 40 grad_norm=5.8335e-01, inner_loss=7.5094e+00\n",
      "[eval] sample 102 iter 0 grad_norm=6.9787e-01, inner_loss=7.6416e+00\n",
      "[eval] sample 102 iter 10 grad_norm=6.6345e-01, inner_loss=7.5951e+00\n",
      "[eval] sample 102 iter 20 grad_norm=6.3118e-01, inner_loss=7.5532e+00\n",
      "[eval] sample 102 iter 30 grad_norm=6.0091e-01, inner_loss=7.5151e+00\n",
      "[eval] sample 102 iter 40 grad_norm=5.7250e-01, inner_loss=7.4806e+00\n",
      "[eval] sample 103 iter 0 grad_norm=6.9950e-01, inner_loss=7.6132e+00\n",
      "[eval] sample 103 iter 10 grad_norm=6.6536e-01, inner_loss=7.5666e+00\n",
      "[eval] sample 103 iter 20 grad_norm=6.3331e-01, inner_loss=7.5243e+00\n",
      "[eval] sample 103 iter 30 grad_norm=6.0320e-01, inner_loss=7.4860e+00\n",
      "[eval] sample 103 iter 40 grad_norm=5.7490e-01, inner_loss=7.4512e+00\n",
      "[eval] sample 104 iter 0 grad_norm=6.0814e-01, inner_loss=7.4954e+00\n",
      "[eval] sample 104 iter 10 grad_norm=5.8152e-01, inner_loss=7.4600e+00\n",
      "[eval] sample 104 iter 20 grad_norm=5.5649e-01, inner_loss=7.4275e+00\n",
      "[eval] sample 104 iter 30 grad_norm=5.3295e-01, inner_loss=7.3978e+00\n",
      "[eval] sample 104 iter 40 grad_norm=5.1078e-01, inner_loss=7.3705e+00\n",
      "[eval] sample 105 iter 0 grad_norm=5.4775e-01, inner_loss=7.3619e+00\n",
      "[eval] sample 105 iter 10 grad_norm=5.2450e-01, inner_loss=7.3331e+00\n",
      "[eval] sample 105 iter 20 grad_norm=5.0263e-01, inner_loss=7.3067e+00\n",
      "[eval] sample 105 iter 30 grad_norm=4.8205e-01, inner_loss=7.2824e+00\n",
      "[eval] sample 105 iter 40 grad_norm=4.6267e-01, inner_loss=7.2601e+00\n",
      "[eval] sample 106 iter 0 grad_norm=6.9879e-01, inner_loss=7.6428e+00\n",
      "[eval] sample 106 iter 10 grad_norm=6.6456e-01, inner_loss=7.5962e+00\n",
      "[eval] sample 106 iter 20 grad_norm=6.3239e-01, inner_loss=7.5541e+00\n",
      "[eval] sample 106 iter 30 grad_norm=6.0215e-01, inner_loss=7.5159e+00\n",
      "[eval] sample 106 iter 40 grad_norm=5.7371e-01, inner_loss=7.4813e+00\n",
      "[eval] sample 107 iter 0 grad_norm=5.4737e-01, inner_loss=7.2248e+00\n",
      "[eval] sample 107 iter 10 grad_norm=5.2056e-01, inner_loss=7.1963e+00\n",
      "[eval] sample 107 iter 20 grad_norm=4.9544e-01, inner_loss=7.1704e+00\n",
      "[eval] sample 107 iter 30 grad_norm=4.7190e-01, inner_loss=7.1470e+00\n",
      "[eval] sample 107 iter 40 grad_norm=4.4981e-01, inner_loss=7.1257e+00\n",
      "[eval] sample 108 iter 0 grad_norm=6.0253e-01, inner_loss=7.4156e+00\n",
      "[eval] sample 108 iter 10 grad_norm=5.7219e-01, inner_loss=7.3810e+00\n",
      "[eval] sample 108 iter 20 grad_norm=5.4393e-01, inner_loss=7.3498e+00\n",
      "[eval] sample 108 iter 30 grad_norm=5.1759e-01, inner_loss=7.3215e+00\n",
      "[eval] sample 108 iter 40 grad_norm=4.9302e-01, inner_loss=7.2960e+00\n",
      "[eval] sample 109 iter 0 grad_norm=5.0233e-01, inner_loss=7.2871e+00\n",
      "[eval] sample 109 iter 10 grad_norm=4.8080e-01, inner_loss=7.2629e+00\n",
      "[eval] sample 109 iter 20 grad_norm=4.6062e-01, inner_loss=7.2407e+00\n",
      "[eval] sample 109 iter 30 grad_norm=4.4167e-01, inner_loss=7.2203e+00\n",
      "[eval] sample 109 iter 40 grad_norm=4.2387e-01, inner_loss=7.2015e+00\n",
      "[eval] sample 110 iter 0 grad_norm=5.8033e-01, inner_loss=7.4484e+00\n",
      "[eval] sample 110 iter 10 grad_norm=5.5073e-01, inner_loss=7.4163e+00\n",
      "[eval] sample 110 iter 20 grad_norm=5.2327e-01, inner_loss=7.3874e+00\n",
      "[eval] sample 110 iter 30 grad_norm=4.9777e-01, inner_loss=7.3613e+00\n",
      "[eval] sample 110 iter 40 grad_norm=4.7409e-01, inner_loss=7.3377e+00\n",
      "[eval] sample 111 iter 0 grad_norm=6.2527e-01, inner_loss=7.3861e+00\n",
      "[eval] sample 111 iter 10 grad_norm=5.9366e-01, inner_loss=7.3489e+00\n",
      "[eval] sample 111 iter 20 grad_norm=5.6411e-01, inner_loss=7.3153e+00\n",
      "[eval] sample 111 iter 30 grad_norm=5.3648e-01, inner_loss=7.2850e+00\n",
      "[eval] sample 111 iter 40 grad_norm=5.1063e-01, inner_loss=7.2575e+00\n",
      "[eval] sample 112 iter 0 grad_norm=5.5565e-01, inner_loss=7.3092e+00\n",
      "[eval] sample 112 iter 10 grad_norm=5.2904e-01, inner_loss=7.2797e+00\n",
      "[eval] sample 112 iter 20 grad_norm=5.0415e-01, inner_loss=7.2530e+00\n",
      "[eval] sample 112 iter 30 grad_norm=4.8086e-01, inner_loss=7.2287e+00\n",
      "[eval] sample 112 iter 40 grad_norm=4.5906e-01, inner_loss=7.2065e+00\n",
      "[eval] sample 113 iter 0 grad_norm=5.9752e-01, inner_loss=7.4831e+00\n",
      "[eval] sample 113 iter 10 grad_norm=5.7041e-01, inner_loss=7.4489e+00\n",
      "[eval] sample 113 iter 20 grad_norm=5.4511e-01, inner_loss=7.4177e+00\n",
      "[eval] sample 113 iter 30 grad_norm=5.2148e-01, inner_loss=7.3892e+00\n",
      "[eval] sample 113 iter 40 grad_norm=4.9939e-01, inner_loss=7.3631e+00\n",
      "[eval] sample 114 iter 0 grad_norm=5.8851e-01, inner_loss=7.3063e+00\n",
      "[eval] sample 114 iter 10 grad_norm=5.5984e-01, inner_loss=7.2732e+00\n",
      "[eval] sample 114 iter 20 grad_norm=5.3299e-01, inner_loss=7.2433e+00\n",
      "[eval] sample 114 iter 30 grad_norm=5.0782e-01, inner_loss=7.2162e+00\n",
      "[eval] sample 114 iter 40 grad_norm=4.8423e-01, inner_loss=7.1915e+00\n",
      "[eval] sample 115 iter 0 grad_norm=4.7082e-01, inner_loss=7.2276e+00\n",
      "[eval] sample 115 iter 10 grad_norm=4.4915e-01, inner_loss=7.2064e+00\n",
      "[eval] sample 115 iter 20 grad_norm=4.2898e-01, inner_loss=7.1871e+00\n",
      "[eval] sample 115 iter 30 grad_norm=4.1020e-01, inner_loss=7.1694e+00\n",
      "[eval] sample 115 iter 40 grad_norm=3.9269e-01, inner_loss=7.1533e+00\n",
      "[eval] sample 116 iter 0 grad_norm=4.5860e-01, inner_loss=7.1385e+00\n",
      "[eval] sample 116 iter 10 grad_norm=4.3723e-01, inner_loss=7.1184e+00\n",
      "[eval] sample 116 iter 20 grad_norm=4.1732e-01, inner_loss=7.1001e+00\n",
      "[eval] sample 116 iter 30 grad_norm=3.9874e-01, inner_loss=7.0834e+00\n",
      "[eval] sample 116 iter 40 grad_norm=3.8140e-01, inner_loss=7.0682e+00\n",
      "[eval] sample 117 iter 0 grad_norm=7.0255e-01, inner_loss=7.6015e+00\n",
      "[eval] sample 117 iter 10 grad_norm=6.6774e-01, inner_loss=7.5544e+00\n",
      "[eval] sample 117 iter 20 grad_norm=6.3514e-01, inner_loss=7.5119e+00\n",
      "[eval] sample 117 iter 30 grad_norm=6.0460e-01, inner_loss=7.4734e+00\n",
      "[eval] sample 117 iter 40 grad_norm=5.7597e-01, inner_loss=7.4385e+00\n",
      "[eval] sample 118 iter 0 grad_norm=4.5957e-01, inner_loss=7.1808e+00\n",
      "[eval] sample 118 iter 10 grad_norm=4.3968e-01, inner_loss=7.1606e+00\n",
      "[eval] sample 118 iter 20 grad_norm=4.2107e-01, inner_loss=7.1420e+00\n",
      "[eval] sample 118 iter 30 grad_norm=4.0362e-01, inner_loss=7.1250e+00\n",
      "[eval] sample 118 iter 40 grad_norm=3.8726e-01, inner_loss=7.1093e+00\n",
      "[eval] sample 119 iter 0 grad_norm=7.7014e-01, inner_loss=7.7828e+00\n",
      "[eval] sample 119 iter 10 grad_norm=7.3389e-01, inner_loss=7.7262e+00\n",
      "[eval] sample 119 iter 20 grad_norm=6.9976e-01, inner_loss=7.6747e+00\n",
      "[eval] sample 119 iter 30 grad_norm=6.6762e-01, inner_loss=7.6278e+00\n",
      "[eval] sample 119 iter 40 grad_norm=6.3733e-01, inner_loss=7.5852e+00\n",
      "[eval] sample 120 iter 0 grad_norm=8.2846e-01, inner_loss=8.1276e+00\n",
      "[eval] sample 120 iter 10 grad_norm=7.9213e-01, inner_loss=8.0619e+00\n",
      "[eval] sample 120 iter 20 grad_norm=7.5804e-01, inner_loss=8.0017e+00\n",
      "[eval] sample 120 iter 30 grad_norm=7.2602e-01, inner_loss=7.9465e+00\n",
      "[eval] sample 120 iter 40 grad_norm=6.9590e-01, inner_loss=7.8959e+00\n",
      "[eval] sample 121 iter 0 grad_norm=5.8264e-01, inner_loss=7.3817e+00\n",
      "[eval] sample 121 iter 10 grad_norm=5.5675e-01, inner_loss=7.3492e+00\n",
      "[eval] sample 121 iter 20 grad_norm=5.3238e-01, inner_loss=7.3195e+00\n",
      "[eval] sample 121 iter 30 grad_norm=5.0944e-01, inner_loss=7.2923e+00\n",
      "[eval] sample 121 iter 40 grad_norm=4.8780e-01, inner_loss=7.2674e+00\n",
      "[eval] sample 122 iter 0 grad_norm=4.9871e-01, inner_loss=7.1614e+00\n",
      "[eval] sample 122 iter 10 grad_norm=4.7492e-01, inner_loss=7.1377e+00\n",
      "[eval] sample 122 iter 20 grad_norm=4.5262e-01, inner_loss=7.1161e+00\n",
      "[eval] sample 122 iter 30 grad_norm=4.3171e-01, inner_loss=7.0965e+00\n",
      "[eval] sample 122 iter 40 grad_norm=4.1208e-01, inner_loss=7.0787e+00\n",
      "[eval] sample 123 iter 0 grad_norm=5.1779e-01, inner_loss=7.3466e+00\n",
      "[eval] sample 123 iter 10 grad_norm=4.9359e-01, inner_loss=7.3210e+00\n",
      "[eval] sample 123 iter 20 grad_norm=4.7104e-01, inner_loss=7.2977e+00\n",
      "[eval] sample 123 iter 30 grad_norm=4.5000e-01, inner_loss=7.2764e+00\n",
      "[eval] sample 123 iter 40 grad_norm=4.3035e-01, inner_loss=7.2570e+00\n",
      "[eval] sample 124 iter 0 grad_norm=7.0781e-01, inner_loss=7.6069e+00\n",
      "[eval] sample 124 iter 10 grad_norm=6.7253e-01, inner_loss=7.5592e+00\n",
      "[eval] sample 124 iter 20 grad_norm=6.3952e-01, inner_loss=7.5161e+00\n",
      "[eval] sample 124 iter 30 grad_norm=6.0859e-01, inner_loss=7.4770e+00\n",
      "[eval] sample 124 iter 40 grad_norm=5.7962e-01, inner_loss=7.4417e+00\n",
      "[eval] sample 125 iter 0 grad_norm=4.9247e-01, inner_loss=7.2378e+00\n",
      "[eval] sample 125 iter 10 grad_norm=4.6987e-01, inner_loss=7.2146e+00\n",
      "[eval] sample 125 iter 20 grad_norm=4.4879e-01, inner_loss=7.1934e+00\n",
      "[eval] sample 125 iter 30 grad_norm=4.2911e-01, inner_loss=7.1741e+00\n",
      "[eval] sample 125 iter 40 grad_norm=4.1072e-01, inner_loss=7.1564e+00\n",
      "[eval] sample 126 iter 0 grad_norm=7.4256e-01, inner_loss=7.7889e+00\n",
      "[eval] sample 126 iter 10 grad_norm=7.0728e-01, inner_loss=7.7362e+00\n",
      "[eval] sample 126 iter 20 grad_norm=6.7413e-01, inner_loss=7.6884e+00\n",
      "[eval] sample 126 iter 30 grad_norm=6.4297e-01, inner_loss=7.6449e+00\n",
      "[eval] sample 126 iter 40 grad_norm=6.1366e-01, inner_loss=7.6054e+00\n",
      "[eval] sample 127 iter 0 grad_norm=7.0554e-01, inner_loss=7.7171e+00\n",
      "[eval] sample 127 iter 10 grad_norm=6.7231e-01, inner_loss=7.6696e+00\n",
      "[eval] sample 127 iter 20 grad_norm=6.4120e-01, inner_loss=7.6263e+00\n",
      "[eval] sample 127 iter 30 grad_norm=6.1204e-01, inner_loss=7.5870e+00\n",
      "[eval] sample 127 iter 40 grad_norm=5.8470e-01, inner_loss=7.5511e+00\n",
      "[eval] sample 128 iter 0 grad_norm=7.1231e-01, inner_loss=7.8269e+00\n",
      "[eval] sample 128 iter 10 grad_norm=6.7734e-01, inner_loss=7.7785e+00\n",
      "[eval] sample 128 iter 20 grad_norm=6.4487e-01, inner_loss=7.7347e+00\n",
      "[eval] sample 128 iter 30 grad_norm=6.1471e-01, inner_loss=7.6950e+00\n",
      "[eval] sample 128 iter 40 grad_norm=5.8665e-01, inner_loss=7.6588e+00\n",
      "[eval] sample 129 iter 0 grad_norm=6.3120e-01, inner_loss=7.5207e+00\n",
      "[eval] sample 129 iter 10 grad_norm=6.0287e-01, inner_loss=7.4826e+00\n",
      "[eval] sample 129 iter 20 grad_norm=5.7624e-01, inner_loss=7.4477e+00\n",
      "[eval] sample 129 iter 30 grad_norm=5.5117e-01, inner_loss=7.4159e+00\n",
      "[eval] sample 129 iter 40 grad_norm=5.2758e-01, inner_loss=7.3868e+00\n",
      "[eval] sample 130 iter 0 grad_norm=7.5116e-01, inner_loss=7.7178e+00\n",
      "[eval] sample 130 iter 10 grad_norm=7.1307e-01, inner_loss=7.6641e+00\n",
      "[eval] sample 130 iter 20 grad_norm=6.7752e-01, inner_loss=7.6156e+00\n",
      "[eval] sample 130 iter 30 grad_norm=6.4431e-01, inner_loss=7.5719e+00\n",
      "[eval] sample 130 iter 40 grad_norm=6.1325e-01, inner_loss=7.5322e+00\n",
      "[eval] sample 131 iter 0 grad_norm=8.0107e-01, inner_loss=7.8214e+00\n",
      "[eval] sample 131 iter 10 grad_norm=7.5988e-01, inner_loss=7.7603e+00\n",
      "[eval] sample 131 iter 20 grad_norm=7.2139e-01, inner_loss=7.7054e+00\n",
      "[eval] sample 131 iter 30 grad_norm=6.8540e-01, inner_loss=7.6558e+00\n",
      "[eval] sample 131 iter 40 grad_norm=6.5173e-01, inner_loss=7.6110e+00\n",
      "[eval] sample 132 iter 0 grad_norm=8.4627e-01, inner_loss=8.0901e+00\n",
      "[eval] sample 132 iter 10 grad_norm=8.0665e-01, inner_loss=8.0216e+00\n",
      "[eval] sample 132 iter 20 grad_norm=7.6949e-01, inner_loss=7.9594e+00\n",
      "[eval] sample 132 iter 30 grad_norm=7.3463e-01, inner_loss=7.9027e+00\n",
      "[eval] sample 132 iter 40 grad_norm=7.0189e-01, inner_loss=7.8510e+00\n",
      "[eval] sample 133 iter 0 grad_norm=8.0063e-01, inner_loss=7.8335e+00\n",
      "[eval] sample 133 iter 10 grad_norm=7.6099e-01, inner_loss=7.7724e+00\n",
      "[eval] sample 133 iter 20 grad_norm=7.2384e-01, inner_loss=7.7171e+00\n",
      "[eval] sample 133 iter 30 grad_norm=6.8899e-01, inner_loss=7.6671e+00\n",
      "[eval] sample 133 iter 40 grad_norm=6.5628e-01, inner_loss=7.6218e+00\n",
      "[eval] sample 134 iter 0 grad_norm=5.8917e-01, inner_loss=7.5271e+00\n",
      "[eval] sample 134 iter 10 grad_norm=5.6079e-01, inner_loss=7.4940e+00\n",
      "[eval] sample 134 iter 20 grad_norm=5.3446e-01, inner_loss=7.4639e+00\n",
      "[eval] sample 134 iter 30 grad_norm=5.1001e-01, inner_loss=7.4366e+00\n",
      "[eval] sample 134 iter 40 grad_norm=4.8728e-01, inner_loss=7.4117e+00\n",
      "[eval] sample 135 iter 0 grad_norm=6.6650e-01, inner_loss=7.4316e+00\n",
      "[eval] sample 135 iter 10 grad_norm=6.3293e-01, inner_loss=7.3893e+00\n",
      "[eval] sample 135 iter 20 grad_norm=6.0151e-01, inner_loss=7.3511e+00\n",
      "[eval] sample 135 iter 30 grad_norm=5.7209e-01, inner_loss=7.3166e+00\n",
      "[eval] sample 135 iter 40 grad_norm=5.4453e-01, inner_loss=7.2853e+00\n",
      "[eval] sample 136 iter 0 grad_norm=6.8623e-01, inner_loss=7.6622e+00\n",
      "[eval] sample 136 iter 10 grad_norm=6.5584e-01, inner_loss=7.6171e+00\n",
      "[eval] sample 136 iter 20 grad_norm=6.2718e-01, inner_loss=7.5758e+00\n",
      "[eval] sample 136 iter 30 grad_norm=6.0015e-01, inner_loss=7.5381e+00\n",
      "[eval] sample 136 iter 40 grad_norm=5.7463e-01, inner_loss=7.5035e+00\n",
      "[eval] sample 137 iter 0 grad_norm=7.0705e-01, inner_loss=7.4756e+00\n",
      "[eval] sample 137 iter 10 grad_norm=6.7020e-01, inner_loss=7.4281e+00\n",
      "[eval] sample 137 iter 20 grad_norm=6.3574e-01, inner_loss=7.3853e+00\n",
      "[eval] sample 137 iter 30 grad_norm=6.0350e-01, inner_loss=7.3469e+00\n",
      "[eval] sample 137 iter 40 grad_norm=5.7331e-01, inner_loss=7.3122e+00\n",
      "[eval] sample 138 iter 0 grad_norm=6.3724e-01, inner_loss=7.5700e+00\n",
      "[eval] sample 138 iter 10 grad_norm=6.0641e-01, inner_loss=7.5313e+00\n",
      "[eval] sample 138 iter 20 grad_norm=5.7756e-01, inner_loss=7.4962e+00\n",
      "[eval] sample 138 iter 30 grad_norm=5.5054e-01, inner_loss=7.4643e+00\n",
      "[eval] sample 138 iter 40 grad_norm=5.2522e-01, inner_loss=7.4353e+00\n",
      "[eval] sample 139 iter 0 grad_norm=6.6371e-01, inner_loss=7.5870e+00\n",
      "[eval] sample 139 iter 10 grad_norm=6.3315e-01, inner_loss=7.5449e+00\n",
      "[eval] sample 139 iter 20 grad_norm=6.0453e-01, inner_loss=7.5065e+00\n",
      "[eval] sample 139 iter 30 grad_norm=5.7769e-01, inner_loss=7.4715e+00\n",
      "[eval] sample 139 iter 40 grad_norm=5.5249e-01, inner_loss=7.4395e+00\n",
      "[eval] sample 140 iter 0 grad_norm=4.0900e-01, inner_loss=7.0201e+00\n",
      "[eval] sample 140 iter 10 grad_norm=3.8868e-01, inner_loss=7.0042e+00\n",
      "[eval] sample 140 iter 20 grad_norm=3.6975e-01, inner_loss=6.9898e+00\n",
      "[eval] sample 140 iter 30 grad_norm=3.5208e-01, inner_loss=6.9767e+00\n",
      "[eval] sample 140 iter 40 grad_norm=3.3557e-01, inner_loss=6.9649e+00\n",
      "[eval] sample 141 iter 0 grad_norm=6.1368e-01, inner_loss=7.4306e+00\n",
      "[eval] sample 141 iter 10 grad_norm=5.8451e-01, inner_loss=7.3946e+00\n",
      "[eval] sample 141 iter 20 grad_norm=5.5718e-01, inner_loss=7.3620e+00\n",
      "[eval] sample 141 iter 30 grad_norm=5.3157e-01, inner_loss=7.3323e+00\n",
      "[eval] sample 141 iter 40 grad_norm=5.0754e-01, inner_loss=7.3052e+00\n",
      "[eval] sample 142 iter 0 grad_norm=5.8747e-01, inner_loss=7.4621e+00\n",
      "[eval] sample 142 iter 10 grad_norm=5.6133e-01, inner_loss=7.4291e+00\n",
      "[eval] sample 142 iter 20 grad_norm=5.3679e-01, inner_loss=7.3988e+00\n",
      "[eval] sample 142 iter 30 grad_norm=5.1373e-01, inner_loss=7.3712e+00\n",
      "[eval] sample 142 iter 40 grad_norm=4.9204e-01, inner_loss=7.3459e+00\n",
      "[eval] sample 143 iter 0 grad_norm=4.8261e-01, inner_loss=7.1128e+00\n",
      "[eval] sample 143 iter 10 grad_norm=4.5947e-01, inner_loss=7.0905e+00\n",
      "[eval] sample 143 iter 20 grad_norm=4.3779e-01, inner_loss=7.0704e+00\n",
      "[eval] sample 143 iter 30 grad_norm=4.1746e-01, inner_loss=7.0520e+00\n",
      "[eval] sample 143 iter 40 grad_norm=3.9838e-01, inner_loss=7.0354e+00\n",
      "[eval] sample 144 iter 0 grad_norm=6.2665e-01, inner_loss=7.5283e+00\n",
      "[eval] sample 144 iter 10 grad_norm=5.9817e-01, inner_loss=7.4908e+00\n",
      "[eval] sample 144 iter 20 grad_norm=5.7155e-01, inner_loss=7.4565e+00\n",
      "[eval] sample 144 iter 30 grad_norm=5.4663e-01, inner_loss=7.4252e+00\n",
      "[eval] sample 144 iter 40 grad_norm=5.2330e-01, inner_loss=7.3965e+00\n",
      "[eval] sample 145 iter 0 grad_norm=4.6509e-01, inner_loss=7.0834e+00\n",
      "[eval] sample 145 iter 10 grad_norm=4.4289e-01, inner_loss=7.0628e+00\n",
      "[eval] sample 145 iter 20 grad_norm=4.2209e-01, inner_loss=7.0440e+00\n",
      "[eval] sample 145 iter 30 grad_norm=4.0258e-01, inner_loss=7.0270e+00\n",
      "[eval] sample 145 iter 40 grad_norm=3.8427e-01, inner_loss=7.0115e+00\n",
      "[eval] sample 146 iter 0 grad_norm=6.1744e-01, inner_loss=7.5798e+00\n",
      "[eval] sample 146 iter 10 grad_norm=5.8651e-01, inner_loss=7.5434e+00\n",
      "[eval] sample 146 iter 20 grad_norm=5.5780e-01, inner_loss=7.5106e+00\n",
      "[eval] sample 146 iter 30 grad_norm=5.3112e-01, inner_loss=7.4809e+00\n",
      "[eval] sample 146 iter 40 grad_norm=5.0631e-01, inner_loss=7.4540e+00\n",
      "[eval] sample 147 iter 0 grad_norm=8.0983e-01, inner_loss=7.9300e+00\n",
      "[eval] sample 147 iter 10 grad_norm=7.6896e-01, inner_loss=7.8675e+00\n",
      "[eval] sample 147 iter 20 grad_norm=7.3072e-01, inner_loss=7.8112e+00\n",
      "[eval] sample 147 iter 30 grad_norm=6.9492e-01, inner_loss=7.7603e+00\n",
      "[eval] sample 147 iter 40 grad_norm=6.6138e-01, inner_loss=7.7142e+00\n",
      "[eval] sample 148 iter 0 grad_norm=6.6876e-01, inner_loss=7.5904e+00\n",
      "[eval] sample 148 iter 10 grad_norm=6.3857e-01, inner_loss=7.5476e+00\n",
      "[eval] sample 148 iter 20 grad_norm=6.1008e-01, inner_loss=7.5085e+00\n",
      "[eval] sample 148 iter 30 grad_norm=5.8320e-01, inner_loss=7.4728e+00\n",
      "[eval] sample 148 iter 40 grad_norm=5.5781e-01, inner_loss=7.4402e+00\n",
      "[eval] sample 149 iter 0 grad_norm=4.0351e-01, inner_loss=7.0317e+00\n",
      "[eval] sample 149 iter 10 grad_norm=3.8287e-01, inner_loss=7.0162e+00\n",
      "[eval] sample 149 iter 20 grad_norm=3.6366e-01, inner_loss=7.0022e+00\n",
      "[eval] sample 149 iter 30 grad_norm=3.4576e-01, inner_loss=6.9896e+00\n",
      "[eval] sample 149 iter 40 grad_norm=3.2909e-01, inner_loss=6.9782e+00\n",
      "[eval] sample 150 iter 0 grad_norm=6.3156e-01, inner_loss=7.5471e+00\n",
      "[eval] sample 150 iter 10 grad_norm=6.0445e-01, inner_loss=7.5088e+00\n",
      "[eval] sample 150 iter 20 grad_norm=5.7895e-01, inner_loss=7.4737e+00\n",
      "[eval] sample 150 iter 30 grad_norm=5.5494e-01, inner_loss=7.4415e+00\n",
      "[eval] sample 150 iter 40 grad_norm=5.3231e-01, inner_loss=7.4119e+00\n",
      "[eval] sample 151 iter 0 grad_norm=7.1685e-01, inner_loss=7.7837e+00\n",
      "[eval] sample 151 iter 10 grad_norm=6.8645e-01, inner_loss=7.7343e+00\n",
      "[eval] sample 151 iter 20 grad_norm=6.5776e-01, inner_loss=7.6891e+00\n",
      "[eval] sample 151 iter 30 grad_norm=6.3066e-01, inner_loss=7.6475e+00\n",
      "[eval] sample 151 iter 40 grad_norm=6.0504e-01, inner_loss=7.6093e+00\n",
      "[eval] sample 152 iter 0 grad_norm=5.3244e-01, inner_loss=7.3339e+00\n",
      "[eval] sample 152 iter 10 grad_norm=5.0848e-01, inner_loss=7.3067e+00\n",
      "[eval] sample 152 iter 20 grad_norm=4.8608e-01, inner_loss=7.2819e+00\n",
      "[eval] sample 152 iter 30 grad_norm=4.6512e-01, inner_loss=7.2593e+00\n",
      "[eval] sample 152 iter 40 grad_norm=4.4549e-01, inner_loss=7.2385e+00\n",
      "[eval] sample 153 iter 0 grad_norm=5.2167e-01, inner_loss=7.2793e+00\n",
      "[eval] sample 153 iter 10 grad_norm=4.9866e-01, inner_loss=7.2532e+00\n",
      "[eval] sample 153 iter 20 grad_norm=4.7706e-01, inner_loss=7.2294e+00\n",
      "[eval] sample 153 iter 30 grad_norm=4.5676e-01, inner_loss=7.2075e+00\n",
      "[eval] sample 153 iter 40 grad_norm=4.3766e-01, inner_loss=7.1875e+00\n",
      "[eval] sample 154 iter 0 grad_norm=4.3050e-01, inner_loss=7.0228e+00\n",
      "[eval] sample 154 iter 10 grad_norm=4.1006e-01, inner_loss=7.0051e+00\n",
      "[eval] sample 154 iter 20 grad_norm=3.9090e-01, inner_loss=6.9890e+00\n",
      "[eval] sample 154 iter 30 grad_norm=3.7293e-01, inner_loss=6.9744e+00\n",
      "[eval] sample 154 iter 40 grad_norm=3.5606e-01, inner_loss=6.9611e+00\n",
      "[eval] sample 155 iter 0 grad_norm=7.7624e-01, inner_loss=7.8485e+00\n",
      "[eval] sample 155 iter 10 grad_norm=7.4060e-01, inner_loss=7.7908e+00\n",
      "[eval] sample 155 iter 20 grad_norm=7.0709e-01, inner_loss=7.7383e+00\n",
      "[eval] sample 155 iter 30 grad_norm=6.7556e-01, inner_loss=7.6904e+00\n",
      "[eval] sample 155 iter 40 grad_norm=6.4587e-01, inner_loss=7.6467e+00\n",
      "[eval] sample 156 iter 0 grad_norm=6.0756e-01, inner_loss=7.3651e+00\n",
      "[eval] sample 156 iter 10 grad_norm=5.7697e-01, inner_loss=7.3300e+00\n",
      "[eval] sample 156 iter 20 grad_norm=5.4834e-01, inner_loss=7.2982e+00\n",
      "[eval] sample 156 iter 30 grad_norm=5.2153e-01, inner_loss=7.2696e+00\n",
      "[eval] sample 156 iter 40 grad_norm=4.9641e-01, inner_loss=7.2436e+00\n",
      "[eval] sample 157 iter 0 grad_norm=6.7327e-01, inner_loss=7.5896e+00\n",
      "[eval] sample 157 iter 10 grad_norm=6.4295e-01, inner_loss=7.5462e+00\n",
      "[eval] sample 157 iter 20 grad_norm=6.1433e-01, inner_loss=7.5066e+00\n",
      "[eval] sample 157 iter 30 grad_norm=5.8731e-01, inner_loss=7.4705e+00\n",
      "[eval] sample 157 iter 40 grad_norm=5.6178e-01, inner_loss=7.4374e+00\n",
      "[eval] sample 158 iter 0 grad_norm=6.9409e-01, inner_loss=7.6001e+00\n",
      "[eval] sample 158 iter 10 grad_norm=6.6048e-01, inner_loss=7.5541e+00\n",
      "[eval] sample 158 iter 20 grad_norm=6.2896e-01, inner_loss=7.5125e+00\n",
      "[eval] sample 158 iter 30 grad_norm=5.9937e-01, inner_loss=7.4747e+00\n",
      "[eval] sample 158 iter 40 grad_norm=5.7159e-01, inner_loss=7.4403e+00\n",
      "[eval] sample 159 iter 0 grad_norm=4.2560e-01, inner_loss=7.0512e+00\n",
      "[eval] sample 159 iter 10 grad_norm=4.0459e-01, inner_loss=7.0339e+00\n",
      "[eval] sample 159 iter 20 grad_norm=3.8505e-01, inner_loss=7.0183e+00\n",
      "[eval] sample 159 iter 30 grad_norm=3.6687e-01, inner_loss=7.0041e+00\n",
      "[eval] sample 159 iter 40 grad_norm=3.4992e-01, inner_loss=6.9912e+00\n",
      "[eval] sample 160 iter 0 grad_norm=6.5695e-01, inner_loss=7.5947e+00\n",
      "[eval] sample 160 iter 10 grad_norm=6.2651e-01, inner_loss=7.5534e+00\n",
      "[eval] sample 160 iter 20 grad_norm=5.9806e-01, inner_loss=7.5159e+00\n",
      "[eval] sample 160 iter 30 grad_norm=5.7146e-01, inner_loss=7.4816e+00\n",
      "[eval] sample 160 iter 40 grad_norm=5.4654e-01, inner_loss=7.4503e+00\n",
      "[eval] sample 161 iter 0 grad_norm=6.4649e-01, inner_loss=7.5050e+00\n",
      "[eval] sample 161 iter 10 grad_norm=6.1494e-01, inner_loss=7.4652e+00\n",
      "[eval] sample 161 iter 20 grad_norm=5.8530e-01, inner_loss=7.4291e+00\n",
      "[eval] sample 161 iter 30 grad_norm=5.5744e-01, inner_loss=7.3964e+00\n",
      "[eval] sample 161 iter 40 grad_norm=5.3125e-01, inner_loss=7.3667e+00\n",
      "[eval] sample 162 iter 0 grad_norm=8.5417e-01, inner_loss=8.1200e+00\n",
      "[eval] sample 162 iter 10 grad_norm=8.1488e-01, inner_loss=8.0502e+00\n",
      "[eval] sample 162 iter 20 grad_norm=7.7799e-01, inner_loss=7.9866e+00\n",
      "[eval] sample 162 iter 30 grad_norm=7.4333e-01, inner_loss=7.9287e+00\n",
      "[eval] sample 162 iter 40 grad_norm=7.1074e-01, inner_loss=7.8757e+00\n",
      "[eval] sample 163 iter 0 grad_norm=7.8445e-01, inner_loss=7.8036e+00\n",
      "[eval] sample 163 iter 10 grad_norm=7.4525e-01, inner_loss=7.7449e+00\n",
      "[eval] sample 163 iter 20 grad_norm=7.0851e-01, inner_loss=7.6920e+00\n",
      "[eval] sample 163 iter 30 grad_norm=6.7407e-01, inner_loss=7.6441e+00\n",
      "[eval] sample 163 iter 40 grad_norm=6.4175e-01, inner_loss=7.6007e+00\n",
      "[eval] sample 164 iter 0 grad_norm=5.5356e-01, inner_loss=7.2854e+00\n",
      "[eval] sample 164 iter 10 grad_norm=5.2532e-01, inner_loss=7.2562e+00\n",
      "[eval] sample 164 iter 20 grad_norm=4.9895e-01, inner_loss=7.2299e+00\n",
      "[eval] sample 164 iter 30 grad_norm=4.7433e-01, inner_loss=7.2062e+00\n",
      "[eval] sample 164 iter 40 grad_norm=4.5131e-01, inner_loss=7.1847e+00\n",
      "[eval] sample 165 iter 0 grad_norm=7.6245e-01, inner_loss=7.9676e+00\n",
      "[eval] sample 165 iter 10 grad_norm=7.3058e-01, inner_loss=7.9118e+00\n",
      "[eval] sample 165 iter 20 grad_norm=7.0052e-01, inner_loss=7.8605e+00\n",
      "[eval] sample 165 iter 30 grad_norm=6.7216e-01, inner_loss=7.8133e+00\n",
      "[eval] sample 165 iter 40 grad_norm=6.4537e-01, inner_loss=7.7698e+00\n",
      "[eval] sample 166 iter 0 grad_norm=4.9881e-01, inner_loss=7.3051e+00\n",
      "[eval] sample 166 iter 10 grad_norm=4.7740e-01, inner_loss=7.2812e+00\n",
      "[eval] sample 166 iter 20 grad_norm=4.5738e-01, inner_loss=7.2593e+00\n",
      "[eval] sample 166 iter 30 grad_norm=4.3864e-01, inner_loss=7.2392e+00\n",
      "[eval] sample 166 iter 40 grad_norm=4.2108e-01, inner_loss=7.2207e+00\n",
      "[eval] sample 167 iter 0 grad_norm=4.2406e-01, inner_loss=7.1656e+00\n",
      "[eval] sample 167 iter 10 grad_norm=4.0439e-01, inner_loss=7.1484e+00\n",
      "[eval] sample 167 iter 20 grad_norm=3.8617e-01, inner_loss=7.1328e+00\n",
      "[eval] sample 167 iter 30 grad_norm=3.6927e-01, inner_loss=7.1185e+00\n",
      "[eval] sample 167 iter 40 grad_norm=3.5358e-01, inner_loss=7.1054e+00\n",
      "[eval] sample 168 iter 0 grad_norm=5.7963e-01, inner_loss=7.2707e+00\n",
      "[eval] sample 168 iter 10 grad_norm=5.5115e-01, inner_loss=7.2386e+00\n",
      "[eval] sample 168 iter 20 grad_norm=5.2447e-01, inner_loss=7.2097e+00\n",
      "[eval] sample 168 iter 30 grad_norm=4.9945e-01, inner_loss=7.1834e+00\n",
      "[eval] sample 168 iter 40 grad_norm=4.7598e-01, inner_loss=7.1596e+00\n",
      "[eval] sample 169 iter 0 grad_norm=5.5687e-01, inner_loss=7.3532e+00\n",
      "[eval] sample 169 iter 10 grad_norm=5.3221e-01, inner_loss=7.3235e+00\n",
      "[eval] sample 169 iter 20 grad_norm=5.0913e-01, inner_loss=7.2963e+00\n",
      "[eval] sample 169 iter 30 grad_norm=4.8750e-01, inner_loss=7.2714e+00\n",
      "[eval] sample 169 iter 40 grad_norm=4.6722e-01, inner_loss=7.2486e+00\n",
      "[eval] sample 170 iter 0 grad_norm=4.8129e-01, inner_loss=7.2243e+00\n",
      "[eval] sample 170 iter 10 grad_norm=4.6010e-01, inner_loss=7.2021e+00\n",
      "[eval] sample 170 iter 20 grad_norm=4.4030e-01, inner_loss=7.1818e+00\n",
      "[eval] sample 170 iter 30 grad_norm=4.2179e-01, inner_loss=7.1632e+00\n",
      "[eval] sample 170 iter 40 grad_norm=4.0446e-01, inner_loss=7.1461e+00\n",
      "[eval] sample 171 iter 0 grad_norm=8.3153e-01, inner_loss=7.7756e+00\n",
      "[eval] sample 171 iter 10 grad_norm=7.8681e-01, inner_loss=7.7099e+00\n",
      "[eval] sample 171 iter 20 grad_norm=7.4508e-01, inner_loss=7.6511e+00\n",
      "[eval] sample 171 iter 30 grad_norm=7.0612e-01, inner_loss=7.5984e+00\n",
      "[eval] sample 171 iter 40 grad_norm=6.6971e-01, inner_loss=7.5509e+00\n",
      "[eval] sample 172 iter 0 grad_norm=7.2706e-01, inner_loss=7.8416e+00\n",
      "[eval] sample 172 iter 10 grad_norm=6.9369e-01, inner_loss=7.7911e+00\n",
      "[eval] sample 172 iter 20 grad_norm=6.6242e-01, inner_loss=7.7450e+00\n",
      "[eval] sample 172 iter 30 grad_norm=6.3309e-01, inner_loss=7.7029e+00\n",
      "[eval] sample 172 iter 40 grad_norm=6.0555e-01, inner_loss=7.6645e+00\n",
      "[eval] sample 173 iter 0 grad_norm=4.8902e-01, inner_loss=7.2362e+00\n",
      "[eval] sample 173 iter 10 grad_norm=4.6764e-01, inner_loss=7.2133e+00\n",
      "[eval] sample 173 iter 20 grad_norm=4.4754e-01, inner_loss=7.1923e+00\n",
      "[eval] sample 173 iter 30 grad_norm=4.2862e-01, inner_loss=7.1731e+00\n",
      "[eval] sample 173 iter 40 grad_norm=4.1079e-01, inner_loss=7.1554e+00\n",
      "[eval] sample 174 iter 0 grad_norm=6.0810e-01, inner_loss=7.3823e+00\n",
      "[eval] sample 174 iter 10 grad_norm=5.7680e-01, inner_loss=7.3471e+00\n",
      "[eval] sample 174 iter 20 grad_norm=5.4757e-01, inner_loss=7.3155e+00\n",
      "[eval] sample 174 iter 30 grad_norm=5.2025e-01, inner_loss=7.2869e+00\n",
      "[eval] sample 174 iter 40 grad_norm=4.9471e-01, inner_loss=7.2611e+00\n",
      "[eval] sample 175 iter 0 grad_norm=3.0754e-01, inner_loss=6.8372e+00\n",
      "[eval] sample 175 iter 10 grad_norm=2.9290e-01, inner_loss=6.8282e+00\n",
      "[eval] sample 175 iter 20 grad_norm=2.7918e-01, inner_loss=6.8200e+00\n",
      "[eval] sample 175 iter 30 grad_norm=2.6633e-01, inner_loss=6.8125e+00\n",
      "[eval] sample 175 iter 40 grad_norm=2.5428e-01, inner_loss=6.8058e+00\n",
      "[eval] sample 176 iter 0 grad_norm=3.4262e-01, inner_loss=6.8760e+00\n",
      "[eval] sample 176 iter 10 grad_norm=3.2638e-01, inner_loss=6.8648e+00\n",
      "[eval] sample 176 iter 20 grad_norm=3.1116e-01, inner_loss=6.8546e+00\n",
      "[eval] sample 176 iter 30 grad_norm=2.9688e-01, inner_loss=6.8453e+00\n",
      "[eval] sample 176 iter 40 grad_norm=2.8348e-01, inner_loss=6.8369e+00\n",
      "[eval] sample 177 iter 0 grad_norm=5.4004e-01, inner_loss=7.4732e+00\n",
      "[eval] sample 177 iter 10 grad_norm=5.1572e-01, inner_loss=7.4453e+00\n",
      "[eval] sample 177 iter 20 grad_norm=4.9318e-01, inner_loss=7.4198e+00\n",
      "[eval] sample 177 iter 30 grad_norm=4.7226e-01, inner_loss=7.3965e+00\n",
      "[eval] sample 177 iter 40 grad_norm=4.5281e-01, inner_loss=7.3750e+00\n",
      "[eval] sample 178 iter 0 grad_norm=5.5216e-01, inner_loss=7.2376e+00\n",
      "[eval] sample 178 iter 10 grad_norm=5.2539e-01, inner_loss=7.2085e+00\n",
      "[eval] sample 178 iter 20 grad_norm=5.0031e-01, inner_loss=7.1822e+00\n",
      "[eval] sample 178 iter 30 grad_norm=4.7679e-01, inner_loss=7.1582e+00\n",
      "[eval] sample 178 iter 40 grad_norm=4.5474e-01, inner_loss=7.1365e+00\n",
      "[eval] sample 179 iter 0 grad_norm=6.6272e-01, inner_loss=7.6653e+00\n",
      "[eval] sample 179 iter 10 grad_norm=6.2989e-01, inner_loss=7.6235e+00\n",
      "[eval] sample 179 iter 20 grad_norm=5.9942e-01, inner_loss=7.5856e+00\n",
      "[eval] sample 179 iter 30 grad_norm=5.7111e-01, inner_loss=7.5513e+00\n",
      "[eval] sample 179 iter 40 grad_norm=5.4479e-01, inner_loss=7.5201e+00\n",
      "[eval] sample 180 iter 0 grad_norm=4.9964e-01, inner_loss=7.1492e+00\n",
      "[eval] sample 180 iter 10 grad_norm=4.7569e-01, inner_loss=7.1254e+00\n",
      "[eval] sample 180 iter 20 grad_norm=4.5324e-01, inner_loss=7.1038e+00\n",
      "[eval] sample 180 iter 30 grad_norm=4.3221e-01, inner_loss=7.0841e+00\n",
      "[eval] sample 180 iter 40 grad_norm=4.1248e-01, inner_loss=7.0662e+00\n",
      "[eval] sample 181 iter 0 grad_norm=6.3588e-01, inner_loss=7.6869e+00\n",
      "[eval] sample 181 iter 10 grad_norm=6.0713e-01, inner_loss=7.6482e+00\n",
      "[eval] sample 181 iter 20 grad_norm=5.8043e-01, inner_loss=7.6129e+00\n",
      "[eval] sample 181 iter 30 grad_norm=5.5560e-01, inner_loss=7.5806e+00\n",
      "[eval] sample 181 iter 40 grad_norm=5.3247e-01, inner_loss=7.5509e+00\n",
      "[eval] sample 182 iter 0 grad_norm=5.7712e-01, inner_loss=7.3444e+00\n",
      "[eval] sample 182 iter 10 grad_norm=5.4914e-01, inner_loss=7.3126e+00\n",
      "[eval] sample 182 iter 20 grad_norm=5.2294e-01, inner_loss=7.2838e+00\n",
      "[eval] sample 182 iter 30 grad_norm=4.9841e-01, inner_loss=7.2577e+00\n",
      "[eval] sample 182 iter 40 grad_norm=4.7542e-01, inner_loss=7.2339e+00\n",
      "[eval] sample 183 iter 0 grad_norm=9.0540e-01, inner_loss=8.2783e+00\n",
      "[eval] sample 183 iter 10 grad_norm=8.6179e-01, inner_loss=8.2000e+00\n",
      "[eval] sample 183 iter 20 grad_norm=8.2082e-01, inner_loss=8.1291e+00\n",
      "[eval] sample 183 iter 30 grad_norm=7.8231e-01, inner_loss=8.0647e+00\n",
      "[eval] sample 183 iter 40 grad_norm=7.4611e-01, inner_loss=8.0062e+00\n",
      "[eval] sample 184 iter 0 grad_norm=5.4027e-01, inner_loss=7.4054e+00\n",
      "[eval] sample 184 iter 10 grad_norm=5.1609e-01, inner_loss=7.3775e+00\n",
      "[eval] sample 184 iter 20 grad_norm=4.9359e-01, inner_loss=7.3520e+00\n",
      "[eval] sample 184 iter 30 grad_norm=4.7261e-01, inner_loss=7.3286e+00\n",
      "[eval] sample 184 iter 40 grad_norm=4.5304e-01, inner_loss=7.3071e+00\n",
      "[eval] sample 185 iter 0 grad_norm=5.6944e-01, inner_loss=7.4330e+00\n",
      "[eval] sample 185 iter 10 grad_norm=5.4471e-01, inner_loss=7.4019e+00\n",
      "[eval] sample 185 iter 20 grad_norm=5.2154e-01, inner_loss=7.3734e+00\n",
      "[eval] sample 185 iter 30 grad_norm=4.9981e-01, inner_loss=7.3473e+00\n",
      "[eval] sample 185 iter 40 grad_norm=4.7940e-01, inner_loss=7.3233e+00\n",
      "[eval] sample 186 iter 0 grad_norm=8.6112e-01, inner_loss=8.0343e+00\n",
      "[eval] sample 186 iter 10 grad_norm=8.1806e-01, inner_loss=7.9637e+00\n",
      "[eval] sample 186 iter 20 grad_norm=7.7784e-01, inner_loss=7.8999e+00\n",
      "[eval] sample 186 iter 30 grad_norm=7.4024e-01, inner_loss=7.8421e+00\n",
      "[eval] sample 186 iter 40 grad_norm=7.0507e-01, inner_loss=7.7898e+00\n",
      "[eval] sample 187 iter 0 grad_norm=6.9284e-01, inner_loss=7.8167e+00\n",
      "[eval] sample 187 iter 10 grad_norm=6.6548e-01, inner_loss=7.7705e+00\n",
      "[eval] sample 187 iter 20 grad_norm=6.3969e-01, inner_loss=7.7278e+00\n",
      "[eval] sample 187 iter 30 grad_norm=6.1535e-01, inner_loss=7.6884e+00\n",
      "[eval] sample 187 iter 40 grad_norm=5.9234e-01, inner_loss=7.6518e+00\n",
      "[eval] sample 188 iter 0 grad_norm=9.0276e-01, inner_loss=8.2025e+00\n",
      "[eval] sample 188 iter 10 grad_norm=8.5910e-01, inner_loss=8.1248e+00\n",
      "[eval] sample 188 iter 20 grad_norm=8.1805e-01, inner_loss=8.0543e+00\n",
      "[eval] sample 188 iter 30 grad_norm=7.7946e-01, inner_loss=7.9904e+00\n",
      "[eval] sample 188 iter 40 grad_norm=7.4315e-01, inner_loss=7.9323e+00\n",
      "[eval] sample 189 iter 0 grad_norm=3.9191e-01, inner_loss=6.9602e+00\n",
      "[eval] sample 189 iter 10 grad_norm=3.7338e-01, inner_loss=6.9455e+00\n",
      "[eval] sample 189 iter 20 grad_norm=3.5601e-01, inner_loss=6.9322e+00\n",
      "[eval] sample 189 iter 30 grad_norm=3.3971e-01, inner_loss=6.9201e+00\n",
      "[eval] sample 189 iter 40 grad_norm=3.2441e-01, inner_loss=6.9090e+00\n",
      "[eval] sample 190 iter 0 grad_norm=7.1058e-01, inner_loss=7.4384e+00\n",
      "[eval] sample 190 iter 10 grad_norm=6.7227e-01, inner_loss=7.3905e+00\n",
      "[eval] sample 190 iter 20 grad_norm=6.3647e-01, inner_loss=7.3476e+00\n",
      "[eval] sample 190 iter 30 grad_norm=6.0302e-01, inner_loss=7.3091e+00\n",
      "[eval] sample 190 iter 40 grad_norm=5.7173e-01, inner_loss=7.2745e+00\n",
      "[eval] sample 191 iter 0 grad_norm=5.2270e-01, inner_loss=7.1820e+00\n",
      "[eval] sample 191 iter 10 grad_norm=4.9729e-01, inner_loss=7.1559e+00\n",
      "[eval] sample 191 iter 20 grad_norm=4.7349e-01, inner_loss=7.1323e+00\n",
      "[eval] sample 191 iter 30 grad_norm=4.5117e-01, inner_loss=7.1109e+00\n",
      "[eval] sample 191 iter 40 grad_norm=4.3024e-01, inner_loss=7.0914e+00\n",
      "[eval] sample 192 iter 0 grad_norm=7.2211e-01, inner_loss=7.8267e+00\n",
      "[eval] sample 192 iter 10 grad_norm=6.9084e-01, inner_loss=7.7766e+00\n",
      "[eval] sample 192 iter 20 grad_norm=6.6139e-01, inner_loss=7.7308e+00\n",
      "[eval] sample 192 iter 30 grad_norm=6.3362e-01, inner_loss=7.6888e+00\n",
      "[eval] sample 192 iter 40 grad_norm=6.0742e-01, inner_loss=7.6503e+00\n",
      "[eval] sample 193 iter 0 grad_norm=5.4677e-01, inner_loss=7.2628e+00\n",
      "[eval] sample 193 iter 10 grad_norm=5.2018e-01, inner_loss=7.2343e+00\n",
      "[eval] sample 193 iter 20 grad_norm=4.9534e-01, inner_loss=7.2085e+00\n",
      "[eval] sample 193 iter 30 grad_norm=4.7213e-01, inner_loss=7.1850e+00\n",
      "[eval] sample 193 iter 40 grad_norm=4.5043e-01, inner_loss=7.1637e+00\n",
      "[eval] sample 194 iter 0 grad_norm=7.7962e-01, inner_loss=7.8473e+00\n",
      "[eval] sample 194 iter 10 grad_norm=7.4278e-01, inner_loss=7.7892e+00\n",
      "[eval] sample 194 iter 20 grad_norm=7.0810e-01, inner_loss=7.7365e+00\n",
      "[eval] sample 194 iter 30 grad_norm=6.7544e-01, inner_loss=7.6885e+00\n",
      "[eval] sample 194 iter 40 grad_norm=6.4466e-01, inner_loss=7.6449e+00\n",
      "[eval] sample 195 iter 0 grad_norm=3.7603e-01, inner_loss=6.9503e+00\n",
      "[eval] sample 195 iter 10 grad_norm=3.5839e-01, inner_loss=6.9368e+00\n",
      "[eval] sample 195 iter 20 grad_norm=3.4186e-01, inner_loss=6.9245e+00\n",
      "[eval] sample 195 iter 30 grad_norm=3.2635e-01, inner_loss=6.9133e+00\n",
      "[eval] sample 195 iter 40 grad_norm=3.1180e-01, inner_loss=6.9031e+00\n",
      "[eval] sample 196 iter 0 grad_norm=6.9282e-01, inner_loss=7.4388e+00\n",
      "[eval] sample 196 iter 10 grad_norm=6.5682e-01, inner_loss=7.3932e+00\n",
      "[eval] sample 196 iter 20 grad_norm=6.2315e-01, inner_loss=7.3521e+00\n",
      "[eval] sample 196 iter 30 grad_norm=5.9165e-01, inner_loss=7.3152e+00\n",
      "[eval] sample 196 iter 40 grad_norm=5.6216e-01, inner_loss=7.2818e+00\n",
      "[eval] sample 197 iter 0 grad_norm=7.2593e-01, inner_loss=7.5829e+00\n",
      "[eval] sample 197 iter 10 grad_norm=6.8565e-01, inner_loss=7.5329e+00\n",
      "[eval] sample 197 iter 20 grad_norm=6.4820e-01, inner_loss=7.4884e+00\n",
      "[eval] sample 197 iter 30 grad_norm=6.1337e-01, inner_loss=7.4485e+00\n",
      "[eval] sample 197 iter 40 grad_norm=5.8096e-01, inner_loss=7.4127e+00\n",
      "[eval] sample 198 iter 0 grad_norm=5.4511e-01, inner_loss=7.3722e+00\n",
      "[eval] sample 198 iter 10 grad_norm=5.2146e-01, inner_loss=7.3437e+00\n",
      "[eval] sample 198 iter 20 grad_norm=4.9933e-01, inner_loss=7.3176e+00\n",
      "[eval] sample 198 iter 30 grad_norm=4.7861e-01, inner_loss=7.2936e+00\n",
      "[eval] sample 198 iter 40 grad_norm=4.5918e-01, inner_loss=7.2716e+00\n",
      "[eval] sample 199 iter 0 grad_norm=8.6072e-01, inner_loss=7.8758e+00\n",
      "[eval] sample 199 iter 10 grad_norm=8.1420e-01, inner_loss=7.8055e+00\n",
      "[eval] sample 199 iter 20 grad_norm=7.7077e-01, inner_loss=7.7425e+00\n",
      "[eval] sample 199 iter 30 grad_norm=7.3023e-01, inner_loss=7.6861e+00\n",
      "[eval] sample 199 iter 40 grad_norm=6.9234e-01, inner_loss=7.6354e+00\n",
      "\n",
      "--- Classification Performance (Evaluation) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        17\n",
      "           1       1.00      0.79      0.88        28\n",
      "           2       0.64      0.88      0.74        16\n",
      "           3       0.50      0.88      0.64        16\n",
      "           4       0.67      1.00      0.80        28\n",
      "           5       0.92      0.55      0.69        20\n",
      "           6       1.00      0.70      0.82        20\n",
      "           7       0.94      0.62      0.75        24\n",
      "           8       0.50      0.80      0.62        10\n",
      "           9       0.83      0.48      0.61        21\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.80      0.76      0.75       200\n",
      "weighted avg       0.83      0.76      0.76       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukta\\AppData\\Local\\Temp\\ipykernel_19036\\4281910160.py:438: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"\\n[HOAG] Sample {sample_count}: label={int(label.cpu().numpy())}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOAG] Sample 201: label=2\n",
      "[HOAG] theta now: [-3.8582838  -0.91774374]\n",
      "\n",
      "[HOAG] Sample 202: label=1\n",
      "[HOAG] theta now: [-3.8626108 -0.9146138]\n",
      "\n",
      "[HOAG] Sample 203: label=0\n",
      "[HOAG] theta now: [-3.8607404  -0.91841435]\n",
      "\n",
      "[HOAG] Sample 204: label=0\n",
      "[HOAG] theta now: [-3.8673332  -0.91347826]\n",
      "\n",
      "[HOAG] Sample 205: label=4\n",
      "[HOAG] theta now: [-3.8701062  -0.91032845]\n",
      "\n",
      "[HOAG] Sample 206: label=9\n",
      "[HOAG] theta now: [-3.8740058 -0.9066871]\n",
      "\n",
      "[HOAG] Sample 207: label=7\n",
      "[HOAG] theta now: [-3.8744407  -0.90637976]\n",
      "\n",
      "[HOAG] Sample 208: label=2\n",
      "[HOAG] theta now: [-3.8777056 -0.9038208]\n",
      "\n",
      "[HOAG] Sample 209: label=9\n",
      "[HOAG] theta now: [-3.8845453  -0.89794475]\n",
      "\n",
      "[HOAG] Sample 210: label=4\n",
      "[HOAG] theta now: [-3.8868158 -0.8960993]\n",
      "\n",
      "[HOAG] Sample 211: label=7\n",
      "[HOAG] theta now: [-3.8930914  -0.89082557]\n",
      "\n",
      "[HOAG] Sample 212: label=0\n",
      "[HOAG] theta now: [-3.8977883 -0.8869624]\n",
      "\n",
      "[HOAG] Sample 213: label=8\n",
      "[HOAG] theta now: [-3.9023297  -0.88331634]\n",
      "\n",
      "[HOAG] Sample 214: label=0\n",
      "[HOAG] theta now: [-3.908968   -0.87800086]\n",
      "\n",
      "[HOAG] Sample 215: label=0\n",
      "[HOAG] theta now: [-3.909544   -0.87757105]\n",
      "\n",
      "[HOAG] Sample 216: label=3\n",
      "[HOAG] theta now: [-3.916274   -0.87171143]\n",
      "\n",
      "[HOAG] Sample 217: label=9\n",
      "[HOAG] theta now: [-3.9266732 -0.8635254]\n",
      "\n",
      "[HOAG] Sample 218: label=2\n",
      "[HOAG] theta now: [-3.926738  -0.8635021]\n",
      "\n",
      "[HOAG] Sample 219: label=4\n",
      "[HOAG] theta now: [-3.9221675 -0.8672751]\n",
      "\n",
      "[HOAG] Sample 220: label=6\n",
      "[HOAG] theta now: [-3.9038815 -0.8809246]\n",
      "\n",
      "[HOAG] Sample 221: label=9\n",
      "[HOAG] theta now: [-3.9136875 -0.8730243]\n",
      "\n",
      "[HOAG] Sample 222: label=3\n",
      "[HOAG] theta now: [-3.9170814 -0.8700502]\n",
      "\n",
      "[HOAG] Sample 223: label=9\n",
      "[HOAG] theta now: [-3.926015  -0.8621727]\n",
      "\n",
      "[HOAG] Sample 224: label=8\n",
      "[HOAG] theta now: [-3.9266357 -0.8617515]\n",
      "\n",
      "[HOAG] Sample 225: label=3\n",
      "[HOAG] theta now: [-3.9275045 -0.8610292]\n",
      "\n",
      "[HOAG] Sample 226: label=8\n",
      "[HOAG] theta now: [-3.9303396 -0.8582621]\n",
      "\n",
      "[HOAG] Sample 227: label=9\n",
      "[HOAG] theta now: [-3.9376147  -0.85275245]\n",
      "\n",
      "[HOAG] Sample 228: label=1\n",
      "[HOAG] theta now: [-3.947801 -0.845189]\n",
      "\n",
      "[HOAG] Sample 229: label=5\n",
      "[HOAG] theta now: [-3.9548926  -0.83988917]\n",
      "\n",
      "[HOAG] Sample 230: label=2\n",
      "[HOAG] theta now: [-3.954064  -0.8414018]\n",
      "\n",
      "[HOAG] Sample 231: label=7\n",
      "[HOAG] theta now: [-3.9588213  -0.83790696]\n",
      "\n",
      "[HOAG] Sample 232: label=1\n",
      "[HOAG] theta now: [-3.9669755 -0.8308136]\n",
      "\n",
      "[HOAG] Sample 233: label=0\n",
      "[HOAG] theta now: [-3.967732 -0.830248]\n",
      "\n",
      "[HOAG] Sample 234: label=0\n",
      "[HOAG] theta now: [-3.9743345  -0.82411575]\n",
      "\n",
      "[HOAG] Sample 235: label=6\n",
      "[HOAG] theta now: [-3.9804122  -0.81959176]\n",
      "\n",
      "[HOAG] Sample 236: label=8\n",
      "[HOAG] theta now: [-3.987144   -0.81366765]\n",
      "\n",
      "[HOAG] Sample 237: label=0\n",
      "[HOAG] theta now: [-3.9878626  -0.81257105]\n",
      "\n",
      "[HOAG] Sample 238: label=8\n",
      "[HOAG] theta now: [-3.9942758 -0.8069695]\n",
      "\n",
      "[HOAG] Sample 239: label=3\n",
      "[HOAG] theta now: [-3.9981391 -0.803983 ]\n",
      "\n",
      "[HOAG] Sample 240: label=9\n",
      "[HOAG] theta now: [-4.001555  -0.8005711]\n",
      "\n",
      "[HOAG] Sample 241: label=3\n",
      "[HOAG] theta now: [-4.0029573 -0.799394 ]\n",
      "\n",
      "[HOAG] Sample 242: label=4\n",
      "[HOAG] theta now: [-4.003798  -0.7986912]\n",
      "\n",
      "[HOAG] Sample 243: label=7\n",
      "[HOAG] theta now: [-4.012209   -0.79198134]\n",
      "\n",
      "[HOAG] Sample 244: label=3\n",
      "[HOAG] theta now: [-4.015218  -0.7895118]\n",
      "\n",
      "[HOAG] Sample 245: label=0\n",
      "[HOAG] theta now: [-4.0206795 -0.7847305]\n",
      "\n",
      "[HOAG] Sample 246: label=3\n",
      "[HOAG] theta now: [-4.0219398  -0.78379065]\n",
      "\n",
      "[HOAG] Sample 247: label=7\n",
      "[HOAG] theta now: [-4.030916   -0.77614015]\n",
      "\n",
      "[HOAG] Sample 248: label=9\n",
      "[HOAG] theta now: [-4.038171  -0.7697686]\n",
      "\n",
      "[HOAG] Sample 249: label=1\n",
      "[HOAG] theta now: [-4.046813  -0.7627003]\n",
      "\n",
      "[HOAG] Sample 250: label=3\n",
      "[HOAG] theta now: [-4.048957   -0.76082176]\n",
      "\n",
      "[HOAG] Sample 251: label=0\n",
      "[HOAG] theta now: [-4.058062  -0.7534994]\n",
      "\n",
      "[HOAG] Sample 252: label=8\n",
      "[HOAG] theta now: [-4.0629654 -0.7493062]\n",
      "\n",
      "[HOAG] Sample 253: label=4\n",
      "[HOAG] theta now: [-4.063568   -0.74855447]\n",
      "\n",
      "[HOAG] Sample 254: label=2\n",
      "[HOAG] theta now: [-4.0595765 -0.7531058]\n",
      "\n",
      "[HOAG] Sample 255: label=0\n",
      "[HOAG] theta now: [-4.0603065  -0.75249964]\n",
      "\n",
      "[HOAG] Sample 256: label=3\n",
      "[HOAG] theta now: [-4.0633383 -0.7492602]\n",
      "\n",
      "[HOAG] Sample 257: label=5\n",
      "[HOAG] theta now: [-4.057013  -0.7540679]\n",
      "\n",
      "[HOAG] Sample 258: label=7\n",
      "[HOAG] theta now: [-4.0595403 -0.752274 ]\n",
      "\n",
      "[HOAG] Sample 259: label=0\n",
      "[HOAG] theta now: [-4.0643187  -0.74809134]\n",
      "\n",
      "[HOAG] Sample 260: label=9\n",
      "[HOAG] theta now: [-4.0688963 -0.7439013]\n",
      "\n",
      "[HOAG] Sample 261: label=0\n",
      "[HOAG] theta now: [-4.0743413 -0.7393682]\n",
      "\n",
      "[HOAG] Sample 262: label=8\n",
      "[HOAG] theta now: [-4.069261  -0.7446774]\n",
      "\n",
      "[HOAG] Sample 263: label=4\n",
      "[HOAG] theta now: [-4.068219  -0.7456618]\n",
      "\n",
      "[HOAG] Sample 264: label=0\n",
      "[HOAG] theta now: [-4.069443  -0.7446903]\n",
      "\n",
      "[HOAG] Sample 265: label=7\n",
      "[HOAG] theta now: [-4.0723734 -0.7392439]\n",
      "\n",
      "[HOAG] Sample 266: label=6\n",
      "[HOAG] theta now: [-4.0784054 -0.7336307]\n",
      "\n",
      "[HOAG] Sample 267: label=5\n",
      "[HOAG] theta now: [-4.0776753 -0.7336058]\n",
      "\n",
      "[HOAG] Sample 268: label=3\n",
      "[HOAG] theta now: [-4.0752296 -0.7354958]\n",
      "\n",
      "[HOAG] Sample 269: label=7\n",
      "[HOAG] theta now: [-4.077519 -0.733628]\n",
      "\n",
      "[HOAG] Sample 270: label=8\n",
      "[HOAG] theta now: [-4.077279  -0.7340076]\n",
      "\n",
      "[HOAG] Sample 271: label=2\n",
      "[HOAG] theta now: [-4.07741   -0.7338979]\n",
      "\n",
      "[HOAG] Sample 272: label=3\n",
      "[HOAG] theta now: [-4.082988   -0.72863656]\n",
      "\n",
      "[HOAG] Sample 273: label=0\n",
      "[HOAG] theta now: [-4.092151   -0.72031593]\n",
      "\n",
      "[HOAG] Sample 274: label=7\n",
      "[HOAG] theta now: [-4.097329  -0.7158261]\n",
      "\n",
      "[HOAG] Sample 275: label=7\n",
      "[HOAG] theta now: [-4.103075  -0.7110949]\n",
      "\n",
      "[HOAG] Sample 276: label=8\n",
      "[HOAG] theta now: [-4.10586   -0.7086764]\n",
      "\n",
      "[HOAG] Sample 277: label=2\n",
      "[HOAG] theta now: [-4.106902   -0.70776975]\n",
      "\n",
      "[HOAG] Sample 278: label=5\n",
      "[HOAG] theta now: [-4.111617  -0.7041589]\n",
      "\n",
      "[HOAG] Sample 279: label=6\n",
      "[HOAG] theta now: [-4.106713  -0.7078051]\n",
      "\n",
      "[HOAG] Sample 280: label=8\n",
      "[HOAG] theta now: [-4.1112     -0.70403814]\n",
      "\n",
      "[HOAG] Sample 281: label=1\n",
      "[HOAG] theta now: [-4.1184464  -0.69791734]\n",
      "\n",
      "[HOAG] Sample 282: label=1\n",
      "[HOAG] theta now: [-4.123186   -0.69408363]\n",
      "\n",
      "[HOAG] Sample 283: label=0\n",
      "[HOAG] theta now: [-4.1292696 -0.6885689]\n",
      "\n",
      "[HOAG] Sample 284: label=3\n",
      "[HOAG] theta now: [-4.1318054  -0.68651134]\n",
      "\n",
      "[HOAG] Sample 285: label=9\n",
      "[HOAG] theta now: [-4.138703   -0.68078464]\n",
      "\n",
      "[HOAG] Sample 286: label=5\n",
      "[HOAG] theta now: [-4.143491   -0.67692393]\n",
      "\n",
      "[HOAG] Sample 287: label=6\n",
      "[HOAG] theta now: [-4.148914  -0.6720746]\n",
      "\n",
      "[HOAG] Sample 288: label=7\n",
      "[HOAG] theta now: [-4.1502705  -0.67089826]\n",
      "\n",
      "[HOAG] Sample 289: label=1\n",
      "[HOAG] theta now: [-4.1625214 -0.6604801]\n",
      "\n",
      "[HOAG] Sample 290: label=3\n",
      "[HOAG] theta now: [-4.1658773  -0.65688217]\n",
      "\n",
      "[HOAG] Sample 291: label=1\n",
      "[HOAG] theta now: [-4.179072 -0.646482]\n",
      "\n",
      "[HOAG] Sample 292: label=2\n",
      "[HOAG] theta now: [-4.1791925  -0.64608693]\n",
      "\n",
      "[HOAG] Sample 293: label=7\n",
      "[HOAG] theta now: [-4.1867046 -0.6397754]\n",
      "\n",
      "[HOAG] Sample 294: label=3\n",
      "[HOAG] theta now: [-4.1826196 -0.6451133]\n",
      "\n",
      "[HOAG] Sample 295: label=4\n",
      "[HOAG] theta now: [-4.184592  -0.6434348]\n",
      "\n",
      "[HOAG] Sample 296: label=5\n",
      "[HOAG] theta now: [-4.187281  -0.6409346]\n",
      "\n",
      "[HOAG] Sample 297: label=3\n",
      "[HOAG] theta now: [-4.19073   -0.6378996]\n",
      "\n",
      "[HOAG] Sample 298: label=3\n",
      "[HOAG] theta now: [-4.193289   -0.63564026]\n",
      "\n",
      "[HOAG] Sample 299: label=3\n",
      "[HOAG] theta now: [-4.1970572  -0.63260615]\n",
      "\n",
      "[HOAG] Sample 300: label=7\n",
      "[HOAG] theta now: [-4.2018623  -0.62839127]\n",
      "\n",
      "[HOAG] Sample 301: label=9\n",
      "[HOAG] theta now: [-4.210009   -0.62105507]\n",
      "\n",
      "[HOAG] Sample 302: label=3\n",
      "[HOAG] theta now: [-4.2131624  -0.61818033]\n",
      "\n",
      "[HOAG] Sample 303: label=8\n",
      "[HOAG] theta now: [-4.21935  -0.612403]\n",
      "\n",
      "[HOAG] Sample 304: label=9\n",
      "[HOAG] theta now: [-4.2259684  -0.60702074]\n",
      "\n",
      "[HOAG] Sample 305: label=5\n",
      "[HOAG] theta now: [-4.2279224 -0.6054998]\n",
      "\n",
      "[HOAG] Sample 306: label=8\n",
      "[HOAG] theta now: [-4.229192  -0.6044013]\n",
      "\n",
      "[HOAG] Sample 307: label=0\n",
      "[HOAG] theta now: [-4.231082  -0.6028068]\n",
      "\n",
      "[HOAG] Sample 308: label=9\n",
      "[HOAG] theta now: [-4.2361436 -0.5981368]\n",
      "\n",
      "[HOAG] Sample 309: label=3\n",
      "[HOAG] theta now: [-4.2409735 -0.593932 ]\n",
      "\n",
      "[HOAG] Sample 310: label=9\n",
      "[HOAG] theta now: [-4.245018  -0.5901461]\n",
      "\n",
      "[HOAG] Sample 311: label=5\n",
      "[HOAG] theta now: [-4.248161  -0.5873805]\n",
      "\n",
      "[HOAG] Sample 312: label=5\n",
      "[HOAG] theta now: [-4.25265   -0.5835223]\n",
      "\n",
      "[HOAG] Sample 313: label=3\n",
      "[HOAG] theta now: [-4.255981   -0.58139735]\n",
      "\n",
      "[HOAG] Sample 314: label=8\n",
      "[HOAG] theta now: [-4.259034   -0.57880133]\n",
      "\n",
      "[HOAG] Sample 315: label=9\n",
      "[HOAG] theta now: [-4.2642527  -0.57422477]\n",
      "\n",
      "[HOAG] Sample 316: label=8\n",
      "[HOAG] theta now: [-4.2675214 -0.5710259]\n",
      "\n",
      "[HOAG] Sample 317: label=8\n",
      "[HOAG] theta now: [-4.2693863 -0.5694074]\n",
      "\n",
      "[HOAG] Sample 318: label=0\n",
      "[HOAG] theta now: [-4.2756433 -0.5634676]\n",
      "\n",
      "[HOAG] Sample 319: label=8\n",
      "[HOAG] theta now: [-4.2814755 -0.5590914]\n",
      "\n",
      "[HOAG] Sample 320: label=7\n",
      "[HOAG] theta now: [-4.281911   -0.55868965]\n",
      "\n",
      "[HOAG] Sample 321: label=8\n",
      "[HOAG] theta now: [-4.280813  -0.5593621]\n",
      "\n",
      "[HOAG] Sample 322: label=8\n",
      "[HOAG] theta now: [-4.2827153 -0.5577398]\n",
      "\n",
      "[HOAG] Sample 323: label=7\n",
      "[HOAG] theta now: [-4.287459   -0.55346674]\n",
      "\n",
      "[HOAG] Sample 324: label=3\n",
      "[HOAG] theta now: [-4.290948  -0.5504011]\n",
      "\n",
      "[HOAG] Sample 325: label=3\n",
      "[HOAG] theta now: [-4.292413   -0.54900646]\n",
      "\n",
      "[HOAG] Sample 326: label=0\n",
      "[HOAG] theta now: [-4.298061   -0.54379886]\n",
      "\n",
      "[HOAG] Sample 327: label=1\n",
      "[HOAG] theta now: [-4.306112  -0.5369946]\n",
      "\n",
      "[HOAG] Sample 328: label=0\n",
      "[HOAG] theta now: [-4.310781  -0.5327432]\n",
      "\n",
      "[HOAG] Sample 329: label=0\n",
      "[HOAG] theta now: [-4.3185678 -0.5262127]\n",
      "\n",
      "[HOAG] Sample 330: label=5\n",
      "[HOAG] theta now: [-4.322899   -0.52201813]\n",
      "\n",
      "[HOAG] Sample 331: label=1\n",
      "[HOAG] theta now: [-4.324499   -0.52065146]\n",
      "\n",
      "[HOAG] Sample 332: label=4\n",
      "[HOAG] theta now: [-4.32638   -0.5189288]\n",
      "\n",
      "[HOAG] Sample 333: label=1\n",
      "[HOAG] theta now: [-4.3387136 -0.5084037]\n",
      "\n",
      "[HOAG] Sample 334: label=3\n",
      "[HOAG] theta now: [-4.3420563  -0.50522125]\n",
      "\n",
      "[HOAG] Sample 335: label=6\n",
      "[HOAG] theta now: [-4.345013  -0.5024785]\n",
      "\n",
      "[HOAG] Sample 336: label=1\n",
      "[HOAG] theta now: [-4.3529177  -0.49574247]\n",
      "\n",
      "[HOAG] Sample 337: label=6\n",
      "[HOAG] theta now: [-4.354452   -0.49424344]\n",
      "\n",
      "[HOAG] Sample 338: label=5\n",
      "[HOAG] theta now: [-4.357015  -0.4920174]\n",
      "\n",
      "[HOAG] Sample 339: label=9\n",
      "[HOAG] theta now: [-4.363999   -0.48542508]\n",
      "\n",
      "[HOAG] Sample 340: label=5\n",
      "[HOAG] theta now: [-4.3631086  -0.48649588]\n",
      "\n",
      "[HOAG] Sample 341: label=1\n",
      "[HOAG] theta now: [-4.363845   -0.48455682]\n",
      "\n",
      "[HOAG] Sample 342: label=8\n",
      "[HOAG] theta now: [-4.3658004 -0.4829896]\n",
      "\n",
      "[HOAG] Sample 343: label=3\n",
      "[HOAG] theta now: [-4.36874    -0.48031336]\n",
      "\n",
      "[HOAG] Sample 344: label=1\n",
      "[HOAG] theta now: [-4.3779073  -0.47193018]\n",
      "\n",
      "[HOAG] Sample 345: label=7\n",
      "[HOAG] theta now: [-4.3836603  -0.46665177]\n",
      "\n",
      "[HOAG] Sample 346: label=2\n",
      "[HOAG] theta now: [-4.383817   -0.46648985]\n",
      "\n",
      "[HOAG] Sample 347: label=1\n",
      "[HOAG] theta now: [-4.3919854  -0.45971614]\n",
      "\n",
      "[HOAG] Sample 348: label=8\n",
      "[HOAG] theta now: [-4.3953395  -0.45661935]\n",
      "\n",
      "[HOAG] Sample 349: label=4\n",
      "[HOAG] theta now: [-4.390152   -0.46205917]\n",
      "\n",
      "[HOAG] Sample 350: label=8\n",
      "[HOAG] theta now: [-4.3910923 -0.4611955]\n",
      "\n",
      "[HOAG] Sample 351: label=3\n",
      "[HOAG] theta now: [-4.3918915  -0.46054253]\n",
      "\n",
      "[HOAG] Sample 352: label=8\n",
      "[HOAG] theta now: [-4.3947177  -0.45798308]\n",
      "\n",
      "[HOAG] Sample 353: label=3\n",
      "[HOAG] theta now: [-4.400472   -0.45259807]\n",
      "\n",
      "[HOAG] Sample 354: label=8\n",
      "[HOAG] theta now: [-4.4023385 -0.4508749]\n",
      "\n",
      "[HOAG] Sample 355: label=8\n",
      "[HOAG] theta now: [-4.4055166 -0.4477647]\n",
      "\n",
      "[HOAG] Sample 356: label=7\n",
      "[HOAG] theta now: [-4.410878   -0.44273213]\n",
      "\n",
      "[HOAG] Sample 357: label=0\n",
      "[HOAG] theta now: [-4.414688   -0.43933722]\n",
      "\n",
      "[HOAG] Sample 358: label=9\n",
      "[HOAG] theta now: [-4.4203997 -0.4339863]\n",
      "\n",
      "[HOAG] Sample 359: label=7\n",
      "[HOAG] theta now: [-4.426056  -0.4288776]\n",
      "\n",
      "[HOAG] Sample 360: label=0\n",
      "[HOAG] theta now: [-4.4286795  -0.42637712]\n",
      "\n",
      "[HOAG] Sample 361: label=8\n",
      "[HOAG] theta now: [-4.4316945 -0.4236689]\n",
      "\n",
      "[HOAG] Sample 362: label=1\n",
      "[HOAG] theta now: [-4.433838  -0.4216354]\n",
      "\n",
      "[HOAG] Sample 363: label=8\n",
      "[HOAG] theta now: [-4.439161  -0.4166175]\n",
      "\n",
      "[HOAG] Sample 364: label=6\n",
      "[HOAG] theta now: [-4.4449596  -0.41159415]\n",
      "\n",
      "[HOAG] Sample 365: label=3\n",
      "[HOAG] theta now: [-4.4494777  -0.40777704]\n",
      "\n",
      "[HOAG] Sample 366: label=3\n",
      "[HOAG] theta now: [-4.451917   -0.40550253]\n",
      "\n",
      "[HOAG] Sample 367: label=8\n",
      "[HOAG] theta now: [-4.454571   -0.40304998]\n",
      "\n",
      "[HOAG] Sample 368: label=7\n",
      "[HOAG] theta now: [-4.4553704  -0.40232325]\n",
      "\n",
      "[HOAG] Sample 369: label=5\n",
      "[HOAG] theta now: [-4.4582267  -0.40002695]\n",
      "\n",
      "[HOAG] Sample 370: label=0\n",
      "[HOAG] theta now: [-4.4597697 -0.3986473]\n",
      "\n",
      "[HOAG] Sample 371: label=8\n",
      "[HOAG] theta now: [-4.4603906  -0.39802828]\n",
      "\n",
      "[HOAG] Sample 372: label=2\n",
      "[HOAG] theta now: [-4.46054   -0.3977661]\n",
      "\n",
      "[HOAG] Sample 373: label=0\n",
      "[HOAG] theta now: [-4.4640393  -0.39441586]\n",
      "\n",
      "[HOAG] Sample 374: label=9\n",
      "[HOAG] theta now: [-4.4665456  -0.39204457]\n",
      "\n",
      "[HOAG] Sample 375: label=1\n",
      "[HOAG] theta now: [-4.470833  -0.3881866]\n",
      "\n",
      "[HOAG] Sample 376: label=5\n",
      "[HOAG] theta now: [-4.468817   -0.39002293]\n",
      "\n",
      "[HOAG] Sample 377: label=7\n",
      "[HOAG] theta now: [-4.471731   -0.38710076]\n",
      "\n",
      "[HOAG] Sample 378: label=8\n",
      "[HOAG] theta now: [-4.473757   -0.38508675]\n",
      "\n",
      "[HOAG] Sample 379: label=1\n",
      "[HOAG] theta now: [-4.4768457 -0.3823343]\n",
      "\n",
      "[HOAG] Sample 380: label=2\n",
      "[HOAG] theta now: [-4.476976   -0.38219184]\n",
      "\n",
      "[HOAG] Sample 381: label=1\n",
      "[HOAG] theta now: [-4.483313   -0.37634167]\n",
      "\n",
      "[HOAG] Sample 382: label=8\n",
      "[HOAG] theta now: [-4.4837055  -0.37598392]\n",
      "\n",
      "[HOAG] Sample 383: label=3\n",
      "[HOAG] theta now: [-4.4863944  -0.37348306]\n",
      "\n",
      "[HOAG] Sample 384: label=9\n",
      "[HOAG] theta now: [-4.490169   -0.36992496]\n",
      "\n",
      "[HOAG] Sample 385: label=5\n",
      "[HOAG] theta now: [-4.4922934  -0.36797023]\n",
      "\n",
      "[HOAG] Sample 386: label=5\n",
      "[HOAG] theta now: [-4.491729   -0.36824185]\n",
      "\n",
      "[HOAG] Sample 387: label=3\n",
      "[HOAG] theta now: [-4.490632  -0.3691566]\n",
      "\n",
      "[HOAG] Sample 388: label=3\n",
      "[HOAG] theta now: [-4.492428   -0.36747688]\n",
      "\n",
      "[HOAG] Sample 389: label=3\n",
      "[HOAG] theta now: [-4.488723   -0.37088037]\n",
      "\n",
      "[HOAG] Sample 390: label=2\n",
      "[HOAG] theta now: [-4.4891357  -0.37049755]\n",
      "\n",
      "[HOAG] Sample 391: label=2\n",
      "[HOAG] theta now: [-4.4889464  -0.37041646]\n",
      "\n",
      "[HOAG] Sample 392: label=3\n",
      "[HOAG] theta now: [-4.490567   -0.36869904]\n",
      "\n",
      "[HOAG] Sample 393: label=8\n",
      "[HOAG] theta now: [-4.4929714  -0.36643684]\n",
      "\n",
      "[HOAG] Sample 394: label=4\n",
      "[HOAG] theta now: [-4.4932017  -0.36591828]\n",
      "\n",
      "[HOAG] Sample 395: label=6\n",
      "[HOAG] theta now: [-4.496911   -0.36270717]\n",
      "\n",
      "[HOAG] Sample 396: label=3\n",
      "[HOAG] theta now: [-4.510681   -0.34956393]\n",
      "\n",
      "[HOAG] Sample 397: label=2\n",
      "[HOAG] theta now: [-4.510755   -0.34949228]\n",
      "\n",
      "[HOAG] Sample 398: label=8\n",
      "[HOAG] theta now: [-4.5127707  -0.34772417]\n",
      "\n",
      "[HOAG] Sample 399: label=2\n",
      "[HOAG] theta now: [-4.511269   -0.34918132]\n",
      "\n",
      "[HOAG] Sample 400: label=7\n",
      "[HOAG] theta now: [-4.5122647  -0.34826928]\n",
      "[HOAG] quick eval after sample 400\n",
      "\n",
      "--- Evaluating Model on Test Set (Iterative Deblurring) ---\n",
      "[eval] sample 0 iter 0 grad_norm=6.3188e-01, inner_loss=6.7471e+00\n",
      "[eval] sample 0 iter 10 grad_norm=6.0232e-01, inner_loss=6.7089e+00\n",
      "[eval] sample 0 iter 20 grad_norm=5.7462e-01, inner_loss=6.6742e+00\n",
      "[eval] sample 0 iter 30 grad_norm=5.4863e-01, inner_loss=6.6426e+00\n",
      "[eval] sample 0 iter 40 grad_norm=5.2425e-01, inner_loss=6.6138e+00\n",
      "[eval] sample 1 iter 0 grad_norm=7.7680e-01, inner_loss=7.1382e+00\n",
      "[eval] sample 1 iter 10 grad_norm=7.4095e-01, inner_loss=7.0805e+00\n",
      "[eval] sample 1 iter 20 grad_norm=7.0741e-01, inner_loss=7.0280e+00\n",
      "[eval] sample 1 iter 30 grad_norm=6.7601e-01, inner_loss=6.9800e+00\n",
      "[eval] sample 1 iter 40 grad_norm=6.4660e-01, inner_loss=6.9362e+00\n",
      "[eval] sample 2 iter 0 grad_norm=4.4585e-01, inner_loss=6.4272e+00\n",
      "[eval] sample 2 iter 10 grad_norm=4.2556e-01, inner_loss=6.4082e+00\n",
      "[eval] sample 2 iter 20 grad_norm=4.0655e-01, inner_loss=6.3909e+00\n",
      "[eval] sample 2 iter 30 grad_norm=3.8871e-01, inner_loss=6.3750e+00\n",
      "[eval] sample 2 iter 40 grad_norm=3.7196e-01, inner_loss=6.3605e+00\n",
      "[eval] sample 3 iter 0 grad_norm=9.5431e-01, inner_loss=7.3748e+00\n",
      "[eval] sample 3 iter 10 grad_norm=9.0925e-01, inner_loss=7.2878e+00\n",
      "[eval] sample 3 iter 20 grad_norm=8.6676e-01, inner_loss=7.2088e+00\n",
      "[eval] sample 3 iter 30 grad_norm=8.2668e-01, inner_loss=7.1370e+00\n",
      "[eval] sample 3 iter 40 grad_norm=7.8885e-01, inner_loss=7.0716e+00\n",
      "[eval] sample 4 iter 0 grad_norm=6.1329e-01, inner_loss=6.7318e+00\n",
      "[eval] sample 4 iter 10 grad_norm=5.8642e-01, inner_loss=6.6958e+00\n",
      "[eval] sample 4 iter 20 grad_norm=5.6109e-01, inner_loss=6.6628e+00\n",
      "[eval] sample 4 iter 30 grad_norm=5.3719e-01, inner_loss=6.6326e+00\n",
      "[eval] sample 4 iter 40 grad_norm=5.1464e-01, inner_loss=6.6049e+00\n",
      "[eval] sample 5 iter 0 grad_norm=5.9277e-01, inner_loss=6.6445e+00\n",
      "[eval] sample 5 iter 10 grad_norm=5.6476e-01, inner_loss=6.6109e+00\n",
      "[eval] sample 5 iter 20 grad_norm=5.3852e-01, inner_loss=6.5804e+00\n",
      "[eval] sample 5 iter 30 grad_norm=5.1392e-01, inner_loss=6.5527e+00\n",
      "[eval] sample 5 iter 40 grad_norm=4.9085e-01, inner_loss=6.5274e+00\n",
      "[eval] sample 6 iter 0 grad_norm=6.3850e-01, inner_loss=6.7786e+00\n",
      "[eval] sample 6 iter 10 grad_norm=6.0987e-01, inner_loss=6.7396e+00\n",
      "[eval] sample 6 iter 20 grad_norm=5.8294e-01, inner_loss=6.7039e+00\n",
      "[eval] sample 6 iter 30 grad_norm=5.5761e-01, inner_loss=6.6713e+00\n",
      "[eval] sample 6 iter 40 grad_norm=5.3376e-01, inner_loss=6.6415e+00\n",
      "[eval] sample 7 iter 0 grad_norm=5.8559e-01, inner_loss=6.6889e+00\n",
      "[eval] sample 7 iter 10 grad_norm=5.5583e-01, inner_loss=6.6563e+00\n",
      "[eval] sample 7 iter 20 grad_norm=5.2823e-01, inner_loss=6.6268e+00\n",
      "[eval] sample 7 iter 30 grad_norm=5.0264e-01, inner_loss=6.6002e+00\n",
      "[eval] sample 7 iter 40 grad_norm=4.7889e-01, inner_loss=6.5761e+00\n",
      "[eval] sample 8 iter 0 grad_norm=8.6474e-01, inner_loss=7.1203e+00\n",
      "[eval] sample 8 iter 10 grad_norm=8.2067e-01, inner_loss=7.0491e+00\n",
      "[eval] sample 8 iter 20 grad_norm=7.7932e-01, inner_loss=6.9850e+00\n",
      "[eval] sample 8 iter 30 grad_norm=7.4051e-01, inner_loss=6.9271e+00\n",
      "[eval] sample 8 iter 40 grad_norm=7.0407e-01, inner_loss=6.8748e+00\n",
      "[eval] sample 9 iter 0 grad_norm=7.4232e-01, inner_loss=6.9915e+00\n",
      "[eval] sample 9 iter 10 grad_norm=7.0680e-01, inner_loss=6.9389e+00\n",
      "[eval] sample 9 iter 20 grad_norm=6.7366e-01, inner_loss=6.8912e+00\n",
      "[eval] sample 9 iter 30 grad_norm=6.4271e-01, inner_loss=6.8478e+00\n",
      "[eval] sample 9 iter 40 grad_norm=6.1378e-01, inner_loss=6.8082e+00\n",
      "[eval] sample 10 iter 0 grad_norm=8.1251e-01, inner_loss=7.1983e+00\n",
      "[eval] sample 10 iter 10 grad_norm=7.7541e-01, inner_loss=7.1351e+00\n",
      "[eval] sample 10 iter 20 grad_norm=7.4054e-01, inner_loss=7.0775e+00\n",
      "[eval] sample 10 iter 30 grad_norm=7.0774e-01, inner_loss=7.0250e+00\n",
      "[eval] sample 10 iter 40 grad_norm=6.7687e-01, inner_loss=6.9770e+00\n",
      "[eval] sample 11 iter 0 grad_norm=6.0490e-01, inner_loss=6.8728e+00\n",
      "[eval] sample 11 iter 10 grad_norm=5.8068e-01, inner_loss=6.8376e+00\n",
      "[eval] sample 11 iter 20 grad_norm=5.5797e-01, inner_loss=6.8052e+00\n",
      "[eval] sample 11 iter 30 grad_norm=5.3664e-01, inner_loss=6.7752e+00\n",
      "[eval] sample 11 iter 40 grad_norm=5.1660e-01, inner_loss=6.7474e+00\n",
      "[eval] sample 12 iter 0 grad_norm=6.1334e-01, inner_loss=6.8185e+00\n",
      "[eval] sample 12 iter 10 grad_norm=5.8725e-01, inner_loss=6.7824e+00\n",
      "[eval] sample 12 iter 20 grad_norm=5.6289e-01, inner_loss=6.7493e+00\n",
      "[eval] sample 12 iter 30 grad_norm=5.4011e-01, inner_loss=6.7188e+00\n",
      "[eval] sample 12 iter 40 grad_norm=5.1878e-01, inner_loss=6.6907e+00\n",
      "[eval] sample 13 iter 0 grad_norm=8.2958e-01, inner_loss=7.1869e+00\n",
      "[eval] sample 13 iter 10 grad_norm=7.9127e-01, inner_loss=7.1211e+00\n",
      "[eval] sample 13 iter 20 grad_norm=7.5519e-01, inner_loss=7.0612e+00\n",
      "[eval] sample 13 iter 30 grad_norm=7.2119e-01, inner_loss=7.0066e+00\n",
      "[eval] sample 13 iter 40 grad_norm=6.8914e-01, inner_loss=6.9568e+00\n",
      "[eval] sample 14 iter 0 grad_norm=6.6758e-01, inner_loss=6.7685e+00\n",
      "[eval] sample 14 iter 10 grad_norm=6.3555e-01, inner_loss=6.7259e+00\n",
      "[eval] sample 14 iter 20 grad_norm=6.0555e-01, inner_loss=6.6873e+00\n",
      "[eval] sample 14 iter 30 grad_norm=5.7745e-01, inner_loss=6.6523e+00\n",
      "[eval] sample 14 iter 40 grad_norm=5.5109e-01, inner_loss=6.6204e+00\n",
      "[eval] sample 15 iter 0 grad_norm=5.9922e-01, inner_loss=6.7682e+00\n",
      "[eval] sample 15 iter 10 grad_norm=5.7364e-01, inner_loss=6.7338e+00\n",
      "[eval] sample 15 iter 20 grad_norm=5.4960e-01, inner_loss=6.7022e+00\n",
      "[eval] sample 15 iter 30 grad_norm=5.2699e-01, inner_loss=6.6732e+00\n",
      "[eval] sample 15 iter 40 grad_norm=5.0572e-01, inner_loss=6.6465e+00\n",
      "[eval] sample 16 iter 0 grad_norm=6.7232e-01, inner_loss=6.8823e+00\n",
      "[eval] sample 16 iter 10 grad_norm=6.4404e-01, inner_loss=6.8389e+00\n",
      "[eval] sample 16 iter 20 grad_norm=6.1735e-01, inner_loss=6.7991e+00\n",
      "[eval] sample 16 iter 30 grad_norm=5.9215e-01, inner_loss=6.7624e+00\n",
      "[eval] sample 16 iter 40 grad_norm=5.6833e-01, inner_loss=6.7287e+00\n",
      "[eval] sample 17 iter 0 grad_norm=7.4530e-01, inner_loss=6.9769e+00\n",
      "[eval] sample 17 iter 10 grad_norm=7.1006e-01, inner_loss=6.9238e+00\n",
      "[eval] sample 17 iter 20 grad_norm=6.7702e-01, inner_loss=6.8756e+00\n",
      "[eval] sample 17 iter 30 grad_norm=6.4604e-01, inner_loss=6.8318e+00\n",
      "[eval] sample 17 iter 40 grad_norm=6.1697e-01, inner_loss=6.7918e+00\n",
      "[eval] sample 18 iter 0 grad_norm=7.8766e-01, inner_loss=7.2523e+00\n",
      "[eval] sample 18 iter 10 grad_norm=7.5486e-01, inner_loss=7.1927e+00\n",
      "[eval] sample 18 iter 20 grad_norm=7.2403e-01, inner_loss=7.1380e+00\n",
      "[eval] sample 18 iter 30 grad_norm=6.9502e-01, inner_loss=7.0875e+00\n",
      "[eval] sample 18 iter 40 grad_norm=6.6770e-01, inner_loss=7.0410e+00\n",
      "[eval] sample 19 iter 0 grad_norm=5.2231e-01, inner_loss=6.5859e+00\n",
      "[eval] sample 19 iter 10 grad_norm=5.0033e-01, inner_loss=6.5597e+00\n",
      "[eval] sample 19 iter 20 grad_norm=4.7971e-01, inner_loss=6.5357e+00\n",
      "[eval] sample 19 iter 30 grad_norm=4.6033e-01, inner_loss=6.5135e+00\n",
      "[eval] sample 19 iter 40 grad_norm=4.4211e-01, inner_loss=6.4932e+00\n",
      "[eval] sample 20 iter 0 grad_norm=7.1324e-01, inner_loss=6.8237e+00\n",
      "[eval] sample 20 iter 10 grad_norm=6.7554e-01, inner_loss=6.7754e+00\n",
      "[eval] sample 20 iter 20 grad_norm=6.4049e-01, inner_loss=6.7320e+00\n",
      "[eval] sample 20 iter 30 grad_norm=6.0790e-01, inner_loss=6.6929e+00\n",
      "[eval] sample 20 iter 40 grad_norm=5.7756e-01, inner_loss=6.6577e+00\n",
      "[eval] sample 21 iter 0 grad_norm=6.2328e-01, inner_loss=6.8390e+00\n",
      "[eval] sample 21 iter 10 grad_norm=5.9688e-01, inner_loss=6.8017e+00\n",
      "[eval] sample 21 iter 20 grad_norm=5.7219e-01, inner_loss=6.7675e+00\n",
      "[eval] sample 21 iter 30 grad_norm=5.4909e-01, inner_loss=6.7360e+00\n",
      "[eval] sample 21 iter 40 grad_norm=5.2744e-01, inner_loss=6.7070e+00\n",
      "[eval] sample 22 iter 0 grad_norm=6.0822e-01, inner_loss=6.7954e+00\n",
      "[eval] sample 22 iter 10 grad_norm=5.8155e-01, inner_loss=6.7599e+00\n",
      "[eval] sample 22 iter 20 grad_norm=5.5658e-01, inner_loss=6.7275e+00\n",
      "[eval] sample 22 iter 30 grad_norm=5.3318e-01, inner_loss=6.6977e+00\n",
      "[eval] sample 22 iter 40 grad_norm=5.1122e-01, inner_loss=6.6704e+00\n",
      "[eval] sample 23 iter 0 grad_norm=6.0493e-01, inner_loss=6.7741e+00\n",
      "[eval] sample 23 iter 10 grad_norm=5.7911e-01, inner_loss=6.7390e+00\n",
      "[eval] sample 23 iter 20 grad_norm=5.5487e-01, inner_loss=6.7068e+00\n",
      "[eval] sample 23 iter 30 grad_norm=5.3209e-01, inner_loss=6.6772e+00\n",
      "[eval] sample 23 iter 40 grad_norm=5.1067e-01, inner_loss=6.6500e+00\n",
      "[eval] sample 24 iter 0 grad_norm=4.1617e-01, inner_loss=6.4217e+00\n",
      "[eval] sample 24 iter 10 grad_norm=3.9928e-01, inner_loss=6.4051e+00\n",
      "[eval] sample 24 iter 20 grad_norm=3.8341e-01, inner_loss=6.3897e+00\n",
      "[eval] sample 24 iter 30 grad_norm=3.6847e-01, inner_loss=6.3756e+00\n",
      "[eval] sample 24 iter 40 grad_norm=3.5440e-01, inner_loss=6.3625e+00\n",
      "[eval] sample 25 iter 0 grad_norm=1.0951e+00, inner_loss=7.6154e+00\n",
      "[eval] sample 25 iter 10 grad_norm=1.0402e+00, inner_loss=7.5011e+00\n",
      "[eval] sample 25 iter 20 grad_norm=9.8842e-01, inner_loss=7.3980e+00\n",
      "[eval] sample 25 iter 30 grad_norm=9.3972e-01, inner_loss=7.3049e+00\n",
      "[eval] sample 25 iter 40 grad_norm=8.9385e-01, inner_loss=7.2206e+00\n",
      "[eval] sample 26 iter 0 grad_norm=5.6098e-01, inner_loss=6.6461e+00\n",
      "[eval] sample 26 iter 10 grad_norm=5.3571e-01, inner_loss=6.6160e+00\n",
      "[eval] sample 26 iter 20 grad_norm=5.1200e-01, inner_loss=6.5885e+00\n",
      "[eval] sample 26 iter 30 grad_norm=4.8975e-01, inner_loss=6.5634e+00\n",
      "[eval] sample 26 iter 40 grad_norm=4.6884e-01, inner_loss=6.5404e+00\n",
      "[eval] sample 27 iter 0 grad_norm=6.6002e-01, inner_loss=6.8758e+00\n",
      "[eval] sample 27 iter 10 grad_norm=6.3244e-01, inner_loss=6.8340e+00\n",
      "[eval] sample 27 iter 20 grad_norm=6.0649e-01, inner_loss=6.7955e+00\n",
      "[eval] sample 27 iter 30 grad_norm=5.8207e-01, inner_loss=6.7602e+00\n",
      "[eval] sample 27 iter 40 grad_norm=5.5904e-01, inner_loss=6.7275e+00\n",
      "[eval] sample 28 iter 0 grad_norm=9.0866e-01, inner_loss=7.3364e+00\n",
      "[eval] sample 28 iter 10 grad_norm=8.6615e-01, inner_loss=7.2575e+00\n",
      "[eval] sample 28 iter 20 grad_norm=8.2617e-01, inner_loss=7.1858e+00\n",
      "[eval] sample 28 iter 30 grad_norm=7.8855e-01, inner_loss=7.1205e+00\n",
      "[eval] sample 28 iter 40 grad_norm=7.5313e-01, inner_loss=7.0609e+00\n",
      "[eval] sample 29 iter 0 grad_norm=5.5419e-01, inner_loss=6.5894e+00\n",
      "[eval] sample 29 iter 10 grad_norm=5.2843e-01, inner_loss=6.5600e+00\n",
      "[eval] sample 29 iter 20 grad_norm=5.0429e-01, inner_loss=6.5333e+00\n",
      "[eval] sample 29 iter 30 grad_norm=4.8166e-01, inner_loss=6.5090e+00\n",
      "[eval] sample 29 iter 40 grad_norm=4.6042e-01, inner_loss=6.4867e+00\n",
      "[eval] sample 30 iter 0 grad_norm=7.0347e-01, inner_loss=6.9072e+00\n",
      "[eval] sample 30 iter 10 grad_norm=6.6898e-01, inner_loss=6.8600e+00\n",
      "[eval] sample 30 iter 20 grad_norm=6.3681e-01, inner_loss=6.8173e+00\n",
      "[eval] sample 30 iter 30 grad_norm=6.0680e-01, inner_loss=6.7786e+00\n",
      "[eval] sample 30 iter 40 grad_norm=5.7879e-01, inner_loss=6.7434e+00\n",
      "[eval] sample 31 iter 0 grad_norm=4.8781e-01, inner_loss=6.4907e+00\n",
      "[eval] sample 31 iter 10 grad_norm=4.6542e-01, inner_loss=6.4679e+00\n",
      "[eval] sample 31 iter 20 grad_norm=4.4444e-01, inner_loss=6.4472e+00\n",
      "[eval] sample 31 iter 30 grad_norm=4.2476e-01, inner_loss=6.4282e+00\n",
      "[eval] sample 31 iter 40 grad_norm=4.0629e-01, inner_loss=6.4109e+00\n",
      "[eval] sample 32 iter 0 grad_norm=7.2723e-01, inner_loss=7.0344e+00\n",
      "[eval] sample 32 iter 10 grad_norm=6.9557e-01, inner_loss=6.9837e+00\n",
      "[eval] sample 32 iter 20 grad_norm=6.6592e-01, inner_loss=6.9373e+00\n",
      "[eval] sample 32 iter 30 grad_norm=6.3812e-01, inner_loss=6.8947e+00\n",
      "[eval] sample 32 iter 40 grad_norm=6.1202e-01, inner_loss=6.8556e+00\n",
      "[eval] sample 33 iter 0 grad_norm=7.1228e-01, inner_loss=6.9352e+00\n",
      "[eval] sample 33 iter 10 grad_norm=6.7866e-01, inner_loss=6.8868e+00\n",
      "[eval] sample 33 iter 20 grad_norm=6.4713e-01, inner_loss=6.8427e+00\n",
      "[eval] sample 33 iter 30 grad_norm=6.1755e-01, inner_loss=6.8027e+00\n",
      "[eval] sample 33 iter 40 grad_norm=5.8979e-01, inner_loss=6.7662e+00\n",
      "[eval] sample 34 iter 0 grad_norm=7.9501e-01, inner_loss=7.0488e+00\n",
      "[eval] sample 34 iter 10 grad_norm=7.5675e-01, inner_loss=6.9884e+00\n",
      "[eval] sample 34 iter 20 grad_norm=7.2091e-01, inner_loss=6.9337e+00\n",
      "[eval] sample 34 iter 30 grad_norm=6.8732e-01, inner_loss=6.8840e+00\n",
      "[eval] sample 34 iter 40 grad_norm=6.5581e-01, inner_loss=6.8389e+00\n",
      "[eval] sample 35 iter 0 grad_norm=7.9556e-01, inner_loss=7.1237e+00\n",
      "[eval] sample 35 iter 10 grad_norm=7.5891e-01, inner_loss=7.0631e+00\n",
      "[eval] sample 35 iter 20 grad_norm=7.2445e-01, inner_loss=7.0080e+00\n",
      "[eval] sample 35 iter 30 grad_norm=6.9203e-01, inner_loss=6.9578e+00\n",
      "[eval] sample 35 iter 40 grad_norm=6.6150e-01, inner_loss=6.9119e+00\n",
      "[eval] sample 36 iter 0 grad_norm=6.8414e-01, inner_loss=6.8327e+00\n",
      "[eval] sample 36 iter 10 grad_norm=6.5104e-01, inner_loss=6.7880e+00\n",
      "[eval] sample 36 iter 20 grad_norm=6.2001e-01, inner_loss=6.7476e+00\n",
      "[eval] sample 36 iter 30 grad_norm=5.9089e-01, inner_loss=6.7108e+00\n",
      "[eval] sample 36 iter 40 grad_norm=5.6357e-01, inner_loss=6.6774e+00\n",
      "[eval] sample 37 iter 0 grad_norm=5.9495e-01, inner_loss=6.6492e+00\n",
      "[eval] sample 37 iter 10 grad_norm=5.6683e-01, inner_loss=6.6154e+00\n",
      "[eval] sample 37 iter 20 grad_norm=5.4049e-01, inner_loss=6.5847e+00\n",
      "[eval] sample 37 iter 30 grad_norm=5.1581e-01, inner_loss=6.5568e+00\n",
      "[eval] sample 37 iter 40 grad_norm=4.9267e-01, inner_loss=6.5313e+00\n",
      "[eval] sample 38 iter 0 grad_norm=5.6224e-01, inner_loss=6.6430e+00\n",
      "[eval] sample 38 iter 10 grad_norm=5.3626e-01, inner_loss=6.6128e+00\n",
      "[eval] sample 38 iter 20 grad_norm=5.1196e-01, inner_loss=6.5853e+00\n",
      "[eval] sample 38 iter 30 grad_norm=4.8919e-01, inner_loss=6.5602e+00\n",
      "[eval] sample 38 iter 40 grad_norm=4.6786e-01, inner_loss=6.5372e+00\n",
      "[eval] sample 39 iter 0 grad_norm=7.1227e-01, inner_loss=6.7992e+00\n",
      "[eval] sample 39 iter 10 grad_norm=6.7685e-01, inner_loss=6.7508e+00\n",
      "[eval] sample 39 iter 20 grad_norm=6.4371e-01, inner_loss=6.7072e+00\n",
      "[eval] sample 39 iter 30 grad_norm=6.1269e-01, inner_loss=6.6676e+00\n",
      "[eval] sample 39 iter 40 grad_norm=5.8362e-01, inner_loss=6.6318e+00\n",
      "[eval] sample 40 iter 0 grad_norm=3.2610e-01, inner_loss=6.2757e+00\n",
      "[eval] sample 40 iter 10 grad_norm=3.1163e-01, inner_loss=6.2655e+00\n",
      "[eval] sample 40 iter 20 grad_norm=2.9806e-01, inner_loss=6.2562e+00\n",
      "[eval] sample 40 iter 30 grad_norm=2.8532e-01, inner_loss=6.2477e+00\n",
      "[eval] sample 40 iter 40 grad_norm=2.7335e-01, inner_loss=6.2399e+00\n",
      "[eval] sample 41 iter 0 grad_norm=5.7606e-01, inner_loss=6.6397e+00\n",
      "[eval] sample 41 iter 10 grad_norm=5.4993e-01, inner_loss=6.6079e+00\n",
      "[eval] sample 41 iter 20 grad_norm=5.2538e-01, inner_loss=6.5790e+00\n",
      "[eval] sample 41 iter 30 grad_norm=5.0231e-01, inner_loss=6.5525e+00\n",
      "[eval] sample 41 iter 40 grad_norm=4.8062e-01, inner_loss=6.5283e+00\n",
      "[eval] sample 42 iter 0 grad_norm=7.0384e-01, inner_loss=6.9035e+00\n",
      "[eval] sample 42 iter 10 grad_norm=6.7220e-01, inner_loss=6.8560e+00\n",
      "[eval] sample 42 iter 20 grad_norm=6.4255e-01, inner_loss=6.8127e+00\n",
      "[eval] sample 42 iter 30 grad_norm=6.1473e-01, inner_loss=6.7731e+00\n",
      "[eval] sample 42 iter 40 grad_norm=5.8861e-01, inner_loss=6.7369e+00\n",
      "[eval] sample 43 iter 0 grad_norm=5.3894e-01, inner_loss=6.5898e+00\n",
      "[eval] sample 43 iter 10 grad_norm=5.1461e-01, inner_loss=6.5620e+00\n",
      "[eval] sample 43 iter 20 grad_norm=4.9178e-01, inner_loss=6.5366e+00\n",
      "[eval] sample 43 iter 30 grad_norm=4.7035e-01, inner_loss=6.5135e+00\n",
      "[eval] sample 43 iter 40 grad_norm=4.5020e-01, inner_loss=6.4922e+00\n",
      "[eval] sample 44 iter 0 grad_norm=6.3859e-01, inner_loss=6.7731e+00\n",
      "[eval] sample 44 iter 10 grad_norm=6.0896e-01, inner_loss=6.7341e+00\n",
      "[eval] sample 44 iter 20 grad_norm=5.8126e-01, inner_loss=6.6986e+00\n",
      "[eval] sample 44 iter 30 grad_norm=5.5535e-01, inner_loss=6.6663e+00\n",
      "[eval] sample 44 iter 40 grad_norm=5.3109e-01, inner_loss=6.6367e+00\n",
      "[eval] sample 45 iter 0 grad_norm=5.5530e-01, inner_loss=6.7047e+00\n",
      "[eval] sample 45 iter 10 grad_norm=5.3371e-01, inner_loss=6.6750e+00\n",
      "[eval] sample 45 iter 20 grad_norm=5.1338e-01, inner_loss=6.6475e+00\n",
      "[eval] sample 45 iter 30 grad_norm=4.9421e-01, inner_loss=6.6221e+00\n",
      "[eval] sample 45 iter 40 grad_norm=4.7613e-01, inner_loss=6.5985e+00\n",
      "[eval] sample 46 iter 0 grad_norm=7.2763e-01, inner_loss=6.8285e+00\n",
      "[eval] sample 46 iter 10 grad_norm=6.9137e-01, inner_loss=6.7781e+00\n",
      "[eval] sample 46 iter 20 grad_norm=6.5743e-01, inner_loss=6.7325e+00\n",
      "[eval] sample 46 iter 30 grad_norm=6.2565e-01, inner_loss=6.6912e+00\n",
      "[eval] sample 46 iter 40 grad_norm=5.9587e-01, inner_loss=6.6538e+00\n",
      "[eval] sample 47 iter 0 grad_norm=4.5006e-01, inner_loss=6.4485e+00\n",
      "[eval] sample 47 iter 10 grad_norm=4.2792e-01, inner_loss=6.4292e+00\n",
      "[eval] sample 47 iter 20 grad_norm=4.0728e-01, inner_loss=6.4117e+00\n",
      "[eval] sample 47 iter 30 grad_norm=3.8801e-01, inner_loss=6.3959e+00\n",
      "[eval] sample 47 iter 40 grad_norm=3.7004e-01, inner_loss=6.3815e+00\n",
      "[eval] sample 48 iter 0 grad_norm=8.9694e-01, inner_loss=7.2213e+00\n",
      "[eval] sample 48 iter 10 grad_norm=8.5277e-01, inner_loss=7.1446e+00\n",
      "[eval] sample 48 iter 20 grad_norm=8.1150e-01, inner_loss=7.0752e+00\n",
      "[eval] sample 48 iter 30 grad_norm=7.7289e-01, inner_loss=7.0123e+00\n",
      "[eval] sample 48 iter 40 grad_norm=7.3674e-01, inner_loss=6.9552e+00\n",
      "[eval] sample 49 iter 0 grad_norm=7.1459e-01, inner_loss=6.9188e+00\n",
      "[eval] sample 49 iter 10 grad_norm=6.8159e-01, inner_loss=6.8699e+00\n",
      "[eval] sample 49 iter 20 grad_norm=6.5056e-01, inner_loss=6.8255e+00\n",
      "[eval] sample 49 iter 30 grad_norm=6.2136e-01, inner_loss=6.7849e+00\n",
      "[eval] sample 49 iter 40 grad_norm=5.9387e-01, inner_loss=6.7480e+00\n",
      "[eval] sample 50 iter 0 grad_norm=6.0052e-01, inner_loss=6.8002e+00\n",
      "[eval] sample 50 iter 10 grad_norm=5.7574e-01, inner_loss=6.7656e+00\n",
      "[eval] sample 50 iter 20 grad_norm=5.5254e-01, inner_loss=6.7337e+00\n",
      "[eval] sample 50 iter 30 grad_norm=5.3079e-01, inner_loss=6.7043e+00\n",
      "[eval] sample 50 iter 40 grad_norm=5.1039e-01, inner_loss=6.6772e+00\n",
      "[eval] sample 51 iter 0 grad_norm=8.2960e-01, inner_loss=7.3980e+00\n",
      "[eval] sample 51 iter 10 grad_norm=7.9534e-01, inner_loss=7.3319e+00\n",
      "[eval] sample 51 iter 20 grad_norm=7.6318e-01, inner_loss=7.2711e+00\n",
      "[eval] sample 51 iter 30 grad_norm=7.3297e-01, inner_loss=7.2150e+00\n",
      "[eval] sample 51 iter 40 grad_norm=7.0455e-01, inner_loss=7.1632e+00\n",
      "[eval] sample 52 iter 0 grad_norm=6.3062e-01, inner_loss=6.8609e+00\n",
      "[eval] sample 52 iter 10 grad_norm=6.0462e-01, inner_loss=6.8226e+00\n",
      "[eval] sample 52 iter 20 grad_norm=5.8017e-01, inner_loss=6.7875e+00\n",
      "[eval] sample 52 iter 30 grad_norm=5.5716e-01, inner_loss=6.7551e+00\n",
      "[eval] sample 52 iter 40 grad_norm=5.3549e-01, inner_loss=6.7252e+00\n",
      "[eval] sample 53 iter 0 grad_norm=4.7996e-01, inner_loss=6.5340e+00\n",
      "[eval] sample 53 iter 10 grad_norm=4.6023e-01, inner_loss=6.5119e+00\n",
      "[eval] sample 53 iter 20 grad_norm=4.4164e-01, inner_loss=6.4915e+00\n",
      "[eval] sample 53 iter 30 grad_norm=4.2411e-01, inner_loss=6.4727e+00\n",
      "[eval] sample 53 iter 40 grad_norm=4.0755e-01, inner_loss=6.4554e+00\n",
      "[eval] sample 54 iter 0 grad_norm=7.9013e-01, inner_loss=7.2764e+00\n",
      "[eval] sample 54 iter 10 grad_norm=7.5854e-01, inner_loss=7.2163e+00\n",
      "[eval] sample 54 iter 20 grad_norm=7.2877e-01, inner_loss=7.1609e+00\n",
      "[eval] sample 54 iter 30 grad_norm=7.0070e-01, inner_loss=7.1097e+00\n",
      "[eval] sample 54 iter 40 grad_norm=6.7420e-01, inner_loss=7.0624e+00\n",
      "[eval] sample 55 iter 0 grad_norm=7.6775e-01, inner_loss=7.1310e+00\n",
      "[eval] sample 55 iter 10 grad_norm=7.3599e-01, inner_loss=7.0743e+00\n",
      "[eval] sample 55 iter 20 grad_norm=7.0606e-01, inner_loss=7.0223e+00\n",
      "[eval] sample 55 iter 30 grad_norm=6.7782e-01, inner_loss=6.9743e+00\n",
      "[eval] sample 55 iter 40 grad_norm=6.5115e-01, inner_loss=6.9301e+00\n",
      "[eval] sample 56 iter 0 grad_norm=8.1427e-01, inner_loss=7.1242e+00\n",
      "[eval] sample 56 iter 10 grad_norm=7.7637e-01, inner_loss=7.0608e+00\n",
      "[eval] sample 56 iter 20 grad_norm=7.4073e-01, inner_loss=7.0031e+00\n",
      "[eval] sample 56 iter 30 grad_norm=7.0718e-01, inner_loss=6.9506e+00\n",
      "[eval] sample 56 iter 40 grad_norm=6.7559e-01, inner_loss=6.9027e+00\n",
      "[eval] sample 57 iter 0 grad_norm=4.6723e-01, inner_loss=6.4633e+00\n",
      "[eval] sample 57 iter 10 grad_norm=4.4595e-01, inner_loss=6.4424e+00\n",
      "[eval] sample 57 iter 20 grad_norm=4.2599e-01, inner_loss=6.4233e+00\n",
      "[eval] sample 57 iter 30 grad_norm=4.0727e-01, inner_loss=6.4059e+00\n",
      "[eval] sample 57 iter 40 grad_norm=3.8970e-01, inner_loss=6.3900e+00\n",
      "[eval] sample 58 iter 0 grad_norm=6.7778e-01, inner_loss=6.9422e+00\n",
      "[eval] sample 58 iter 10 grad_norm=6.4786e-01, inner_loss=6.8981e+00\n",
      "[eval] sample 58 iter 20 grad_norm=6.1986e-01, inner_loss=6.8579e+00\n",
      "[eval] sample 58 iter 30 grad_norm=5.9362e-01, inner_loss=6.8210e+00\n",
      "[eval] sample 58 iter 40 grad_norm=5.6901e-01, inner_loss=6.7871e+00\n",
      "[eval] sample 59 iter 0 grad_norm=4.3800e-01, inner_loss=6.4387e+00\n",
      "[eval] sample 59 iter 10 grad_norm=4.1835e-01, inner_loss=6.4203e+00\n",
      "[eval] sample 59 iter 20 grad_norm=3.9995e-01, inner_loss=6.4035e+00\n",
      "[eval] sample 59 iter 30 grad_norm=3.8271e-01, inner_loss=6.3882e+00\n",
      "[eval] sample 59 iter 40 grad_norm=3.6655e-01, inner_loss=6.3741e+00\n",
      "[eval] sample 60 iter 0 grad_norm=8.3638e-01, inner_loss=7.1347e+00\n",
      "[eval] sample 60 iter 10 grad_norm=7.9564e-01, inner_loss=7.0680e+00\n",
      "[eval] sample 60 iter 20 grad_norm=7.5750e-01, inner_loss=7.0075e+00\n",
      "[eval] sample 60 iter 30 grad_norm=7.2177e-01, inner_loss=6.9527e+00\n",
      "[eval] sample 60 iter 40 grad_norm=6.8828e-01, inner_loss=6.9029e+00\n",
      "[eval] sample 61 iter 0 grad_norm=7.3920e-01, inner_loss=7.0260e+00\n",
      "[eval] sample 61 iter 10 grad_norm=7.0541e-01, inner_loss=6.9738e+00\n",
      "[eval] sample 61 iter 20 grad_norm=6.7380e-01, inner_loss=6.9261e+00\n",
      "[eval] sample 61 iter 30 grad_norm=6.4419e-01, inner_loss=6.8826e+00\n",
      "[eval] sample 61 iter 40 grad_norm=6.1644e-01, inner_loss=6.8428e+00\n",
      "[eval] sample 62 iter 0 grad_norm=4.3137e-01, inner_loss=6.4509e+00\n",
      "[eval] sample 62 iter 10 grad_norm=4.1280e-01, inner_loss=6.4331e+00\n",
      "[eval] sample 62 iter 20 grad_norm=3.9541e-01, inner_loss=6.4167e+00\n",
      "[eval] sample 62 iter 30 grad_norm=3.7911e-01, inner_loss=6.4017e+00\n",
      "[eval] sample 62 iter 40 grad_norm=3.6382e-01, inner_loss=6.3879e+00\n",
      "[eval] sample 63 iter 0 grad_norm=6.3929e-01, inner_loss=6.8607e+00\n",
      "[eval] sample 63 iter 10 grad_norm=6.1250e-01, inner_loss=6.8215e+00\n",
      "[eval] sample 63 iter 20 grad_norm=5.8741e-01, inner_loss=6.7854e+00\n",
      "[eval] sample 63 iter 30 grad_norm=5.6388e-01, inner_loss=6.7522e+00\n",
      "[eval] sample 63 iter 40 grad_norm=5.4180e-01, inner_loss=6.7216e+00\n",
      "[eval] sample 64 iter 0 grad_norm=7.6076e-01, inner_loss=7.1285e+00\n",
      "[eval] sample 64 iter 10 grad_norm=7.2692e-01, inner_loss=7.0731e+00\n",
      "[eval] sample 64 iter 20 grad_norm=6.9526e-01, inner_loss=7.0224e+00\n",
      "[eval] sample 64 iter 30 grad_norm=6.6561e-01, inner_loss=6.9760e+00\n",
      "[eval] sample 64 iter 40 grad_norm=6.3782e-01, inner_loss=6.9335e+00\n",
      "[eval] sample 65 iter 0 grad_norm=4.2418e-01, inner_loss=6.4216e+00\n",
      "[eval] sample 65 iter 10 grad_norm=4.0314e-01, inner_loss=6.4045e+00\n",
      "[eval] sample 65 iter 20 grad_norm=3.8361e-01, inner_loss=6.3890e+00\n",
      "[eval] sample 65 iter 30 grad_norm=3.6547e-01, inner_loss=6.3749e+00\n",
      "[eval] sample 65 iter 40 grad_norm=3.4862e-01, inner_loss=6.3621e+00\n",
      "[eval] sample 66 iter 0 grad_norm=5.8373e-01, inner_loss=6.7621e+00\n",
      "[eval] sample 66 iter 10 grad_norm=5.5855e-01, inner_loss=6.7294e+00\n",
      "[eval] sample 66 iter 20 grad_norm=5.3495e-01, inner_loss=6.6995e+00\n",
      "[eval] sample 66 iter 30 grad_norm=5.1279e-01, inner_loss=6.6720e+00\n",
      "[eval] sample 66 iter 40 grad_norm=4.9199e-01, inner_loss=6.6467e+00\n",
      "[eval] sample 67 iter 0 grad_norm=6.6739e-01, inner_loss=6.8340e+00\n",
      "[eval] sample 67 iter 10 grad_norm=6.3678e-01, inner_loss=6.7914e+00\n",
      "[eval] sample 67 iter 20 grad_norm=6.0804e-01, inner_loss=6.7526e+00\n",
      "[eval] sample 67 iter 30 grad_norm=5.8104e-01, inner_loss=6.7172e+00\n",
      "[eval] sample 67 iter 40 grad_norm=5.5566e-01, inner_loss=6.6848e+00\n",
      "[eval] sample 68 iter 0 grad_norm=8.0590e-01, inner_loss=7.1824e+00\n",
      "[eval] sample 68 iter 10 grad_norm=7.6828e-01, inner_loss=7.1204e+00\n",
      "[eval] sample 68 iter 20 grad_norm=7.3331e-01, inner_loss=7.0639e+00\n",
      "[eval] sample 68 iter 30 grad_norm=7.0078e-01, inner_loss=7.0124e+00\n",
      "[eval] sample 68 iter 40 grad_norm=6.7047e-01, inner_loss=6.9653e+00\n",
      "[eval] sample 69 iter 0 grad_norm=8.0345e-01, inner_loss=7.1665e+00\n",
      "[eval] sample 69 iter 10 grad_norm=7.6634e-01, inner_loss=7.1047e+00\n",
      "[eval] sample 69 iter 20 grad_norm=7.3143e-01, inner_loss=7.0485e+00\n",
      "[eval] sample 69 iter 30 grad_norm=6.9859e-01, inner_loss=6.9973e+00\n",
      "[eval] sample 69 iter 40 grad_norm=6.6769e-01, inner_loss=6.9506e+00\n",
      "[eval] sample 70 iter 0 grad_norm=7.1990e-01, inner_loss=6.9472e+00\n",
      "[eval] sample 70 iter 10 grad_norm=6.8637e-01, inner_loss=6.8977e+00\n",
      "[eval] sample 70 iter 20 grad_norm=6.5494e-01, inner_loss=6.8526e+00\n",
      "[eval] sample 70 iter 30 grad_norm=6.2546e-01, inner_loss=6.8116e+00\n",
      "[eval] sample 70 iter 40 grad_norm=5.9780e-01, inner_loss=6.7741e+00\n",
      "[eval] sample 71 iter 0 grad_norm=1.0343e+00, inner_loss=7.6597e+00\n",
      "[eval] sample 71 iter 10 grad_norm=9.8431e-01, inner_loss=7.5576e+00\n",
      "[eval] sample 71 iter 20 grad_norm=9.3732e-01, inner_loss=7.4651e+00\n",
      "[eval] sample 71 iter 30 grad_norm=8.9314e-01, inner_loss=7.3812e+00\n",
      "[eval] sample 71 iter 40 grad_norm=8.5159e-01, inner_loss=7.3049e+00\n",
      "[eval] sample 72 iter 0 grad_norm=7.2271e-01, inner_loss=6.9237e+00\n",
      "[eval] sample 72 iter 10 grad_norm=6.8718e-01, inner_loss=6.8739e+00\n",
      "[eval] sample 72 iter 20 grad_norm=6.5396e-01, inner_loss=6.8288e+00\n",
      "[eval] sample 72 iter 30 grad_norm=6.2289e-01, inner_loss=6.7880e+00\n",
      "[eval] sample 72 iter 40 grad_norm=5.9380e-01, inner_loss=6.7509e+00\n",
      "[eval] sample 73 iter 0 grad_norm=7.6716e-01, inner_loss=6.9453e+00\n",
      "[eval] sample 73 iter 10 grad_norm=7.2597e-01, inner_loss=6.8895e+00\n",
      "[eval] sample 73 iter 20 grad_norm=6.8769e-01, inner_loss=6.8394e+00\n",
      "[eval] sample 73 iter 30 grad_norm=6.5210e-01, inner_loss=6.7944e+00\n",
      "[eval] sample 73 iter 40 grad_norm=6.1901e-01, inner_loss=6.7539e+00\n",
      "[eval] sample 74 iter 0 grad_norm=6.2702e-01, inner_loss=6.6889e+00\n",
      "[eval] sample 74 iter 10 grad_norm=5.9690e-01, inner_loss=6.6514e+00\n",
      "[eval] sample 74 iter 20 grad_norm=5.6870e-01, inner_loss=6.6173e+00\n",
      "[eval] sample 74 iter 30 grad_norm=5.4228e-01, inner_loss=6.5864e+00\n",
      "[eval] sample 74 iter 40 grad_norm=5.1752e-01, inner_loss=6.5583e+00\n",
      "[eval] sample 75 iter 0 grad_norm=6.2284e-01, inner_loss=6.7932e+00\n",
      "[eval] sample 75 iter 10 grad_norm=5.9391e-01, inner_loss=6.7561e+00\n",
      "[eval] sample 75 iter 20 grad_norm=5.6697e-01, inner_loss=6.7224e+00\n",
      "[eval] sample 75 iter 30 grad_norm=5.4185e-01, inner_loss=6.6916e+00\n",
      "[eval] sample 75 iter 40 grad_norm=5.1840e-01, inner_loss=6.6634e+00\n",
      "[eval] sample 76 iter 0 grad_norm=5.4718e-01, inner_loss=6.6468e+00\n",
      "[eval] sample 76 iter 10 grad_norm=5.2381e-01, inner_loss=6.6180e+00\n",
      "[eval] sample 76 iter 20 grad_norm=5.0190e-01, inner_loss=6.5917e+00\n",
      "[eval] sample 76 iter 30 grad_norm=4.8132e-01, inner_loss=6.5675e+00\n",
      "[eval] sample 76 iter 40 grad_norm=4.6199e-01, inner_loss=6.5452e+00\n",
      "[eval] sample 77 iter 0 grad_norm=5.7720e-01, inner_loss=6.6680e+00\n",
      "[eval] sample 77 iter 10 grad_norm=5.5227e-01, inner_loss=6.6361e+00\n",
      "[eval] sample 77 iter 20 grad_norm=5.2873e-01, inner_loss=6.6068e+00\n",
      "[eval] sample 77 iter 30 grad_norm=5.0648e-01, inner_loss=6.5800e+00\n",
      "[eval] sample 77 iter 40 grad_norm=4.8544e-01, inner_loss=6.5553e+00\n",
      "[eval] sample 78 iter 0 grad_norm=5.9623e-01, inner_loss=6.6155e+00\n",
      "[eval] sample 78 iter 10 grad_norm=5.6395e-01, inner_loss=6.5818e+00\n",
      "[eval] sample 78 iter 20 grad_norm=5.3392e-01, inner_loss=6.5516e+00\n",
      "[eval] sample 78 iter 30 grad_norm=5.0599e-01, inner_loss=6.5245e+00\n",
      "[eval] sample 78 iter 40 grad_norm=4.7999e-01, inner_loss=6.5001e+00\n",
      "[eval] sample 79 iter 0 grad_norm=9.5157e-01, inner_loss=7.2779e+00\n",
      "[eval] sample 79 iter 10 grad_norm=9.0272e-01, inner_loss=7.1918e+00\n",
      "[eval] sample 79 iter 20 grad_norm=8.5702e-01, inner_loss=7.1142e+00\n",
      "[eval] sample 79 iter 30 grad_norm=8.1425e-01, inner_loss=7.0442e+00\n",
      "[eval] sample 79 iter 40 grad_norm=7.7420e-01, inner_loss=6.9810e+00\n",
      "[eval] sample 80 iter 0 grad_norm=6.7945e-01, inner_loss=6.8936e+00\n",
      "[eval] sample 80 iter 10 grad_norm=6.4891e-01, inner_loss=6.8494e+00\n",
      "[eval] sample 80 iter 20 grad_norm=6.2033e-01, inner_loss=6.8090e+00\n",
      "[eval] sample 80 iter 30 grad_norm=5.9359e-01, inner_loss=6.7721e+00\n",
      "[eval] sample 80 iter 40 grad_norm=5.6853e-01, inner_loss=6.7383e+00\n",
      "[eval] sample 81 iter 0 grad_norm=7.4219e-01, inner_loss=6.9532e+00\n",
      "[eval] sample 81 iter 10 grad_norm=7.0722e-01, inner_loss=6.9006e+00\n",
      "[eval] sample 81 iter 20 grad_norm=6.7444e-01, inner_loss=6.8528e+00\n",
      "[eval] sample 81 iter 30 grad_norm=6.4369e-01, inner_loss=6.8092e+00\n",
      "[eval] sample 81 iter 40 grad_norm=6.1482e-01, inner_loss=6.7696e+00\n",
      "[eval] sample 82 iter 0 grad_norm=8.9504e-01, inner_loss=7.2433e+00\n",
      "[eval] sample 82 iter 10 grad_norm=8.5085e-01, inner_loss=7.1669e+00\n",
      "[eval] sample 82 iter 20 grad_norm=8.0942e-01, inner_loss=7.0979e+00\n",
      "[eval] sample 82 iter 30 grad_norm=7.7057e-01, inner_loss=7.0353e+00\n",
      "[eval] sample 82 iter 40 grad_norm=7.3412e-01, inner_loss=6.9786e+00\n",
      "[eval] sample 83 iter 0 grad_norm=6.3329e-01, inner_loss=6.7532e+00\n",
      "[eval] sample 83 iter 10 grad_norm=6.0452e-01, inner_loss=6.7148e+00\n",
      "[eval] sample 83 iter 20 grad_norm=5.7751e-01, inner_loss=6.6798e+00\n",
      "[eval] sample 83 iter 30 grad_norm=5.5213e-01, inner_loss=6.6479e+00\n",
      "[eval] sample 83 iter 40 grad_norm=5.2826e-01, inner_loss=6.6186e+00\n",
      "[eval] sample 84 iter 0 grad_norm=6.0280e-01, inner_loss=6.8566e+00\n",
      "[eval] sample 84 iter 10 grad_norm=5.7660e-01, inner_loss=6.8218e+00\n",
      "[eval] sample 84 iter 20 grad_norm=5.5221e-01, inner_loss=6.7899e+00\n",
      "[eval] sample 84 iter 30 grad_norm=5.2950e-01, inner_loss=6.7606e+00\n",
      "[eval] sample 84 iter 40 grad_norm=5.0831e-01, inner_loss=6.7336e+00\n",
      "[eval] sample 85 iter 0 grad_norm=8.8527e-01, inner_loss=7.1977e+00\n",
      "[eval] sample 85 iter 10 grad_norm=8.4170e-01, inner_loss=7.1230e+00\n",
      "[eval] sample 85 iter 20 grad_norm=8.0090e-01, inner_loss=7.0554e+00\n",
      "[eval] sample 85 iter 30 grad_norm=7.6266e-01, inner_loss=6.9941e+00\n",
      "[eval] sample 85 iter 40 grad_norm=7.2680e-01, inner_loss=6.9385e+00\n",
      "[eval] sample 86 iter 0 grad_norm=8.0430e-01, inner_loss=7.0096e+00\n",
      "[eval] sample 86 iter 10 grad_norm=7.6438e-01, inner_loss=6.9480e+00\n",
      "[eval] sample 86 iter 20 grad_norm=7.2706e-01, inner_loss=6.8923e+00\n",
      "[eval] sample 86 iter 30 grad_norm=6.9213e-01, inner_loss=6.8418e+00\n",
      "[eval] sample 86 iter 40 grad_norm=6.5944e-01, inner_loss=6.7960e+00\n",
      "[eval] sample 87 iter 0 grad_norm=7.1734e-01, inner_loss=6.9402e+00\n",
      "[eval] sample 87 iter 10 grad_norm=6.8364e-01, inner_loss=6.8910e+00\n",
      "[eval] sample 87 iter 20 grad_norm=6.5211e-01, inner_loss=6.8463e+00\n",
      "[eval] sample 87 iter 30 grad_norm=6.2260e-01, inner_loss=6.8056e+00\n",
      "[eval] sample 87 iter 40 grad_norm=5.9495e-01, inner_loss=6.7685e+00\n",
      "[eval] sample 88 iter 0 grad_norm=7.1247e-01, inner_loss=6.9568e+00\n",
      "[eval] sample 88 iter 10 grad_norm=6.7887e-01, inner_loss=6.9083e+00\n",
      "[eval] sample 88 iter 20 grad_norm=6.4742e-01, inner_loss=6.8642e+00\n",
      "[eval] sample 88 iter 30 grad_norm=6.1797e-01, inner_loss=6.8241e+00\n",
      "[eval] sample 88 iter 40 grad_norm=5.9037e-01, inner_loss=6.7876e+00\n",
      "[eval] sample 89 iter 0 grad_norm=6.6536e-01, inner_loss=6.7325e+00\n",
      "[eval] sample 89 iter 10 grad_norm=6.3267e-01, inner_loss=6.6903e+00\n",
      "[eval] sample 89 iter 20 grad_norm=6.0208e-01, inner_loss=6.6521e+00\n",
      "[eval] sample 89 iter 30 grad_norm=5.7344e-01, inner_loss=6.6175e+00\n",
      "[eval] sample 89 iter 40 grad_norm=5.4660e-01, inner_loss=6.5861e+00\n",
      "[eval] sample 90 iter 0 grad_norm=6.5625e-01, inner_loss=6.8754e+00\n",
      "[eval] sample 90 iter 10 grad_norm=6.2560e-01, inner_loss=6.8342e+00\n",
      "[eval] sample 90 iter 20 grad_norm=5.9698e-01, inner_loss=6.7968e+00\n",
      "[eval] sample 90 iter 30 grad_norm=5.7024e-01, inner_loss=6.7627e+00\n",
      "[eval] sample 90 iter 40 grad_norm=5.4523e-01, inner_loss=6.7315e+00\n",
      "[eval] sample 91 iter 0 grad_norm=5.6943e-01, inner_loss=6.6930e+00\n",
      "[eval] sample 91 iter 10 grad_norm=5.4223e-01, inner_loss=6.6621e+00\n",
      "[eval] sample 91 iter 20 grad_norm=5.1696e-01, inner_loss=6.6340e+00\n",
      "[eval] sample 91 iter 30 grad_norm=4.9344e-01, inner_loss=6.6084e+00\n",
      "[eval] sample 91 iter 40 grad_norm=4.7156e-01, inner_loss=6.5851e+00\n",
      "[eval] sample 92 iter 0 grad_norm=4.0248e-01, inner_loss=6.3964e+00\n",
      "[eval] sample 92 iter 10 grad_norm=3.8344e-01, inner_loss=6.3809e+00\n",
      "[eval] sample 92 iter 20 grad_norm=3.6574e-01, inner_loss=6.3669e+00\n",
      "[eval] sample 92 iter 30 grad_norm=3.4925e-01, inner_loss=6.3541e+00\n",
      "[eval] sample 92 iter 40 grad_norm=3.3388e-01, inner_loss=6.3424e+00\n",
      "[eval] sample 93 iter 0 grad_norm=8.7371e-01, inner_loss=7.0880e+00\n",
      "[eval] sample 93 iter 10 grad_norm=8.2673e-01, inner_loss=7.0155e+00\n",
      "[eval] sample 93 iter 20 grad_norm=7.8296e-01, inner_loss=6.9506e+00\n",
      "[eval] sample 93 iter 30 grad_norm=7.4217e-01, inner_loss=6.8923e+00\n",
      "[eval] sample 93 iter 40 grad_norm=7.0413e-01, inner_loss=6.8399e+00\n",
      "[eval] sample 94 iter 0 grad_norm=7.7987e-01, inner_loss=6.8605e+00\n",
      "[eval] sample 94 iter 10 grad_norm=7.3874e-01, inner_loss=6.8027e+00\n",
      "[eval] sample 94 iter 20 grad_norm=7.0033e-01, inner_loss=6.7508e+00\n",
      "[eval] sample 94 iter 30 grad_norm=6.6442e-01, inner_loss=6.7042e+00\n",
      "[eval] sample 94 iter 40 grad_norm=6.3085e-01, inner_loss=6.6621e+00\n",
      "[eval] sample 95 iter 0 grad_norm=7.6337e-01, inner_loss=7.0095e+00\n",
      "[eval] sample 95 iter 10 grad_norm=7.2479e-01, inner_loss=6.9540e+00\n",
      "[eval] sample 95 iter 20 grad_norm=6.8897e-01, inner_loss=6.9039e+00\n",
      "[eval] sample 95 iter 30 grad_norm=6.5569e-01, inner_loss=6.8586e+00\n",
      "[eval] sample 95 iter 40 grad_norm=6.2473e-01, inner_loss=6.8176e+00\n",
      "[eval] sample 96 iter 0 grad_norm=5.1721e-01, inner_loss=6.5234e+00\n",
      "[eval] sample 96 iter 10 grad_norm=4.9305e-01, inner_loss=6.4978e+00\n",
      "[eval] sample 96 iter 20 grad_norm=4.7041e-01, inner_loss=6.4746e+00\n",
      "[eval] sample 96 iter 30 grad_norm=4.4920e-01, inner_loss=6.4534e+00\n",
      "[eval] sample 96 iter 40 grad_norm=4.2929e-01, inner_loss=6.4341e+00\n",
      "[eval] sample 97 iter 0 grad_norm=7.5669e-01, inner_loss=7.0145e+00\n",
      "[eval] sample 97 iter 10 grad_norm=7.2007e-01, inner_loss=6.9598e+00\n",
      "[eval] sample 97 iter 20 grad_norm=6.8595e-01, inner_loss=6.9103e+00\n",
      "[eval] sample 97 iter 30 grad_norm=6.5413e-01, inner_loss=6.8653e+00\n",
      "[eval] sample 97 iter 40 grad_norm=6.2442e-01, inner_loss=6.8244e+00\n",
      "[eval] sample 98 iter 0 grad_norm=6.5534e-01, inner_loss=6.7706e+00\n",
      "[eval] sample 98 iter 10 grad_norm=6.2317e-01, inner_loss=6.7296e+00\n",
      "[eval] sample 98 iter 20 grad_norm=5.9306e-01, inner_loss=6.6926e+00\n",
      "[eval] sample 98 iter 30 grad_norm=5.6488e-01, inner_loss=6.6590e+00\n",
      "[eval] sample 98 iter 40 grad_norm=5.3848e-01, inner_loss=6.6285e+00\n",
      "[eval] sample 99 iter 0 grad_norm=7.9242e-01, inner_loss=7.1685e+00\n",
      "[eval] sample 99 iter 10 grad_norm=7.5866e-01, inner_loss=7.1083e+00\n",
      "[eval] sample 99 iter 20 grad_norm=7.2695e-01, inner_loss=7.0530e+00\n",
      "[eval] sample 99 iter 30 grad_norm=6.9713e-01, inner_loss=7.0022e+00\n",
      "[eval] sample 99 iter 40 grad_norm=6.6906e-01, inner_loss=6.9554e+00\n",
      "[eval] sample 100 iter 0 grad_norm=4.4679e-01, inner_loss=6.5031e+00\n",
      "[eval] sample 100 iter 10 grad_norm=4.2857e-01, inner_loss=6.4839e+00\n",
      "[eval] sample 100 iter 20 grad_norm=4.1150e-01, inner_loss=6.4663e+00\n",
      "[eval] sample 100 iter 30 grad_norm=3.9547e-01, inner_loss=6.4500e+00\n",
      "[eval] sample 100 iter 40 grad_norm=3.8041e-01, inner_loss=6.4349e+00\n",
      "[eval] sample 101 iter 0 grad_norm=7.5542e-01, inner_loss=7.0353e+00\n",
      "[eval] sample 101 iter 10 grad_norm=7.2049e-01, inner_loss=6.9807e+00\n",
      "[eval] sample 101 iter 20 grad_norm=6.8765e-01, inner_loss=6.9310e+00\n",
      "[eval] sample 101 iter 30 grad_norm=6.5678e-01, inner_loss=6.8857e+00\n",
      "[eval] sample 101 iter 40 grad_norm=6.2773e-01, inner_loss=6.8444e+00\n",
      "[eval] sample 102 iter 0 grad_norm=7.4377e-01, inner_loss=7.0036e+00\n",
      "[eval] sample 102 iter 10 grad_norm=7.0854e-01, inner_loss=6.9508e+00\n",
      "[eval] sample 102 iter 20 grad_norm=6.7549e-01, inner_loss=6.9028e+00\n",
      "[eval] sample 102 iter 30 grad_norm=6.4450e-01, inner_loss=6.8591e+00\n",
      "[eval] sample 102 iter 40 grad_norm=6.1540e-01, inner_loss=6.8194e+00\n",
      "[eval] sample 103 iter 0 grad_norm=7.4399e-01, inner_loss=6.9777e+00\n",
      "[eval] sample 103 iter 10 grad_norm=7.0894e-01, inner_loss=6.9248e+00\n",
      "[eval] sample 103 iter 20 grad_norm=6.7602e-01, inner_loss=6.8768e+00\n",
      "[eval] sample 103 iter 30 grad_norm=6.4508e-01, inner_loss=6.8330e+00\n",
      "[eval] sample 103 iter 40 grad_norm=6.1600e-01, inner_loss=6.7932e+00\n",
      "[eval] sample 104 iter 0 grad_norm=6.5170e-01, inner_loss=6.8790e+00\n",
      "[eval] sample 104 iter 10 grad_norm=6.2450e-01, inner_loss=6.8383e+00\n",
      "[eval] sample 104 iter 20 grad_norm=5.9893e-01, inner_loss=6.8008e+00\n",
      "[eval] sample 104 iter 30 grad_norm=5.7487e-01, inner_loss=6.7663e+00\n",
      "[eval] sample 104 iter 40 grad_norm=5.5221e-01, inner_loss=6.7345e+00\n",
      "[eval] sample 105 iter 0 grad_norm=5.8782e-01, inner_loss=6.7594e+00\n",
      "[eval] sample 105 iter 10 grad_norm=5.6413e-01, inner_loss=6.7261e+00\n",
      "[eval] sample 105 iter 20 grad_norm=5.4184e-01, inner_loss=6.6955e+00\n",
      "[eval] sample 105 iter 30 grad_norm=5.2086e-01, inner_loss=6.6672e+00\n",
      "[eval] sample 105 iter 40 grad_norm=5.0108e-01, inner_loss=6.6411e+00\n",
      "[eval] sample 106 iter 0 grad_norm=7.4498e-01, inner_loss=7.0030e+00\n",
      "[eval] sample 106 iter 10 grad_norm=7.0983e-01, inner_loss=6.9500e+00\n",
      "[eval] sample 106 iter 20 grad_norm=6.7681e-01, inner_loss=6.9018e+00\n",
      "[eval] sample 106 iter 30 grad_norm=6.4576e-01, inner_loss=6.8580e+00\n",
      "[eval] sample 106 iter 40 grad_norm=6.1656e-01, inner_loss=6.8181e+00\n",
      "[eval] sample 107 iter 0 grad_norm=5.8331e-01, inner_loss=6.6267e+00\n",
      "[eval] sample 107 iter 10 grad_norm=5.5579e-01, inner_loss=6.5942e+00\n",
      "[eval] sample 107 iter 20 grad_norm=5.3001e-01, inner_loss=6.5647e+00\n",
      "[eval] sample 107 iter 30 grad_norm=5.0583e-01, inner_loss=6.5378e+00\n",
      "[eval] sample 107 iter 40 grad_norm=4.8315e-01, inner_loss=6.5133e+00\n",
      "[eval] sample 108 iter 0 grad_norm=6.4175e-01, inner_loss=6.7970e+00\n",
      "[eval] sample 108 iter 10 grad_norm=6.1073e-01, inner_loss=6.7577e+00\n",
      "[eval] sample 108 iter 20 grad_norm=5.8184e-01, inner_loss=6.7220e+00\n",
      "[eval] sample 108 iter 30 grad_norm=5.5492e-01, inner_loss=6.6897e+00\n",
      "[eval] sample 108 iter 40 grad_norm=5.2982e-01, inner_loss=6.6602e+00\n",
      "[eval] sample 109 iter 0 grad_norm=5.4137e-01, inner_loss=6.6895e+00\n",
      "[eval] sample 109 iter 10 grad_norm=5.1946e-01, inner_loss=6.6613e+00\n",
      "[eval] sample 109 iter 20 grad_norm=4.9892e-01, inner_loss=6.6353e+00\n",
      "[eval] sample 109 iter 30 grad_norm=4.7964e-01, inner_loss=6.6113e+00\n",
      "[eval] sample 109 iter 40 grad_norm=4.6152e-01, inner_loss=6.5891e+00\n",
      "[eval] sample 110 iter 0 grad_norm=6.2141e-01, inner_loss=6.8232e+00\n",
      "[eval] sample 110 iter 10 grad_norm=5.9127e-01, inner_loss=6.7864e+00\n",
      "[eval] sample 110 iter 20 grad_norm=5.6332e-01, inner_loss=6.7530e+00\n",
      "[eval] sample 110 iter 30 grad_norm=5.3739e-01, inner_loss=6.7227e+00\n",
      "[eval] sample 110 iter 40 grad_norm=5.1331e-01, inner_loss=6.6950e+00\n",
      "[eval] sample 111 iter 0 grad_norm=6.6417e-01, inner_loss=6.7697e+00\n",
      "[eval] sample 111 iter 10 grad_norm=6.3174e-01, inner_loss=6.7276e+00\n",
      "[eval] sample 111 iter 20 grad_norm=6.0142e-01, inner_loss=6.6895e+00\n",
      "[eval] sample 111 iter 30 grad_norm=5.7306e-01, inner_loss=6.6550e+00\n",
      "[eval] sample 111 iter 40 grad_norm=5.4652e-01, inner_loss=6.6236e+00\n",
      "[eval] sample 112 iter 0 grad_norm=5.9464e-01, inner_loss=6.7037e+00\n",
      "[eval] sample 112 iter 10 grad_norm=5.6744e-01, inner_loss=6.6698e+00\n",
      "[eval] sample 112 iter 20 grad_norm=5.4200e-01, inner_loss=6.6390e+00\n",
      "[eval] sample 112 iter 30 grad_norm=5.1820e-01, inner_loss=6.6108e+00\n",
      "[eval] sample 112 iter 40 grad_norm=4.9591e-01, inner_loss=6.5851e+00\n",
      "[eval] sample 113 iter 0 grad_norm=6.3934e-01, inner_loss=6.8654e+00\n",
      "[eval] sample 113 iter 10 grad_norm=6.1176e-01, inner_loss=6.8262e+00\n",
      "[eval] sample 113 iter 20 grad_norm=5.8602e-01, inner_loss=6.7903e+00\n",
      "[eval] sample 113 iter 30 grad_norm=5.6198e-01, inner_loss=6.7573e+00\n",
      "[eval] sample 113 iter 40 grad_norm=5.3949e-01, inner_loss=6.7269e+00\n",
      "[eval] sample 114 iter 0 grad_norm=6.2623e-01, inner_loss=6.6985e+00\n",
      "[eval] sample 114 iter 10 grad_norm=5.9680e-01, inner_loss=6.6610e+00\n",
      "[eval] sample 114 iter 20 grad_norm=5.6923e-01, inner_loss=6.6270e+00\n",
      "[eval] sample 114 iter 30 grad_norm=5.4339e-01, inner_loss=6.5960e+00\n",
      "[eval] sample 114 iter 40 grad_norm=5.1914e-01, inner_loss=6.5677e+00\n",
      "[eval] sample 115 iter 0 grad_norm=5.0780e-01, inner_loss=6.6338e+00\n",
      "[eval] sample 115 iter 10 grad_norm=4.8583e-01, inner_loss=6.6091e+00\n",
      "[eval] sample 115 iter 20 grad_norm=4.6539e-01, inner_loss=6.5864e+00\n",
      "[eval] sample 115 iter 30 grad_norm=4.4637e-01, inner_loss=6.5656e+00\n",
      "[eval] sample 115 iter 40 grad_norm=4.2863e-01, inner_loss=6.5464e+00\n",
      "[eval] sample 116 iter 0 grad_norm=4.9229e-01, inner_loss=6.5523e+00\n",
      "[eval] sample 116 iter 10 grad_norm=4.7053e-01, inner_loss=6.5291e+00\n",
      "[eval] sample 116 iter 20 grad_norm=4.5027e-01, inner_loss=6.5078e+00\n",
      "[eval] sample 116 iter 30 grad_norm=4.3137e-01, inner_loss=6.4884e+00\n",
      "[eval] sample 116 iter 40 grad_norm=4.1373e-01, inner_loss=6.4705e+00\n",
      "[eval] sample 117 iter 0 grad_norm=7.4684e-01, inner_loss=6.9627e+00\n",
      "[eval] sample 117 iter 10 grad_norm=7.1110e-01, inner_loss=6.9094e+00\n",
      "[eval] sample 117 iter 20 grad_norm=6.7762e-01, inner_loss=6.8611e+00\n",
      "[eval] sample 117 iter 30 grad_norm=6.4625e-01, inner_loss=6.8172e+00\n",
      "[eval] sample 117 iter 40 grad_norm=6.1684e-01, inner_loss=6.7772e+00\n",
      "[eval] sample 118 iter 0 grad_norm=4.9547e-01, inner_loss=6.5936e+00\n",
      "[eval] sample 118 iter 10 grad_norm=4.7528e-01, inner_loss=6.5700e+00\n",
      "[eval] sample 118 iter 20 grad_norm=4.5639e-01, inner_loss=6.5482e+00\n",
      "[eval] sample 118 iter 30 grad_norm=4.3868e-01, inner_loss=6.5282e+00\n",
      "[eval] sample 118 iter 40 grad_norm=4.2206e-01, inner_loss=6.5096e+00\n",
      "[eval] sample 119 iter 0 grad_norm=8.1975e-01, inner_loss=7.1264e+00\n",
      "[eval] sample 119 iter 10 grad_norm=7.8246e-01, inner_loss=7.0621e+00\n",
      "[eval] sample 119 iter 20 grad_norm=7.4735e-01, inner_loss=7.0035e+00\n",
      "[eval] sample 119 iter 30 grad_norm=7.1426e-01, inner_loss=6.9500e+00\n",
      "[eval] sample 119 iter 40 grad_norm=6.8306e-01, inner_loss=6.9011e+00\n",
      "[eval] sample 120 iter 0 grad_norm=8.8455e-01, inner_loss=7.4442e+00\n",
      "[eval] sample 120 iter 10 grad_norm=8.4733e-01, inner_loss=7.3691e+00\n",
      "[eval] sample 120 iter 20 grad_norm=8.1239e-01, inner_loss=7.3001e+00\n",
      "[eval] sample 120 iter 30 grad_norm=7.7955e-01, inner_loss=7.2367e+00\n",
      "[eval] sample 120 iter 40 grad_norm=7.4865e-01, inner_loss=7.1782e+00\n",
      "[eval] sample 121 iter 0 grad_norm=6.2477e-01, inner_loss=6.7709e+00\n",
      "[eval] sample 121 iter 10 grad_norm=5.9825e-01, inner_loss=6.7334e+00\n",
      "[eval] sample 121 iter 20 grad_norm=5.7328e-01, inner_loss=6.6990e+00\n",
      "[eval] sample 121 iter 30 grad_norm=5.4974e-01, inner_loss=6.6674e+00\n",
      "[eval] sample 121 iter 40 grad_norm=5.2754e-01, inner_loss=6.6384e+00\n",
      "[eval] sample 122 iter 0 grad_norm=5.3246e-01, inner_loss=6.5737e+00\n",
      "[eval] sample 122 iter 10 grad_norm=5.0811e-01, inner_loss=6.5466e+00\n",
      "[eval] sample 122 iter 20 grad_norm=4.8529e-01, inner_loss=6.5218e+00\n",
      "[eval] sample 122 iter 30 grad_norm=4.6388e-01, inner_loss=6.4993e+00\n",
      "[eval] sample 122 iter 40 grad_norm=4.4378e-01, inner_loss=6.4786e+00\n",
      "[eval] sample 123 iter 0 grad_norm=5.5684e-01, inner_loss=6.7403e+00\n",
      "[eval] sample 123 iter 10 grad_norm=5.3228e-01, inner_loss=6.7106e+00\n",
      "[eval] sample 123 iter 20 grad_norm=5.0939e-01, inner_loss=6.6834e+00\n",
      "[eval] sample 123 iter 30 grad_norm=4.8804e-01, inner_loss=6.6585e+00\n",
      "[eval] sample 123 iter 40 grad_norm=4.6811e-01, inner_loss=6.6356e+00\n",
      "[eval] sample 124 iter 0 grad_norm=7.5220e-01, inner_loss=6.9675e+00\n",
      "[eval] sample 124 iter 10 grad_norm=7.1601e-01, inner_loss=6.9135e+00\n",
      "[eval] sample 124 iter 20 grad_norm=6.8212e-01, inner_loss=6.8645e+00\n",
      "[eval] sample 124 iter 30 grad_norm=6.5038e-01, inner_loss=6.8201e+00\n",
      "[eval] sample 124 iter 40 grad_norm=6.2063e-01, inner_loss=6.7796e+00\n",
      "[eval] sample 125 iter 0 grad_norm=5.2899e-01, inner_loss=6.6425e+00\n",
      "[eval] sample 125 iter 10 grad_norm=5.0600e-01, inner_loss=6.6156e+00\n",
      "[eval] sample 125 iter 20 grad_norm=4.8457e-01, inner_loss=6.5911e+00\n",
      "[eval] sample 125 iter 30 grad_norm=4.6456e-01, inner_loss=6.5685e+00\n",
      "[eval] sample 125 iter 40 grad_norm=4.4585e-01, inner_loss=6.5477e+00\n",
      "[eval] sample 126 iter 0 grad_norm=7.9252e-01, inner_loss=7.1375e+00\n",
      "[eval] sample 126 iter 10 grad_norm=7.5631e-01, inner_loss=7.0774e+00\n",
      "[eval] sample 126 iter 20 grad_norm=7.2227e-01, inner_loss=7.0227e+00\n",
      "[eval] sample 126 iter 30 grad_norm=6.9026e-01, inner_loss=6.9727e+00\n",
      "[eval] sample 126 iter 40 grad_norm=6.6015e-01, inner_loss=6.9270e+00\n",
      "[eval] sample 127 iter 0 grad_norm=7.5376e-01, inner_loss=7.0728e+00\n",
      "[eval] sample 127 iter 10 grad_norm=7.1975e-01, inner_loss=7.0184e+00\n",
      "[eval] sample 127 iter 20 grad_norm=6.8790e-01, inner_loss=6.9687e+00\n",
      "[eval] sample 127 iter 30 grad_norm=6.5804e-01, inner_loss=6.9234e+00\n",
      "[eval] sample 127 iter 40 grad_norm=6.3004e-01, inner_loss=6.8818e+00\n",
      "[eval] sample 128 iter 0 grad_norm=7.6161e-01, inner_loss=7.1632e+00\n",
      "[eval] sample 128 iter 10 grad_norm=7.2605e-01, inner_loss=7.1077e+00\n",
      "[eval] sample 128 iter 20 grad_norm=6.9305e-01, inner_loss=7.0573e+00\n",
      "[eval] sample 128 iter 30 grad_norm=6.6240e-01, inner_loss=7.0113e+00\n",
      "[eval] sample 128 iter 40 grad_norm=6.3389e-01, inner_loss=6.9692e+00\n",
      "[eval] sample 129 iter 0 grad_norm=6.7607e-01, inner_loss=6.8984e+00\n",
      "[eval] sample 129 iter 10 grad_norm=6.4708e-01, inner_loss=6.8545e+00\n",
      "[eval] sample 129 iter 20 grad_norm=6.1980e-01, inner_loss=6.8143e+00\n",
      "[eval] sample 129 iter 30 grad_norm=5.9413e-01, inner_loss=6.7774e+00\n",
      "[eval] sample 129 iter 40 grad_norm=5.6994e-01, inner_loss=6.7435e+00\n",
      "[eval] sample 130 iter 0 grad_norm=7.9808e-01, inner_loss=7.0548e+00\n",
      "[eval] sample 130 iter 10 grad_norm=7.5899e-01, inner_loss=6.9940e+00\n",
      "[eval] sample 130 iter 20 grad_norm=7.2249e-01, inner_loss=6.9391e+00\n",
      "[eval] sample 130 iter 30 grad_norm=6.8839e-01, inner_loss=6.8892e+00\n",
      "[eval] sample 130 iter 40 grad_norm=6.5649e-01, inner_loss=6.8439e+00\n",
      "[eval] sample 131 iter 0 grad_norm=8.4932e-01, inner_loss=7.1442e+00\n",
      "[eval] sample 131 iter 10 grad_norm=8.0698e-01, inner_loss=7.0755e+00\n",
      "[eval] sample 131 iter 20 grad_norm=7.6741e-01, inner_loss=7.0134e+00\n",
      "[eval] sample 131 iter 30 grad_norm=7.3041e-01, inner_loss=6.9571e+00\n",
      "[eval] sample 131 iter 40 grad_norm=6.9577e-01, inner_loss=6.9062e+00\n",
      "[eval] sample 132 iter 0 grad_norm=9.0118e-01, inner_loss=7.4027e+00\n",
      "[eval] sample 132 iter 10 grad_norm=8.6051e-01, inner_loss=7.3250e+00\n",
      "[eval] sample 132 iter 20 grad_norm=8.2237e-01, inner_loss=7.2540e+00\n",
      "[eval] sample 132 iter 30 grad_norm=7.8656e-01, inner_loss=7.1892e+00\n",
      "[eval] sample 132 iter 40 grad_norm=7.5292e-01, inner_loss=7.1298e+00\n",
      "[eval] sample 133 iter 0 grad_norm=8.4939e-01, inner_loss=7.1664e+00\n",
      "[eval] sample 133 iter 10 grad_norm=8.0864e-01, inner_loss=7.0976e+00\n",
      "[eval] sample 133 iter 20 grad_norm=7.7042e-01, inner_loss=7.0351e+00\n",
      "[eval] sample 133 iter 30 grad_norm=7.3457e-01, inner_loss=6.9783e+00\n",
      "[eval] sample 133 iter 40 grad_norm=7.0091e-01, inner_loss=6.9267e+00\n",
      "[eval] sample 134 iter 0 grad_norm=6.3303e-01, inner_loss=6.8995e+00\n",
      "[eval] sample 134 iter 10 grad_norm=6.0424e-01, inner_loss=6.8611e+00\n",
      "[eval] sample 134 iter 20 grad_norm=5.7754e-01, inner_loss=6.8261e+00\n",
      "[eval] sample 134 iter 30 grad_norm=5.5276e-01, inner_loss=6.7941e+00\n",
      "[eval] sample 134 iter 40 grad_norm=5.2973e-01, inner_loss=6.7648e+00\n",
      "[eval] sample 135 iter 0 grad_norm=7.0557e-01, inner_loss=6.8099e+00\n",
      "[eval] sample 135 iter 10 grad_norm=6.7104e-01, inner_loss=6.7624e+00\n",
      "[eval] sample 135 iter 20 grad_norm=6.3872e-01, inner_loss=6.7194e+00\n",
      "[eval] sample 135 iter 30 grad_norm=6.0845e-01, inner_loss=6.6805e+00\n",
      "[eval] sample 135 iter 40 grad_norm=5.8007e-01, inner_loss=6.6451e+00\n",
      "[eval] sample 136 iter 0 grad_norm=7.3358e-01, inner_loss=7.0283e+00\n",
      "[eval] sample 136 iter 10 grad_norm=7.0241e-01, inner_loss=6.9766e+00\n",
      "[eval] sample 136 iter 20 grad_norm=6.7300e-01, inner_loss=6.9292e+00\n",
      "[eval] sample 136 iter 30 grad_norm=6.4524e-01, inner_loss=6.8857e+00\n",
      "[eval] sample 136 iter 40 grad_norm=6.1902e-01, inner_loss=6.8457e+00\n",
      "[eval] sample 137 iter 0 grad_norm=7.4769e-01, inner_loss=6.8419e+00\n",
      "[eval] sample 137 iter 10 grad_norm=7.0974e-01, inner_loss=6.7887e+00\n",
      "[eval] sample 137 iter 20 grad_norm=6.7424e-01, inner_loss=6.7407e+00\n",
      "[eval] sample 137 iter 30 grad_norm=6.4101e-01, inner_loss=6.6974e+00\n",
      "[eval] sample 137 iter 40 grad_norm=6.0990e-01, inner_loss=6.6582e+00\n",
      "[eval] sample 138 iter 0 grad_norm=6.8065e-01, inner_loss=6.9399e+00\n",
      "[eval] sample 138 iter 10 grad_norm=6.4915e-01, inner_loss=6.8956e+00\n",
      "[eval] sample 138 iter 20 grad_norm=6.1967e-01, inner_loss=6.8553e+00\n",
      "[eval] sample 138 iter 30 grad_norm=5.9207e-01, inner_loss=6.8185e+00\n",
      "[eval] sample 138 iter 40 grad_norm=5.6621e-01, inner_loss=6.7849e+00\n",
      "[eval] sample 139 iter 0 grad_norm=7.0992e-01, inner_loss=6.9524e+00\n",
      "[eval] sample 139 iter 10 grad_norm=6.7866e-01, inner_loss=6.9041e+00\n",
      "[eval] sample 139 iter 20 grad_norm=6.4935e-01, inner_loss=6.8599e+00\n",
      "[eval] sample 139 iter 30 grad_norm=6.2186e-01, inner_loss=6.8194e+00\n",
      "[eval] sample 139 iter 40 grad_norm=5.9605e-01, inner_loss=6.7823e+00\n",
      "[eval] sample 140 iter 0 grad_norm=4.3850e-01, inner_loss=6.4431e+00\n",
      "[eval] sample 140 iter 10 grad_norm=4.1778e-01, inner_loss=6.4248e+00\n",
      "[eval] sample 140 iter 20 grad_norm=3.9847e-01, inner_loss=6.4081e+00\n",
      "[eval] sample 140 iter 30 grad_norm=3.8045e-01, inner_loss=6.3929e+00\n",
      "[eval] sample 140 iter 40 grad_norm=3.6363e-01, inner_loss=6.3790e+00\n",
      "[eval] sample 141 iter 0 grad_norm=6.4979e-01, inner_loss=6.8270e+00\n",
      "[eval] sample 141 iter 10 grad_norm=6.2003e-01, inner_loss=6.7866e+00\n",
      "[eval] sample 141 iter 20 grad_norm=5.9214e-01, inner_loss=6.7498e+00\n",
      "[eval] sample 141 iter 30 grad_norm=5.6600e-01, inner_loss=6.7162e+00\n",
      "[eval] sample 141 iter 40 grad_norm=5.4148e-01, inner_loss=6.6855e+00\n",
      "[eval] sample 142 iter 0 grad_norm=6.3094e-01, inner_loss=6.8478e+00\n",
      "[eval] sample 142 iter 10 grad_norm=6.0429e-01, inner_loss=6.8095e+00\n",
      "[eval] sample 142 iter 20 grad_norm=5.7926e-01, inner_loss=6.7745e+00\n",
      "[eval] sample 142 iter 30 grad_norm=5.5573e-01, inner_loss=6.7422e+00\n",
      "[eval] sample 142 iter 40 grad_norm=5.3359e-01, inner_loss=6.7125e+00\n",
      "[eval] sample 143 iter 0 grad_norm=5.1449e-01, inner_loss=6.5291e+00\n",
      "[eval] sample 143 iter 10 grad_norm=4.9078e-01, inner_loss=6.5038e+00\n",
      "[eval] sample 143 iter 20 grad_norm=4.6855e-01, inner_loss=6.4808e+00\n",
      "[eval] sample 143 iter 30 grad_norm=4.4771e-01, inner_loss=6.4597e+00\n",
      "[eval] sample 143 iter 40 grad_norm=4.2815e-01, inner_loss=6.4405e+00\n",
      "[eval] sample 144 iter 0 grad_norm=6.6946e-01, inner_loss=6.9046e+00\n",
      "[eval] sample 144 iter 10 grad_norm=6.4040e-01, inner_loss=6.8616e+00\n",
      "[eval] sample 144 iter 20 grad_norm=6.1324e-01, inner_loss=6.8222e+00\n",
      "[eval] sample 144 iter 30 grad_norm=5.8782e-01, inner_loss=6.7861e+00\n",
      "[eval] sample 144 iter 40 grad_norm=5.6399e-01, inner_loss=6.7529e+00\n",
      "[eval] sample 145 iter 0 grad_norm=4.9593e-01, inner_loss=6.5032e+00\n",
      "[eval] sample 145 iter 10 grad_norm=4.7319e-01, inner_loss=6.4797e+00\n",
      "[eval] sample 145 iter 20 grad_norm=4.5187e-01, inner_loss=6.4583e+00\n",
      "[eval] sample 145 iter 30 grad_norm=4.3187e-01, inner_loss=6.4387e+00\n",
      "[eval] sample 145 iter 40 grad_norm=4.1311e-01, inner_loss=6.4208e+00\n",
      "[eval] sample 146 iter 0 grad_norm=6.6192e-01, inner_loss=6.9427e+00\n",
      "[eval] sample 146 iter 10 grad_norm=6.3046e-01, inner_loss=6.9009e+00\n",
      "[eval] sample 146 iter 20 grad_norm=6.0128e-01, inner_loss=6.8629e+00\n",
      "[eval] sample 146 iter 30 grad_norm=5.7417e-01, inner_loss=6.8283e+00\n",
      "[eval] sample 146 iter 40 grad_norm=5.4897e-01, inner_loss=6.7967e+00\n",
      "[eval] sample 147 iter 0 grad_norm=8.6074e-01, inner_loss=7.2509e+00\n",
      "[eval] sample 147 iter 10 grad_norm=8.1880e-01, inner_loss=7.1802e+00\n",
      "[eval] sample 147 iter 20 grad_norm=7.7956e-01, inner_loss=7.1162e+00\n",
      "[eval] sample 147 iter 30 grad_norm=7.4280e-01, inner_loss=7.0582e+00\n",
      "[eval] sample 147 iter 40 grad_norm=7.0837e-01, inner_loss=7.0054e+00\n",
      "[eval] sample 148 iter 0 grad_norm=7.1488e-01, inner_loss=6.9614e+00\n",
      "[eval] sample 148 iter 10 grad_norm=6.8389e-01, inner_loss=6.9123e+00\n",
      "[eval] sample 148 iter 20 grad_norm=6.5464e-01, inner_loss=6.8675e+00\n",
      "[eval] sample 148 iter 30 grad_norm=6.2702e-01, inner_loss=6.8263e+00\n",
      "[eval] sample 148 iter 40 grad_norm=6.0092e-01, inner_loss=6.7886e+00\n",
      "[eval] sample 149 iter 0 grad_norm=4.3355e-01, inner_loss=6.4533e+00\n",
      "[eval] sample 149 iter 10 grad_norm=4.1255e-01, inner_loss=6.4354e+00\n",
      "[eval] sample 149 iter 20 grad_norm=3.9302e-01, inner_loss=6.4191e+00\n",
      "[eval] sample 149 iter 30 grad_norm=3.7484e-01, inner_loss=6.4044e+00\n",
      "[eval] sample 149 iter 40 grad_norm=3.5791e-01, inner_loss=6.3909e+00\n",
      "[eval] sample 150 iter 0 grad_norm=6.7648e-01, inner_loss=6.9242e+00\n",
      "[eval] sample 150 iter 10 grad_norm=6.4876e-01, inner_loss=6.8802e+00\n",
      "[eval] sample 150 iter 20 grad_norm=6.2267e-01, inner_loss=6.8397e+00\n",
      "[eval] sample 150 iter 30 grad_norm=5.9808e-01, inner_loss=6.8024e+00\n",
      "[eval] sample 150 iter 40 grad_norm=5.7490e-01, inner_loss=6.7679e+00\n",
      "[eval] sample 151 iter 0 grad_norm=7.6800e-01, inner_loss=7.1378e+00\n",
      "[eval] sample 151 iter 10 grad_norm=7.3685e-01, inner_loss=7.0811e+00\n",
      "[eval] sample 151 iter 20 grad_norm=7.0744e-01, inner_loss=7.0289e+00\n",
      "[eval] sample 151 iter 30 grad_norm=6.7964e-01, inner_loss=6.9807e+00\n",
      "[eval] sample 151 iter 40 grad_norm=6.5333e-01, inner_loss=6.9362e+00\n",
      "[eval] sample 152 iter 0 grad_norm=5.7170e-01, inner_loss=6.7328e+00\n",
      "[eval] sample 152 iter 10 grad_norm=5.4734e-01, inner_loss=6.7014e+00\n",
      "[eval] sample 152 iter 20 grad_norm=5.2457e-01, inner_loss=6.6727e+00\n",
      "[eval] sample 152 iter 30 grad_norm=5.0327e-01, inner_loss=6.6462e+00\n",
      "[eval] sample 152 iter 40 grad_norm=4.8331e-01, inner_loss=6.6218e+00\n",
      "[eval] sample 153 iter 0 grad_norm=5.6066e-01, inner_loss=6.6811e+00\n",
      "[eval] sample 153 iter 10 grad_norm=5.3717e-01, inner_loss=6.6509e+00\n",
      "[eval] sample 153 iter 20 grad_norm=5.1510e-01, inner_loss=6.6232e+00\n",
      "[eval] sample 153 iter 30 grad_norm=4.9436e-01, inner_loss=6.5977e+00\n",
      "[eval] sample 153 iter 40 grad_norm=4.7483e-01, inner_loss=6.5742e+00\n",
      "[eval] sample 154 iter 0 grad_norm=4.5933e-01, inner_loss=6.4489e+00\n",
      "[eval] sample 154 iter 10 grad_norm=4.3840e-01, inner_loss=6.4287e+00\n",
      "[eval] sample 154 iter 20 grad_norm=4.1878e-01, inner_loss=6.4103e+00\n",
      "[eval] sample 154 iter 30 grad_norm=4.0037e-01, inner_loss=6.3935e+00\n",
      "[eval] sample 154 iter 40 grad_norm=3.8310e-01, inner_loss=6.3781e+00\n",
      "[eval] sample 155 iter 0 grad_norm=8.2731e-01, inner_loss=7.1871e+00\n",
      "[eval] sample 155 iter 10 grad_norm=7.9068e-01, inner_loss=7.1215e+00\n",
      "[eval] sample 155 iter 20 grad_norm=7.5623e-01, inner_loss=7.0616e+00\n",
      "[eval] sample 155 iter 30 grad_norm=7.2379e-01, inner_loss=7.0067e+00\n",
      "[eval] sample 155 iter 40 grad_norm=6.9323e-01, inner_loss=6.9564e+00\n",
      "[eval] sample 156 iter 0 grad_norm=6.4552e-01, inner_loss=6.7534e+00\n",
      "[eval] sample 156 iter 10 grad_norm=6.1415e-01, inner_loss=6.7137e+00\n",
      "[eval] sample 156 iter 20 grad_norm=5.8479e-01, inner_loss=6.6777e+00\n",
      "[eval] sample 156 iter 30 grad_norm=5.5729e-01, inner_loss=6.6450e+00\n",
      "[eval] sample 156 iter 40 grad_norm=5.3152e-01, inner_loss=6.6153e+00\n",
      "[eval] sample 157 iter 0 grad_norm=7.1913e-01, inner_loss=6.9608e+00\n",
      "[eval] sample 157 iter 10 grad_norm=6.8799e-01, inner_loss=6.9112e+00\n",
      "[eval] sample 157 iter 20 grad_norm=6.5858e-01, inner_loss=6.8657e+00\n",
      "[eval] sample 157 iter 30 grad_norm=6.3080e-01, inner_loss=6.8241e+00\n",
      "[eval] sample 157 iter 40 grad_norm=6.0454e-01, inner_loss=6.7859e+00\n",
      "[eval] sample 158 iter 0 grad_norm=7.3941e-01, inner_loss=6.9643e+00\n",
      "[eval] sample 158 iter 10 grad_norm=7.0488e-01, inner_loss=6.9120e+00\n",
      "[eval] sample 158 iter 20 grad_norm=6.7248e-01, inner_loss=6.8645e+00\n",
      "[eval] sample 158 iter 30 grad_norm=6.4206e-01, inner_loss=6.8212e+00\n",
      "[eval] sample 158 iter 40 grad_norm=6.1348e-01, inner_loss=6.7817e+00\n",
      "[eval] sample 159 iter 0 grad_norm=4.5591e-01, inner_loss=6.4711e+00\n",
      "[eval] sample 159 iter 10 grad_norm=4.3450e-01, inner_loss=6.4513e+00\n",
      "[eval] sample 159 iter 20 grad_norm=4.1460e-01, inner_loss=6.4332e+00\n",
      "[eval] sample 159 iter 30 grad_norm=3.9608e-01, inner_loss=6.4167e+00\n",
      "[eval] sample 159 iter 40 grad_norm=3.7882e-01, inner_loss=6.4017e+00\n",
      "[eval] sample 160 iter 0 grad_norm=7.0203e-01, inner_loss=6.9614e+00\n",
      "[eval] sample 160 iter 10 grad_norm=6.7091e-01, inner_loss=6.9141e+00\n",
      "[eval] sample 160 iter 20 grad_norm=6.4181e-01, inner_loss=6.8710e+00\n",
      "[eval] sample 160 iter 30 grad_norm=6.1459e-01, inner_loss=6.8314e+00\n",
      "[eval] sample 160 iter 40 grad_norm=5.8909e-01, inner_loss=6.7952e+00\n",
      "[eval] sample 161 iter 0 grad_norm=6.8953e-01, inner_loss=6.8791e+00\n",
      "[eval] sample 161 iter 10 grad_norm=6.5718e-01, inner_loss=6.8336e+00\n",
      "[eval] sample 161 iter 20 grad_norm=6.2679e-01, inner_loss=6.7923e+00\n",
      "[eval] sample 161 iter 30 grad_norm=5.9822e-01, inner_loss=6.7547e+00\n",
      "[eval] sample 161 iter 40 grad_norm=5.7134e-01, inner_loss=6.7205e+00\n",
      "[eval] sample 162 iter 0 grad_norm=9.0995e-01, inner_loss=7.4339e+00\n",
      "[eval] sample 162 iter 10 grad_norm=8.6963e-01, inner_loss=7.3546e+00\n",
      "[eval] sample 162 iter 20 grad_norm=8.3176e-01, inner_loss=7.2821e+00\n",
      "[eval] sample 162 iter 30 grad_norm=7.9615e-01, inner_loss=7.2157e+00\n",
      "[eval] sample 162 iter 40 grad_norm=7.6264e-01, inner_loss=7.1549e+00\n",
      "[eval] sample 163 iter 0 grad_norm=8.3229e-01, inner_loss=7.1423e+00\n",
      "[eval] sample 163 iter 10 grad_norm=7.9201e-01, inner_loss=7.0762e+00\n",
      "[eval] sample 163 iter 20 grad_norm=7.5426e-01, inner_loss=7.0163e+00\n",
      "[eval] sample 163 iter 30 grad_norm=7.1885e-01, inner_loss=6.9620e+00\n",
      "[eval] sample 163 iter 40 grad_norm=6.8562e-01, inner_loss=6.9126e+00\n",
      "[eval] sample 164 iter 0 grad_norm=5.9014e-01, inner_loss=6.6783e+00\n",
      "[eval] sample 164 iter 10 grad_norm=5.6122e-01, inner_loss=6.6451e+00\n",
      "[eval] sample 164 iter 20 grad_norm=5.3422e-01, inner_loss=6.6150e+00\n",
      "[eval] sample 164 iter 30 grad_norm=5.0901e-01, inner_loss=6.5878e+00\n",
      "[eval] sample 164 iter 40 grad_norm=4.8545e-01, inner_loss=6.5630e+00\n",
      "[eval] sample 165 iter 0 grad_norm=8.1625e-01, inner_loss=7.3080e+00\n",
      "[eval] sample 165 iter 10 grad_norm=7.8360e-01, inner_loss=7.2439e+00\n",
      "[eval] sample 165 iter 20 grad_norm=7.5279e-01, inner_loss=7.1847e+00\n",
      "[eval] sample 165 iter 30 grad_norm=7.2370e-01, inner_loss=7.1301e+00\n",
      "[eval] sample 165 iter 40 grad_norm=6.9620e-01, inner_loss=7.0797e+00\n",
      "[eval] sample 166 iter 0 grad_norm=5.3759e-01, inner_loss=6.7062e+00\n",
      "[eval] sample 166 iter 10 grad_norm=5.1586e-01, inner_loss=6.6784e+00\n",
      "[eval] sample 166 iter 20 grad_norm=4.9554e-01, inner_loss=6.6528e+00\n",
      "[eval] sample 166 iter 30 grad_norm=4.7653e-01, inner_loss=6.6291e+00\n",
      "[eval] sample 166 iter 40 grad_norm=4.5870e-01, inner_loss=6.6072e+00\n",
      "[eval] sample 167 iter 0 grad_norm=4.5863e-01, inner_loss=6.5786e+00\n",
      "[eval] sample 167 iter 10 grad_norm=4.3879e-01, inner_loss=6.5584e+00\n",
      "[eval] sample 167 iter 20 grad_norm=4.2043e-01, inner_loss=6.5399e+00\n",
      "[eval] sample 167 iter 30 grad_norm=4.0341e-01, inner_loss=6.5229e+00\n",
      "[eval] sample 167 iter 40 grad_norm=3.8762e-01, inner_loss=6.5073e+00\n",
      "[eval] sample 168 iter 0 grad_norm=6.1533e-01, inner_loss=6.6691e+00\n",
      "[eval] sample 168 iter 10 grad_norm=5.8607e-01, inner_loss=6.6329e+00\n",
      "[eval] sample 168 iter 20 grad_norm=5.5865e-01, inner_loss=6.6001e+00\n",
      "[eval] sample 168 iter 30 grad_norm=5.3293e-01, inner_loss=6.5702e+00\n",
      "[eval] sample 168 iter 40 grad_norm=5.0880e-01, inner_loss=6.5430e+00\n",
      "[eval] sample 169 iter 0 grad_norm=5.9626e-01, inner_loss=6.7479e+00\n",
      "[eval] sample 169 iter 10 grad_norm=5.7110e-01, inner_loss=6.7138e+00\n",
      "[eval] sample 169 iter 20 grad_norm=5.4755e-01, inner_loss=6.6825e+00\n",
      "[eval] sample 169 iter 30 grad_norm=5.2548e-01, inner_loss=6.6536e+00\n",
      "[eval] sample 169 iter 40 grad_norm=5.0477e-01, inner_loss=6.6270e+00\n",
      "[eval] sample 170 iter 0 grad_norm=5.1755e-01, inner_loss=6.6331e+00\n",
      "[eval] sample 170 iter 10 grad_norm=4.9600e-01, inner_loss=6.6073e+00\n",
      "[eval] sample 170 iter 20 grad_norm=4.7588e-01, inner_loss=6.5837e+00\n",
      "[eval] sample 170 iter 30 grad_norm=4.5706e-01, inner_loss=6.5619e+00\n",
      "[eval] sample 170 iter 40 grad_norm=4.3944e-01, inner_loss=6.5418e+00\n",
      "[eval] sample 171 iter 0 grad_norm=8.7883e-01, inner_loss=7.0976e+00\n",
      "[eval] sample 171 iter 10 grad_norm=8.3280e-01, inner_loss=7.0242e+00\n",
      "[eval] sample 171 iter 20 grad_norm=7.8984e-01, inner_loss=6.9582e+00\n",
      "[eval] sample 171 iter 30 grad_norm=7.4970e-01, inner_loss=6.8988e+00\n",
      "[eval] sample 171 iter 40 grad_norm=7.1220e-01, inner_loss=6.8453e+00\n",
      "[eval] sample 172 iter 0 grad_norm=7.7838e-01, inner_loss=7.1865e+00\n",
      "[eval] sample 172 iter 10 grad_norm=7.4425e-01, inner_loss=7.1285e+00\n",
      "[eval] sample 172 iter 20 grad_norm=7.1225e-01, inner_loss=7.0753e+00\n",
      "[eval] sample 172 iter 30 grad_norm=6.8223e-01, inner_loss=7.0266e+00\n",
      "[eval] sample 172 iter 40 grad_norm=6.5403e-01, inner_loss=6.9819e+00\n",
      "[eval] sample 173 iter 0 grad_norm=5.2698e-01, inner_loss=6.6439e+00\n",
      "[eval] sample 173 iter 10 grad_norm=5.0519e-01, inner_loss=6.6172e+00\n",
      "[eval] sample 173 iter 20 grad_norm=4.8470e-01, inner_loss=6.5927e+00\n",
      "[eval] sample 173 iter 30 grad_norm=4.6540e-01, inner_loss=6.5700e+00\n",
      "[eval] sample 173 iter 40 grad_norm=4.4722e-01, inner_loss=6.5492e+00\n",
      "[eval] sample 174 iter 0 grad_norm=6.4756e-01, inner_loss=6.7636e+00\n",
      "[eval] sample 174 iter 10 grad_norm=6.1546e-01, inner_loss=6.7236e+00\n",
      "[eval] sample 174 iter 20 grad_norm=5.8548e-01, inner_loss=6.6875e+00\n",
      "[eval] sample 174 iter 30 grad_norm=5.5746e-01, inner_loss=6.6548e+00\n",
      "[eval] sample 174 iter 40 grad_norm=5.3126e-01, inner_loss=6.6251e+00\n",
      "[eval] sample 175 iter 0 grad_norm=3.2967e-01, inner_loss=6.2809e+00\n",
      "[eval] sample 175 iter 10 grad_norm=3.1474e-01, inner_loss=6.2705e+00\n",
      "[eval] sample 175 iter 20 grad_norm=3.0076e-01, inner_loss=6.2610e+00\n",
      "[eval] sample 175 iter 30 grad_norm=2.8767e-01, inner_loss=6.2524e+00\n",
      "[eval] sample 175 iter 40 grad_norm=2.7540e-01, inner_loss=6.2444e+00\n",
      "[eval] sample 176 iter 0 grad_norm=3.6624e-01, inner_loss=6.3159e+00\n",
      "[eval] sample 176 iter 10 grad_norm=3.4963e-01, inner_loss=6.3031e+00\n",
      "[eval] sample 176 iter 20 grad_norm=3.3406e-01, inner_loss=6.2913e+00\n",
      "[eval] sample 176 iter 30 grad_norm=3.1945e-01, inner_loss=6.2807e+00\n",
      "[eval] sample 176 iter 40 grad_norm=3.0573e-01, inner_loss=6.2709e+00\n",
      "[eval] sample 177 iter 0 grad_norm=5.8274e-01, inner_loss=6.8563e+00\n",
      "[eval] sample 177 iter 10 grad_norm=5.5817e-01, inner_loss=6.8237e+00\n",
      "[eval] sample 177 iter 20 grad_norm=5.3540e-01, inner_loss=6.7938e+00\n",
      "[eval] sample 177 iter 30 grad_norm=5.1428e-01, inner_loss=6.7662e+00\n",
      "[eval] sample 177 iter 40 grad_norm=4.9465e-01, inner_loss=6.7407e+00\n",
      "[eval] sample 178 iter 0 grad_norm=5.8701e-01, inner_loss=6.6411e+00\n",
      "[eval] sample 178 iter 10 grad_norm=5.5954e-01, inner_loss=6.6081e+00\n",
      "[eval] sample 178 iter 20 grad_norm=5.3379e-01, inner_loss=6.5782e+00\n",
      "[eval] sample 178 iter 30 grad_norm=5.0965e-01, inner_loss=6.5509e+00\n",
      "[eval] sample 178 iter 40 grad_norm=4.8700e-01, inner_loss=6.5260e+00\n",
      "[eval] sample 179 iter 0 grad_norm=7.0894e-01, inner_loss=7.0179e+00\n",
      "[eval] sample 179 iter 10 grad_norm=6.7550e-01, inner_loss=6.9699e+00\n",
      "[eval] sample 179 iter 20 grad_norm=6.4447e-01, inner_loss=6.9262e+00\n",
      "[eval] sample 179 iter 30 grad_norm=6.1565e-01, inner_loss=6.8865e+00\n",
      "[eval] sample 179 iter 40 grad_norm=5.8886e-01, inner_loss=6.8501e+00\n",
      "[eval] sample 180 iter 0 grad_norm=5.3226e-01, inner_loss=6.5626e+00\n",
      "[eval] sample 180 iter 10 grad_norm=5.0772e-01, inner_loss=6.5355e+00\n",
      "[eval] sample 180 iter 20 grad_norm=4.8472e-01, inner_loss=6.5108e+00\n",
      "[eval] sample 180 iter 30 grad_norm=4.6316e-01, inner_loss=6.4883e+00\n",
      "[eval] sample 180 iter 40 grad_norm=4.4294e-01, inner_loss=6.4678e+00\n",
      "[eval] sample 181 iter 0 grad_norm=6.8383e-01, inner_loss=7.0452e+00\n",
      "[eval] sample 181 iter 10 grad_norm=6.5464e-01, inner_loss=7.0003e+00\n",
      "[eval] sample 181 iter 20 grad_norm=6.2753e-01, inner_loss=6.9591e+00\n",
      "[eval] sample 181 iter 30 grad_norm=6.0232e-01, inner_loss=6.9213e+00\n",
      "[eval] sample 181 iter 40 grad_norm=5.7883e-01, inner_loss=6.8863e+00\n",
      "[eval] sample 182 iter 0 grad_norm=6.1667e-01, inner_loss=6.7356e+00\n",
      "[eval] sample 182 iter 10 grad_norm=5.8803e-01, inner_loss=6.6993e+00\n",
      "[eval] sample 182 iter 20 grad_norm=5.6122e-01, inner_loss=6.6662e+00\n",
      "[eval] sample 182 iter 30 grad_norm=5.3610e-01, inner_loss=6.6360e+00\n",
      "[eval] sample 182 iter 40 grad_norm=5.1256e-01, inner_loss=6.6085e+00\n",
      "[eval] sample 183 iter 0 grad_norm=9.6372e-01, inner_loss=7.5754e+00\n",
      "[eval] sample 183 iter 10 grad_norm=9.1889e-01, inner_loss=7.4866e+00\n",
      "[eval] sample 183 iter 20 grad_norm=8.7676e-01, inner_loss=7.4058e+00\n",
      "[eval] sample 183 iter 30 grad_norm=8.3715e-01, inner_loss=7.3322e+00\n",
      "[eval] sample 183 iter 40 grad_norm=7.9990e-01, inner_loss=7.2651e+00\n",
      "[eval] sample 184 iter 0 grad_norm=5.8195e-01, inner_loss=6.7931e+00\n",
      "[eval] sample 184 iter 10 grad_norm=5.5740e-01, inner_loss=6.7606e+00\n",
      "[eval] sample 184 iter 20 grad_norm=5.3456e-01, inner_loss=6.7307e+00\n",
      "[eval] sample 184 iter 30 grad_norm=5.1328e-01, inner_loss=6.7033e+00\n",
      "[eval] sample 184 iter 40 grad_norm=4.9341e-01, inner_loss=6.6779e+00\n",
      "[eval] sample 185 iter 0 grad_norm=6.1220e-01, inner_loss=6.8189e+00\n",
      "[eval] sample 185 iter 10 grad_norm=5.8700e-01, inner_loss=6.7829e+00\n",
      "[eval] sample 185 iter 20 grad_norm=5.6338e-01, inner_loss=6.7498e+00\n",
      "[eval] sample 185 iter 30 grad_norm=5.4122e-01, inner_loss=6.7192e+00\n",
      "[eval] sample 185 iter 40 grad_norm=5.2041e-01, inner_loss=6.6910e+00\n",
      "[eval] sample 186 iter 0 grad_norm=9.1406e-01, inner_loss=7.3399e+00\n",
      "[eval] sample 186 iter 10 grad_norm=8.6984e-01, inner_loss=7.2602e+00\n",
      "[eval] sample 186 iter 20 grad_norm=8.2852e-01, inner_loss=7.1879e+00\n",
      "[eval] sample 186 iter 30 grad_norm=7.8988e-01, inner_loss=7.1223e+00\n",
      "[eval] sample 186 iter 40 grad_norm=7.5372e-01, inner_loss=7.0626e+00\n",
      "[eval] sample 187 iter 0 grad_norm=7.4451e-01, inner_loss=7.1752e+00\n",
      "[eval] sample 187 iter 10 grad_norm=7.1658e-01, inner_loss=7.1217e+00\n",
      "[eval] sample 187 iter 20 grad_norm=6.9022e-01, inner_loss=7.0722e+00\n",
      "[eval] sample 187 iter 30 grad_norm=6.6533e-01, inner_loss=7.0262e+00\n",
      "[eval] sample 187 iter 40 grad_norm=6.4178e-01, inner_loss=6.9834e+00\n",
      "[eval] sample 188 iter 0 grad_norm=9.5971e-01, inner_loss=7.5031e+00\n",
      "[eval] sample 188 iter 10 grad_norm=9.1480e-01, inner_loss=7.4151e+00\n",
      "[eval] sample 188 iter 20 grad_norm=8.7258e-01, inner_loss=7.3351e+00\n",
      "[eval] sample 188 iter 30 grad_norm=8.3285e-01, inner_loss=7.2622e+00\n",
      "[eval] sample 188 iter 40 grad_norm=7.9546e-01, inner_loss=7.1958e+00\n",
      "[eval] sample 189 iter 0 grad_norm=4.1919e-01, inner_loss=6.3919e+00\n",
      "[eval] sample 189 iter 10 grad_norm=4.0024e-01, inner_loss=6.3751e+00\n",
      "[eval] sample 189 iter 20 grad_norm=3.8246e-01, inner_loss=6.3598e+00\n",
      "[eval] sample 189 iter 30 grad_norm=3.6578e-01, inner_loss=6.3457e+00\n",
      "[eval] sample 189 iter 40 grad_norm=3.5012e-01, inner_loss=6.3329e+00\n",
      "[eval] sample 190 iter 0 grad_norm=7.5011e-01, inner_loss=6.8036e+00\n",
      "[eval] sample 190 iter 10 grad_norm=7.1062e-01, inner_loss=6.7501e+00\n",
      "[eval] sample 190 iter 20 grad_norm=6.7372e-01, inner_loss=6.7021e+00\n",
      "[eval] sample 190 iter 30 grad_norm=6.3921e-01, inner_loss=6.6589e+00\n",
      "[eval] sample 190 iter 40 grad_norm=6.0693e-01, inner_loss=6.6200e+00\n",
      "[eval] sample 191 iter 0 grad_norm=5.5694e-01, inner_loss=6.5897e+00\n",
      "[eval] sample 191 iter 10 grad_norm=5.3087e-01, inner_loss=6.5601e+00\n",
      "[eval] sample 191 iter 20 grad_norm=5.0645e-01, inner_loss=6.5331e+00\n",
      "[eval] sample 191 iter 30 grad_norm=4.8356e-01, inner_loss=6.5086e+00\n",
      "[eval] sample 191 iter 40 grad_norm=4.6208e-01, inner_loss=6.4862e+00\n",
      "[eval] sample 192 iter 0 grad_norm=7.7475e-01, inner_loss=7.1744e+00\n",
      "[eval] sample 192 iter 10 grad_norm=7.4273e-01, inner_loss=7.1167e+00\n",
      "[eval] sample 192 iter 20 grad_norm=7.1255e-01, inner_loss=7.0637e+00\n",
      "[eval] sample 192 iter 30 grad_norm=6.8408e-01, inner_loss=7.0148e+00\n",
      "[eval] sample 192 iter 40 grad_norm=6.5721e-01, inner_loss=6.9697e+00\n",
      "[eval] sample 193 iter 0 grad_norm=5.8402e-01, inner_loss=6.6600e+00\n",
      "[eval] sample 193 iter 10 grad_norm=5.5685e-01, inner_loss=6.6274e+00\n",
      "[eval] sample 193 iter 20 grad_norm=5.3147e-01, inner_loss=6.5977e+00\n",
      "[eval] sample 193 iter 30 grad_norm=5.0776e-01, inner_loss=6.5706e+00\n",
      "[eval] sample 193 iter 40 grad_norm=4.8558e-01, inner_loss=6.5459e+00\n",
      "[eval] sample 194 iter 0 grad_norm=8.3040e-01, inner_loss=7.1888e+00\n",
      "[eval] sample 194 iter 10 grad_norm=7.9254e-01, inner_loss=7.1229e+00\n",
      "[eval] sample 194 iter 20 grad_norm=7.5688e-01, inner_loss=7.0627e+00\n",
      "[eval] sample 194 iter 30 grad_norm=7.2329e-01, inner_loss=7.0078e+00\n",
      "[eval] sample 194 iter 40 grad_norm=6.9162e-01, inner_loss=6.9577e+00\n",
      "[eval] sample 195 iter 0 grad_norm=4.0331e-01, inner_loss=6.3829e+00\n",
      "[eval] sample 195 iter 10 grad_norm=3.8530e-01, inner_loss=6.3673e+00\n",
      "[eval] sample 195 iter 20 grad_norm=3.6843e-01, inner_loss=6.3531e+00\n",
      "[eval] sample 195 iter 30 grad_norm=3.5261e-01, inner_loss=6.3401e+00\n",
      "[eval] sample 195 iter 40 grad_norm=3.3775e-01, inner_loss=6.3282e+00\n",
      "[eval] sample 196 iter 0 grad_norm=7.3164e-01, inner_loss=6.8114e+00\n",
      "[eval] sample 196 iter 10 grad_norm=6.9457e-01, inner_loss=6.7605e+00\n",
      "[eval] sample 196 iter 20 grad_norm=6.5989e-01, inner_loss=6.7145e+00\n",
      "[eval] sample 196 iter 30 grad_norm=6.2743e-01, inner_loss=6.6730e+00\n",
      "[eval] sample 196 iter 40 grad_norm=5.9703e-01, inner_loss=6.6354e+00\n",
      "[eval] sample 197 iter 0 grad_norm=7.6835e-01, inner_loss=6.9280e+00\n",
      "[eval] sample 197 iter 10 grad_norm=7.2698e-01, inner_loss=6.8719e+00\n",
      "[eval] sample 197 iter 20 grad_norm=6.8853e-01, inner_loss=6.8217e+00\n",
      "[eval] sample 197 iter 30 grad_norm=6.5277e-01, inner_loss=6.7767e+00\n",
      "[eval] sample 197 iter 40 grad_norm=6.1949e-01, inner_loss=6.7361e+00\n",
      "[eval] sample 198 iter 0 grad_norm=5.8557e-01, inner_loss=6.7673e+00\n",
      "[eval] sample 198 iter 10 grad_norm=5.6150e-01, inner_loss=6.7344e+00\n",
      "[eval] sample 198 iter 20 grad_norm=5.3899e-01, inner_loss=6.7040e+00\n",
      "[eval] sample 198 iter 30 grad_norm=5.1789e-01, inner_loss=6.6761e+00\n",
      "[eval] sample 198 iter 40 grad_norm=4.9811e-01, inner_loss=6.6502e+00\n",
      "[eval] sample 199 iter 0 grad_norm=9.0940e-01, inner_loss=7.1849e+00\n",
      "[eval] sample 199 iter 10 grad_norm=8.6150e-01, inner_loss=7.1063e+00\n",
      "[eval] sample 199 iter 20 grad_norm=8.1679e-01, inner_loss=7.0357e+00\n",
      "[eval] sample 199 iter 30 grad_norm=7.7502e-01, inner_loss=6.9722e+00\n",
      "[eval] sample 199 iter 40 grad_norm=7.3599e-01, inner_loss=6.9150e+00\n",
      "\n",
      "--- Classification Performance (Evaluation) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        17\n",
      "           1       1.00      0.79      0.88        28\n",
      "           2       0.64      0.88      0.74        16\n",
      "           3       0.50      0.88      0.64        16\n",
      "           4       0.67      1.00      0.80        28\n",
      "           5       0.92      0.55      0.69        20\n",
      "           6       1.00      0.70      0.82        20\n",
      "           7       0.94      0.62      0.75        24\n",
      "           8       0.50      0.80      0.62        10\n",
      "           9       0.83      0.48      0.61        21\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.80      0.76      0.75       200\n",
      "weighted avg       0.83      0.76      0.76       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOAG] Sample 401: label=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukta\\AppData\\Local\\Temp\\ipykernel_19036\\4281910160.py:438: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"\\n[HOAG] Sample {sample_count}: label={int(label.cpu().numpy())}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HOAG] theta now: [-4.5152435  -0.34539473]\n",
      "\n",
      "[HOAG] Sample 402: label=1\n",
      "[HOAG] theta now: [-4.518479  -0.3424514]\n",
      "\n",
      "[HOAG] Sample 403: label=1\n",
      "[HOAG] theta now: [-4.5217843 -0.3391012]\n",
      "\n",
      "[HOAG] Sample 404: label=6\n",
      "[HOAG] theta now: [-4.525091   -0.33604187]\n",
      "\n",
      "[HOAG] Sample 405: label=4\n",
      "[HOAG] theta now: [-4.52708    -0.33417028]\n",
      "\n",
      "[HOAG] Sample 406: label=1\n",
      "[HOAG] theta now: [-4.531051  -0.3305631]\n",
      "\n",
      "[HOAG] Sample 407: label=0\n",
      "[HOAG] theta now: [-4.5318117 -0.329894 ]\n",
      "\n",
      "[HOAG] Sample 408: label=6\n",
      "[HOAG] theta now: [-4.53272    -0.32904086]\n",
      "\n",
      "[HOAG] Sample 409: label=7\n",
      "[HOAG] theta now: [-4.5333843  -0.32841077]\n",
      "\n",
      "[HOAG] Sample 410: label=4\n",
      "[HOAG] theta now: [-4.5349827 -0.3269905]\n",
      "\n",
      "[HOAG] Sample 411: label=8\n",
      "[HOAG] theta now: [-4.539338   -0.32287022]\n",
      "\n",
      "[HOAG] Sample 412: label=0\n",
      "[HOAG] theta now: [-4.540472  -0.3217866]\n",
      "\n",
      "[HOAG] Sample 413: label=6\n",
      "[HOAG] theta now: [-4.5440826  -0.31852168]\n",
      "\n",
      "[HOAG] Sample 414: label=1\n",
      "[HOAG] theta now: [-4.550216   -0.31273732]\n",
      "\n",
      "[HOAG] Sample 415: label=5\n",
      "[HOAG] theta now: [-4.5518494 -0.3112763]\n",
      "\n",
      "[HOAG] Sample 416: label=2\n",
      "[HOAG] theta now: [-4.551958   -0.31110263]\n",
      "\n",
      "[HOAG] Sample 417: label=0\n",
      "[HOAG] theta now: [-4.553405   -0.30983433]\n",
      "\n",
      "[HOAG] Sample 418: label=9\n",
      "[HOAG] theta now: [-4.556709  -0.3067408]\n",
      "\n",
      "[HOAG] Sample 419: label=6\n",
      "[HOAG] theta now: [-4.5583534 -0.3052268]\n",
      "\n",
      "[HOAG] Sample 420: label=1\n",
      "[HOAG] theta now: [-4.5605736  -0.30319327]\n",
      "\n",
      "[HOAG] Sample 421: label=2\n",
      "[HOAG] theta now: [-4.56177    -0.30204228]\n",
      "\n",
      "[HOAG] Sample 422: label=5\n",
      "[HOAG] theta now: [-4.5647016  -0.29938772]\n",
      "\n",
      "[HOAG] Sample 423: label=3\n",
      "[HOAG] theta now: [-4.568155   -0.29615292]\n",
      "\n",
      "[HOAG] Sample 424: label=1\n",
      "[HOAG] theta now: [-4.572917  -0.2918113]\n",
      "\n",
      "[HOAG] Sample 425: label=1\n",
      "[HOAG] theta now: [-4.5749087  -0.29000267]\n",
      "\n",
      "[HOAG] Sample 426: label=7\n",
      "[HOAG] theta now: [-4.5776     -0.28749427]\n",
      "\n",
      "[HOAG] Sample 427: label=2\n",
      "[HOAG] theta now: [-4.577435   -0.28753865]\n",
      "\n",
      "[HOAG] Sample 428: label=0\n",
      "[HOAG] theta now: [-4.5778747  -0.28715786]\n",
      "\n",
      "[HOAG] Sample 429: label=7\n",
      "[HOAG] theta now: [-4.5797176  -0.28549635]\n",
      "\n",
      "[HOAG] Sample 430: label=1\n",
      "[HOAG] theta now: [-4.5821476  -0.28328896]\n",
      "\n",
      "[HOAG] Sample 431: label=3\n",
      "[HOAG] theta now: [-4.584352   -0.28143045]\n",
      "\n",
      "[HOAG] Sample 432: label=1\n",
      "[HOAG] theta now: [-4.5856385  -0.28022942]\n",
      "\n",
      "[HOAG] Sample 433: label=4\n",
      "[HOAG] theta now: [-4.586402   -0.27970234]\n",
      "\n",
      "[HOAG] Sample 434: label=1\n",
      "[HOAG] theta now: [-4.5924397 -0.274048 ]\n",
      "\n",
      "[HOAG] Sample 435: label=1\n",
      "[HOAG] theta now: [-4.59398    -0.27263796]\n",
      "\n",
      "[HOAG] Sample 436: label=2\n",
      "[HOAG] theta now: [-4.5944405 -0.2721683]\n",
      "\n",
      "[HOAG] Sample 437: label=4\n",
      "[HOAG] theta now: [-4.594743  -0.2718568]\n",
      "\n",
      "[HOAG] Sample 438: label=3\n",
      "[HOAG] theta now: [-4.5968914 -0.2702178]\n",
      "\n",
      "[HOAG] Sample 439: label=4\n",
      "[HOAG] theta now: [-4.597567   -0.26957542]\n",
      "\n",
      "[HOAG] Sample 440: label=7\n",
      "[HOAG] theta now: [-4.6026754  -0.26473776]\n",
      "\n",
      "[HOAG] Sample 441: label=5\n",
      "[HOAG] theta now: [-4.6033015  -0.26404318]\n",
      "\n",
      "[HOAG] Sample 442: label=6\n",
      "[HOAG] theta now: [-4.598332   -0.26848665]\n",
      "\n",
      "[HOAG] Sample 443: label=6\n",
      "[HOAG] theta now: [-4.6026826 -0.2642164]\n",
      "\n",
      "[HOAG] Sample 444: label=7\n",
      "[HOAG] theta now: [-4.6052814  -0.26176554]\n",
      "\n",
      "[HOAG] Sample 445: label=7\n",
      "[HOAG] theta now: [-4.607966  -0.2592453]\n",
      "\n",
      "[HOAG] Sample 446: label=4\n",
      "[HOAG] theta now: [-4.6080694  -0.25912985]\n",
      "\n",
      "[HOAG] Sample 447: label=7\n",
      "[HOAG] theta now: [-4.6116123  -0.25577477]\n",
      "\n",
      "[HOAG] Sample 448: label=5\n",
      "[HOAG] theta now: [-4.6142263  -0.25329262]\n",
      "\n",
      "[HOAG] Sample 449: label=0\n",
      "[HOAG] theta now: [-4.6160297  -0.25159544]\n",
      "\n",
      "[HOAG] Sample 450: label=6\n",
      "[HOAG] theta now: [-4.6187763  -0.24896717]\n",
      "\n",
      "[HOAG] Sample 451: label=9\n",
      "[HOAG] theta now: [-4.622531   -0.24537946]\n",
      "\n",
      "[HOAG] Sample 452: label=9\n",
      "[HOAG] theta now: [-4.6266313  -0.24153295]\n",
      "\n",
      "[HOAG] Sample 453: label=6\n",
      "[HOAG] theta now: [-4.6282244  -0.24004386]\n",
      "\n",
      "[HOAG] Sample 454: label=9\n",
      "[HOAG] theta now: [-4.6304398  -0.23778194]\n",
      "\n",
      "[HOAG] Sample 455: label=2\n",
      "[HOAG] theta now: [-4.631437   -0.23690811]\n",
      "\n",
      "[HOAG] Sample 456: label=6\n",
      "[HOAG] theta now: [-4.6319785  -0.23637615]\n",
      "\n",
      "[HOAG] Sample 457: label=9\n",
      "[HOAG] theta now: [-4.634216   -0.23426291]\n",
      "\n",
      "[HOAG] Sample 458: label=3\n",
      "[HOAG] theta now: [-4.635611   -0.23269694]\n",
      "\n",
      "[HOAG] Sample 459: label=6\n",
      "[HOAG] theta now: [-4.637612   -0.23084073]\n",
      "\n",
      "[HOAG] Sample 460: label=7\n",
      "[HOAG] theta now: [-4.638344   -0.23012994]\n",
      "\n",
      "[HOAG] Sample 461: label=7\n",
      "[HOAG] theta now: [-4.6422815  -0.22612347]\n",
      "\n",
      "[HOAG] Sample 462: label=3\n",
      "[HOAG] theta now: [-4.643352   -0.22517386]\n",
      "\n",
      "[HOAG] Sample 463: label=3\n",
      "[HOAG] theta now: [-4.6446743  -0.22390763]\n",
      "\n",
      "[HOAG] Sample 464: label=3\n",
      "[HOAG] theta now: [-4.6478324  -0.22074497]\n",
      "\n",
      "[HOAG] Sample 465: label=9\n",
      "[HOAG] theta now: [-4.6491165  -0.21951093]\n",
      "\n",
      "[HOAG] Sample 466: label=7\n",
      "[HOAG] theta now: [-4.6550045  -0.21376498]\n",
      "\n",
      "[HOAG] Sample 467: label=8\n",
      "[HOAG] theta now: [-4.6567764  -0.21215993]\n",
      "\n",
      "[HOAG] Sample 468: label=6\n",
      "[HOAG] theta now: [-4.6580124 -0.2108964]\n",
      "\n",
      "[HOAG] Sample 469: label=5\n",
      "[HOAG] theta now: [-4.660879   -0.20820467]\n",
      "\n",
      "[HOAG] Sample 470: label=0\n",
      "[HOAG] theta now: [-4.664114   -0.20472562]\n",
      "\n",
      "[HOAG] Sample 471: label=2\n",
      "[HOAG] theta now: [-4.6647     -0.20414992]\n",
      "\n",
      "[HOAG] Sample 472: label=7\n",
      "[HOAG] theta now: [-4.6649756  -0.20389394]\n",
      "\n",
      "[HOAG] Sample 473: label=9\n",
      "[HOAG] theta now: [-4.6683326  -0.20057723]\n",
      "\n",
      "[HOAG] Sample 474: label=7\n",
      "[HOAG] theta now: [-4.6701317  -0.19885157]\n",
      "\n",
      "[HOAG] Sample 475: label=6\n",
      "[HOAG] theta now: [-4.671595   -0.19747864]\n",
      "\n",
      "[HOAG] Sample 476: label=1\n",
      "[HOAG] theta now: [-4.6782002 -0.191199 ]\n",
      "\n",
      "[HOAG] Sample 477: label=1\n",
      "[HOAG] theta now: [-4.6816564  -0.18803391]\n",
      "\n",
      "[HOAG] Sample 478: label=6\n",
      "[HOAG] theta now: [-4.687248   -0.18254039]\n",
      "\n",
      "[HOAG] Sample 479: label=4\n",
      "[HOAG] theta now: [-4.6870446  -0.18283032]\n",
      "\n",
      "[HOAG] Sample 480: label=7\n",
      "[HOAG] theta now: [-4.687128   -0.18274829]\n",
      "\n",
      "[HOAG] Sample 481: label=1\n",
      "[HOAG] theta now: [-4.6898885  -0.18026713]\n",
      "\n",
      "[HOAG] Sample 482: label=5\n",
      "[HOAG] theta now: [-4.691209   -0.17892444]\n",
      "\n",
      "[HOAG] Sample 483: label=1\n",
      "[HOAG] theta now: [-4.69454    -0.17576838]\n",
      "\n",
      "[HOAG] Sample 484: label=0\n",
      "[HOAG] theta now: [-4.6956716  -0.17464036]\n",
      "\n",
      "[HOAG] Sample 485: label=8\n",
      "[HOAG] theta now: [-4.6954017  -0.17506325]\n",
      "\n",
      "[HOAG] Sample 486: label=4\n",
      "[HOAG] theta now: [-4.695876   -0.17452273]\n",
      "\n",
      "[HOAG] Sample 487: label=2\n",
      "[HOAG] theta now: [-4.6958966  -0.17450328]\n",
      "\n",
      "[HOAG] Sample 488: label=5\n",
      "[HOAG] theta now: [-4.697954   -0.17261323]\n",
      "\n",
      "[HOAG] Sample 489: label=4\n",
      "[HOAG] theta now: [-4.6959977  -0.17449413]\n",
      "\n",
      "[HOAG] Sample 490: label=6\n",
      "[HOAG] theta now: [-4.6966443  -0.17388661]\n",
      "\n",
      "[HOAG] Sample 491: label=1\n",
      "[HOAG] theta now: [-4.697882   -0.17267662]\n",
      "\n",
      "[HOAG] Sample 492: label=0\n",
      "[HOAG] theta now: [-4.701477   -0.16909072]\n",
      "\n",
      "[HOAG] Sample 493: label=5\n",
      "[HOAG] theta now: [-4.7008886  -0.16931252]\n",
      "\n",
      "[HOAG] Sample 494: label=7\n",
      "[HOAG] theta now: [-4.705627   -0.16464855]\n",
      "\n",
      "[HOAG] Sample 495: label=4\n",
      "[HOAG] theta now: [-4.705747   -0.16451901]\n",
      "\n",
      "[HOAG] Sample 496: label=8\n",
      "[HOAG] theta now: [-4.709338   -0.16106857]\n",
      "\n",
      "[HOAG] Sample 497: label=4\n",
      "[HOAG] theta now: [-4.710129   -0.16030496]\n",
      "\n",
      "[HOAG] Sample 498: label=4\n",
      "[HOAG] theta now: [-4.7104154  -0.16002394]\n",
      "\n",
      "[HOAG] Sample 499: label=6\n",
      "[HOAG] theta now: [-4.7117677  -0.15875243]\n",
      "\n",
      "[HOAG] Sample 500: label=0\n",
      "[HOAG] theta now: [-4.712423   -0.15815352]\n",
      "=== HOAG epoch 2/2 ===\n",
      "\n",
      "[HOAG] Sample 501: label=8\n",
      "[HOAG] theta now: [-4.7159834  -0.15473127]\n",
      "\n",
      "[HOAG] Sample 502: label=0\n",
      "[HOAG] theta now: [-4.718832   -0.15201657]\n",
      "\n",
      "[HOAG] Sample 503: label=2\n",
      "[HOAG] theta now: [-4.7189026  -0.15186675]\n",
      "\n",
      "[HOAG] Sample 504: label=8\n",
      "[HOAG] theta now: [-4.7206483  -0.15025657]\n",
      "\n",
      "[HOAG] Sample 505: label=3\n",
      "[HOAG] theta now: [-4.721503  -0.1494413]\n",
      "\n",
      "[HOAG] Sample 506: label=7\n",
      "[HOAG] theta now: [-4.725267   -0.14591861]\n",
      "\n",
      "[HOAG] Sample 507: label=7\n",
      "[HOAG] theta now: [-4.7252326  -0.14606366]\n",
      "\n",
      "[HOAG] Sample 508: label=3\n",
      "[HOAG] theta now: [-4.725996   -0.14529228]\n",
      "\n",
      "[HOAG] Sample 509: label=1\n",
      "[HOAG] theta now: [-4.7276406  -0.14375482]\n",
      "\n",
      "[HOAG] Sample 510: label=8\n",
      "[HOAG] theta now: [-4.7289815  -0.14249496]\n",
      "\n",
      "[HOAG] Sample 511: label=7\n",
      "[HOAG] theta now: [-4.733355   -0.13827889]\n",
      "\n",
      "[HOAG] Sample 512: label=1\n",
      "[HOAG] theta now: [-4.735412  -0.1363407]\n",
      "\n",
      "[HOAG] Sample 513: label=3\n",
      "[HOAG] theta now: [-4.737177   -0.13462271]\n",
      "\n",
      "[HOAG] Sample 514: label=3\n",
      "[HOAG] theta now: [-4.739043   -0.13295373]\n",
      "\n",
      "[HOAG] Sample 515: label=8\n",
      "[HOAG] theta now: [-4.739685   -0.13233043]\n",
      "\n",
      "[HOAG] Sample 516: label=7\n",
      "[HOAG] theta now: [-4.7414565  -0.13046126]\n",
      "\n",
      "[HOAG] Sample 517: label=6\n",
      "[HOAG] theta now: [-4.743135   -0.12892771]\n",
      "\n",
      "[HOAG] Sample 518: label=7\n",
      "[HOAG] theta now: [-4.7439766  -0.12810609]\n",
      "\n",
      "[HOAG] Sample 519: label=4\n",
      "[HOAG] theta now: [-4.7467     -0.12541991]\n",
      "\n",
      "[HOAG] Sample 520: label=4\n",
      "[HOAG] theta now: [-4.747099  -0.1249211]\n",
      "\n",
      "[HOAG] Sample 521: label=5\n",
      "[HOAG] theta now: [-4.7483172  -0.12398119]\n",
      "\n",
      "[HOAG] Sample 522: label=4\n",
      "[HOAG] theta now: [-4.7483196  -0.12387552]\n",
      "\n",
      "[HOAG] Sample 523: label=5\n",
      "[HOAG] theta now: [-4.748021   -0.12401177]\n",
      "\n",
      "[HOAG] Sample 524: label=8\n",
      "[HOAG] theta now: [-4.7512174  -0.12093361]\n",
      "\n",
      "[HOAG] Sample 525: label=0\n",
      "[HOAG] theta now: [-4.751685   -0.12049709]\n",
      "\n",
      "[HOAG] Sample 526: label=5\n",
      "[HOAG] theta now: [-4.753059   -0.11944345]\n",
      "\n",
      "[HOAG] Sample 527: label=0\n",
      "[HOAG] theta now: [-4.753605  -0.1189431]\n",
      "\n",
      "[HOAG] Sample 528: label=7\n",
      "[HOAG] theta now: [-4.756366   -0.11624879]\n",
      "\n",
      "[HOAG] Sample 529: label=7\n",
      "[HOAG] theta now: [-4.757754   -0.11493146]\n",
      "\n",
      "[HOAG] Sample 530: label=7\n",
      "[HOAG] theta now: [-4.75982    -0.11289162]\n",
      "\n",
      "[HOAG] Sample 531: label=6\n",
      "[HOAG] theta now: [-4.761163   -0.11161143]\n",
      "\n",
      "[HOAG] Sample 532: label=4\n",
      "[HOAG] theta now: [-4.7612023  -0.11143856]\n",
      "\n",
      "[HOAG] Sample 533: label=1\n",
      "[HOAG] theta now: [-4.7625165  -0.11012403]\n",
      "\n",
      "[HOAG] Sample 534: label=9\n",
      "[HOAG] theta now: [-4.7604446  -0.11166435]\n",
      "\n",
      "[HOAG] Sample 535: label=4\n",
      "[HOAG] theta now: [-4.7626348  -0.10951748]\n",
      "\n",
      "[HOAG] Sample 536: label=8\n",
      "[HOAG] theta now: [-4.763341   -0.10886124]\n",
      "\n",
      "[HOAG] Sample 537: label=0\n",
      "[HOAG] theta now: [-4.7658176  -0.10639126]\n",
      "\n",
      "[HOAG] Sample 538: label=7\n",
      "[HOAG] theta now: [-4.7660537  -0.10616876]\n",
      "\n",
      "[HOAG] Sample 539: label=7\n",
      "[HOAG] theta now: [-4.7675104  -0.10490642]\n",
      "\n",
      "[HOAG] Sample 540: label=3\n",
      "[HOAG] theta now: [-4.769206   -0.10328606]\n",
      "\n",
      "[HOAG] Sample 541: label=1\n",
      "[HOAG] theta now: [-4.7712345  -0.10137013]\n",
      "\n",
      "[HOAG] Sample 542: label=1\n",
      "[HOAG] theta now: [-4.77237    -0.10029627]\n",
      "\n",
      "[HOAG] Sample 543: label=5\n",
      "[HOAG] theta now: [-4.775015   -0.09741569]\n",
      "\n",
      "[HOAG] Sample 544: label=1\n",
      "[HOAG] theta now: [-4.7795954  -0.09317719]\n",
      "\n",
      "[HOAG] Sample 545: label=9\n",
      "[HOAG] theta now: [-4.784311  -0.0888133]\n",
      "\n",
      "[HOAG] Sample 546: label=7\n",
      "[HOAG] theta now: [-4.7852883  -0.08788696]\n",
      "\n",
      "[HOAG] Sample 547: label=6\n",
      "[HOAG] theta now: [-4.787562   -0.08567224]\n",
      "\n",
      "[HOAG] Sample 548: label=8\n",
      "[HOAG] theta now: [-4.7905498 -0.0831513]\n",
      "\n",
      "[HOAG] Sample 549: label=1\n",
      "[HOAG] theta now: [-4.7941246  -0.07967823]\n",
      "\n",
      "[HOAG] Sample 550: label=2\n",
      "[HOAG] theta now: [-4.7941675  -0.07963363]\n",
      "\n",
      "[HOAG] Sample 551: label=1\n",
      "[HOAG] theta now: [-4.7970643  -0.07686199]\n",
      "\n",
      "[HOAG] Sample 552: label=7\n",
      "[HOAG] theta now: [-4.8001795  -0.07381633]\n",
      "\n",
      "[HOAG] Sample 553: label=1\n",
      "[HOAG] theta now: [-4.803663  -0.0703919]\n",
      "\n",
      "[HOAG] Sample 554: label=5\n",
      "[HOAG] theta now: [-4.803261   -0.07050838]\n",
      "\n",
      "[HOAG] Sample 555: label=1\n",
      "[HOAG] theta now: [-4.8046403  -0.06920046]\n",
      "\n",
      "[HOAG] Sample 556: label=9\n",
      "[HOAG] theta now: [-4.80683    -0.06708718]\n",
      "\n",
      "[HOAG] Sample 557: label=4\n",
      "[HOAG] theta now: [-4.807577   -0.06622784]\n",
      "\n",
      "[HOAG] Sample 558: label=7\n",
      "[HOAG] theta now: [-4.809079   -0.06476834]\n",
      "\n",
      "[HOAG] Sample 559: label=9\n",
      "[HOAG] theta now: [-4.8114877  -0.06244669]\n",
      "\n",
      "[HOAG] Sample 560: label=0\n",
      "[HOAG] theta now: [-4.8119054  -0.06202522]\n",
      "\n",
      "[HOAG] Sample 561: label=3\n",
      "[HOAG] theta now: [-4.812901   -0.06112813]\n",
      "\n",
      "[HOAG] Sample 562: label=0\n",
      "[HOAG] theta now: [-4.814335   -0.05974765]\n",
      "\n",
      "[HOAG] Sample 563: label=9\n",
      "[HOAG] theta now: [-4.816605   -0.05748929]\n",
      "\n",
      "[HOAG] Sample 564: label=7\n",
      "[HOAG] theta now: [-4.8185163  -0.05564137]\n",
      "\n",
      "[HOAG] Sample 565: label=0\n",
      "[HOAG] theta now: [-4.82062    -0.05361972]\n",
      "\n",
      "[HOAG] Sample 566: label=2\n",
      "[HOAG] theta now: [-4.8210387  -0.05325174]\n",
      "\n",
      "[HOAG] Sample 567: label=7\n",
      "[HOAG] theta now: [-4.821463   -0.05283166]\n",
      "\n",
      "[HOAG] Sample 568: label=6\n",
      "[HOAG] theta now: [-4.822869   -0.05144945]\n",
      "\n",
      "[HOAG] Sample 569: label=8\n",
      "[HOAG] theta now: [-4.8252034  -0.04913069]\n",
      "\n",
      "[HOAG] Sample 570: label=5\n",
      "[HOAG] theta now: [-4.826922   -0.04753291]\n",
      "\n",
      "[HOAG] Sample 571: label=2\n",
      "[HOAG] theta now: [-4.8272405  -0.04710696]\n",
      "\n",
      "[HOAG] Sample 572: label=2\n",
      "[HOAG] theta now: [-4.8280864  -0.04628318]\n",
      "\n",
      "[HOAG] Sample 573: label=8\n",
      "[HOAG] theta now: [-4.830031   -0.04440457]\n",
      "\n",
      "[HOAG] Sample 574: label=3\n",
      "[HOAG] theta now: [-4.831638   -0.04275507]\n",
      "\n",
      "[HOAG] Sample 575: label=7\n",
      "[HOAG] theta now: [-4.8316984 -0.0426943]\n",
      "\n",
      "[HOAG] Sample 576: label=2\n",
      "[HOAG] theta now: [-4.8329024  -0.04160209]\n",
      "\n",
      "[HOAG] Sample 577: label=6\n",
      "[HOAG] theta now: [-4.8335137  -0.04101473]\n",
      "\n",
      "[HOAG] Sample 578: label=6\n",
      "[HOAG] theta now: [-4.830438   -0.04395151]\n",
      "\n",
      "[HOAG] Sample 579: label=3\n",
      "[HOAG] theta now: [-4.830367   -0.04383662]\n",
      "\n",
      "[HOAG] Sample 580: label=1\n",
      "[HOAG] theta now: [-4.833648  -0.0407042]\n",
      "\n",
      "[HOAG] Sample 581: label=7\n",
      "[HOAG] theta now: [-4.835658   -0.03866234]\n",
      "\n",
      "[HOAG] Sample 582: label=2\n",
      "[HOAG] theta now: [-4.836318   -0.03803862]\n",
      "\n",
      "[HOAG] Sample 583: label=2\n",
      "[HOAG] theta now: [-4.835166   -0.03913266]\n",
      "\n",
      "[HOAG] Sample 584: label=7\n",
      "[HOAG] theta now: [-4.835487   -0.03877301]\n",
      "\n",
      "[HOAG] Sample 585: label=1\n",
      "[HOAG] theta now: [-4.8380976  -0.03628282]\n",
      "\n",
      "[HOAG] Sample 586: label=2\n",
      "[HOAG] theta now: [-4.8388567 -0.0355947]\n",
      "\n",
      "[HOAG] Sample 587: label=2\n",
      "[HOAG] theta now: [-4.838991   -0.03520936]\n",
      "\n",
      "[HOAG] Sample 588: label=3\n",
      "[HOAG] theta now: [-4.83941    -0.03482223]\n",
      "\n",
      "[HOAG] Sample 589: label=9\n",
      "[HOAG] theta now: [-4.8435097 -0.0308837]\n",
      "\n",
      "[HOAG] Sample 590: label=9\n",
      "[HOAG] theta now: [-4.8465905  -0.02785387]\n",
      "\n",
      "[HOAG] Sample 591: label=4\n",
      "[HOAG] theta now: [-4.8471875  -0.02723149]\n",
      "\n",
      "[HOAG] Sample 592: label=5\n",
      "[HOAG] theta now: [-4.849541   -0.02504464]\n",
      "\n",
      "[HOAG] Sample 593: label=1\n",
      "[HOAG] theta now: [-4.85328   -0.0213492]\n",
      "\n",
      "[HOAG] Sample 594: label=3\n",
      "[HOAG] theta now: [-4.854842  -0.0201622]\n",
      "\n",
      "[HOAG] Sample 595: label=9\n",
      "[HOAG] theta now: [-4.85744   -0.0176113]\n",
      "\n",
      "[HOAG] Sample 596: label=2\n",
      "[HOAG] theta now: [-4.8552513  -0.01952472]\n",
      "\n",
      "[HOAG] Sample 597: label=6\n",
      "[HOAG] theta now: [-4.8559074 -0.0188457]\n",
      "\n",
      "[HOAG] Sample 598: label=3\n",
      "[HOAG] theta now: [-4.8569717  -0.01780957]\n",
      "\n",
      "[HOAG] Sample 599: label=5\n",
      "[HOAG] theta now: [-4.856241   -0.01851331]\n",
      "\n",
      "[HOAG] Sample 600: label=4\n",
      "[HOAG] theta now: [-4.856836   -0.01775691]\n",
      "[HOAG] quick eval after sample 600\n",
      "\n",
      "--- Evaluating Model on Test Set (Iterative Deblurring) ---\n",
      "[eval] sample 0 iter 0 grad_norm=6.4008e-01, inner_loss=6.6475e+00\n",
      "[eval] sample 0 iter 10 grad_norm=6.1042e-01, inner_loss=6.6083e+00\n",
      "[eval] sample 0 iter 20 grad_norm=5.8261e-01, inner_loss=6.5727e+00\n",
      "[eval] sample 0 iter 30 grad_norm=5.5653e-01, inner_loss=6.5401e+00\n",
      "[eval] sample 0 iter 40 grad_norm=5.3206e-01, inner_loss=6.5105e+00\n",
      "[eval] sample 1 iter 0 grad_norm=7.8747e-01, inner_loss=7.0289e+00\n",
      "[eval] sample 1 iter 10 grad_norm=7.5148e-01, inner_loss=6.9696e+00\n",
      "[eval] sample 1 iter 20 grad_norm=7.1782e-01, inner_loss=6.9155e+00\n",
      "[eval] sample 1 iter 30 grad_norm=6.8631e-01, inner_loss=6.8661e+00\n",
      "[eval] sample 1 iter 40 grad_norm=6.5679e-01, inner_loss=6.8210e+00\n",
      "[eval] sample 2 iter 0 grad_norm=4.5203e-01, inner_loss=6.3346e+00\n",
      "[eval] sample 2 iter 10 grad_norm=4.3167e-01, inner_loss=6.3150e+00\n",
      "[eval] sample 2 iter 20 grad_norm=4.1258e-01, inner_loss=6.2972e+00\n",
      "[eval] sample 2 iter 30 grad_norm=3.9467e-01, inner_loss=6.2809e+00\n",
      "[eval] sample 2 iter 40 grad_norm=3.7786e-01, inner_loss=6.2659e+00\n",
      "[eval] sample 3 iter 0 grad_norm=9.6634e-01, inner_loss=7.2563e+00\n",
      "[eval] sample 3 iter 10 grad_norm=9.2102e-01, inner_loss=7.1671e+00\n",
      "[eval] sample 3 iter 20 grad_norm=8.7829e-01, inner_loss=7.0860e+00\n",
      "[eval] sample 3 iter 30 grad_norm=8.3798e-01, inner_loss=7.0122e+00\n",
      "[eval] sample 3 iter 40 grad_norm=7.9992e-01, inner_loss=6.9450e+00\n",
      "[eval] sample 4 iter 0 grad_norm=6.2173e-01, inner_loss=6.6326e+00\n",
      "[eval] sample 4 iter 10 grad_norm=5.9475e-01, inner_loss=6.5955e+00\n",
      "[eval] sample 4 iter 20 grad_norm=5.6931e-01, inner_loss=6.5616e+00\n",
      "[eval] sample 4 iter 30 grad_norm=5.4532e-01, inner_loss=6.5304e+00\n",
      "[eval] sample 4 iter 40 grad_norm=5.2266e-01, inner_loss=6.5019e+00\n",
      "[eval] sample 5 iter 0 grad_norm=6.0057e-01, inner_loss=6.5462e+00\n",
      "[eval] sample 5 iter 10 grad_norm=5.7243e-01, inner_loss=6.5118e+00\n",
      "[eval] sample 5 iter 20 grad_norm=5.4607e-01, inner_loss=6.4804e+00\n",
      "[eval] sample 5 iter 30 grad_norm=5.2136e-01, inner_loss=6.4519e+00\n",
      "[eval] sample 5 iter 40 grad_norm=4.9819e-01, inner_loss=6.4258e+00\n",
      "[eval] sample 6 iter 0 grad_norm=6.4734e-01, inner_loss=6.6777e+00\n",
      "[eval] sample 6 iter 10 grad_norm=6.1859e-01, inner_loss=6.6375e+00\n",
      "[eval] sample 6 iter 20 grad_norm=5.9155e-01, inner_loss=6.6008e+00\n",
      "[eval] sample 6 iter 30 grad_norm=5.6611e-01, inner_loss=6.5673e+00\n",
      "[eval] sample 6 iter 40 grad_norm=5.4216e-01, inner_loss=6.5365e+00\n",
      "[eval] sample 7 iter 0 grad_norm=5.9369e-01, inner_loss=6.5883e+00\n",
      "[eval] sample 7 iter 10 grad_norm=5.6383e-01, inner_loss=6.5547e+00\n",
      "[eval] sample 7 iter 20 grad_norm=5.3615e-01, inner_loss=6.5244e+00\n",
      "[eval] sample 7 iter 30 grad_norm=5.1048e-01, inner_loss=6.4970e+00\n",
      "[eval] sample 7 iter 40 grad_norm=4.8667e-01, inner_loss=6.4721e+00\n",
      "[eval] sample 8 iter 0 grad_norm=8.7521e-01, inner_loss=7.0076e+00\n",
      "[eval] sample 8 iter 10 grad_norm=8.3089e-01, inner_loss=6.9347e+00\n",
      "[eval] sample 8 iter 20 grad_norm=7.8931e-01, inner_loss=6.8689e+00\n",
      "[eval] sample 8 iter 30 grad_norm=7.5028e-01, inner_loss=6.8095e+00\n",
      "[eval] sample 8 iter 40 grad_norm=7.1363e-01, inner_loss=6.7558e+00\n",
      "[eval] sample 9 iter 0 grad_norm=7.5219e-01, inner_loss=6.8834e+00\n",
      "[eval] sample 9 iter 10 grad_norm=7.1654e-01, inner_loss=6.8293e+00\n",
      "[eval] sample 9 iter 20 grad_norm=6.8327e-01, inner_loss=6.7802e+00\n",
      "[eval] sample 9 iter 30 grad_norm=6.5220e-01, inner_loss=6.7356e+00\n",
      "[eval] sample 9 iter 40 grad_norm=6.2317e-01, inner_loss=6.6948e+00\n",
      "[eval] sample 10 iter 0 grad_norm=8.2355e-01, inner_loss=7.0881e+00\n",
      "[eval] sample 10 iter 10 grad_norm=7.8628e-01, inner_loss=7.0231e+00\n",
      "[eval] sample 10 iter 20 grad_norm=7.5125e-01, inner_loss=6.9639e+00\n",
      "[eval] sample 10 iter 30 grad_norm=7.1830e-01, inner_loss=6.9098e+00\n",
      "[eval] sample 10 iter 40 grad_norm=6.8730e-01, inner_loss=6.8603e+00\n",
      "[eval] sample 11 iter 0 grad_norm=6.1442e-01, inner_loss=6.7712e+00\n",
      "[eval] sample 11 iter 10 grad_norm=5.9016e-01, inner_loss=6.7348e+00\n",
      "[eval] sample 11 iter 20 grad_norm=5.6740e-01, inner_loss=6.7013e+00\n",
      "[eval] sample 11 iter 30 grad_norm=5.4604e-01, inner_loss=6.6702e+00\n",
      "[eval] sample 11 iter 40 grad_norm=5.2596e-01, inner_loss=6.6415e+00\n",
      "[eval] sample 12 iter 0 grad_norm=6.2234e-01, inner_loss=6.7173e+00\n",
      "[eval] sample 12 iter 10 grad_norm=5.9619e-01, inner_loss=6.6802e+00\n",
      "[eval] sample 12 iter 20 grad_norm=5.7177e-01, inner_loss=6.6460e+00\n",
      "[eval] sample 12 iter 30 grad_norm=5.4894e-01, inner_loss=6.6145e+00\n",
      "[eval] sample 12 iter 40 grad_norm=5.2757e-01, inner_loss=6.5855e+00\n",
      "[eval] sample 13 iter 0 grad_norm=8.4060e-01, inner_loss=7.0763e+00\n",
      "[eval] sample 13 iter 10 grad_norm=8.0212e-01, inner_loss=7.0087e+00\n",
      "[eval] sample 13 iter 20 grad_norm=7.6586e-01, inner_loss=6.9471e+00\n",
      "[eval] sample 13 iter 30 grad_norm=7.3170e-01, inner_loss=6.8909e+00\n",
      "[eval] sample 13 iter 40 grad_norm=6.9949e-01, inner_loss=6.8396e+00\n",
      "[eval] sample 14 iter 0 grad_norm=6.7597e-01, inner_loss=6.6670e+00\n",
      "[eval] sample 14 iter 10 grad_norm=6.4379e-01, inner_loss=6.6234e+00\n",
      "[eval] sample 14 iter 20 grad_norm=6.1365e-01, inner_loss=6.5838e+00\n",
      "[eval] sample 14 iter 30 grad_norm=5.8540e-01, inner_loss=6.5478e+00\n",
      "[eval] sample 14 iter 40 grad_norm=5.5892e-01, inner_loss=6.5150e+00\n",
      "[eval] sample 15 iter 0 grad_norm=6.0800e-01, inner_loss=6.6687e+00\n",
      "[eval] sample 15 iter 10 grad_norm=5.8233e-01, inner_loss=6.6332e+00\n",
      "[eval] sample 15 iter 20 grad_norm=5.5821e-01, inner_loss=6.6006e+00\n",
      "[eval] sample 15 iter 30 grad_norm=5.3553e-01, inner_loss=6.5707e+00\n",
      "[eval] sample 15 iter 40 grad_norm=5.1419e-01, inner_loss=6.5431e+00\n",
      "[eval] sample 16 iter 0 grad_norm=6.8180e-01, inner_loss=6.7799e+00\n",
      "[eval] sample 16 iter 10 grad_norm=6.5340e-01, inner_loss=6.7352e+00\n",
      "[eval] sample 16 iter 20 grad_norm=6.2661e-01, inner_loss=6.6942e+00\n",
      "[eval] sample 16 iter 30 grad_norm=6.0131e-01, inner_loss=6.6564e+00\n",
      "[eval] sample 16 iter 40 grad_norm=5.7740e-01, inner_loss=6.6216e+00\n",
      "[eval] sample 17 iter 0 grad_norm=7.5499e-01, inner_loss=6.8713e+00\n",
      "[eval] sample 17 iter 10 grad_norm=7.1960e-01, inner_loss=6.8168e+00\n",
      "[eval] sample 17 iter 20 grad_norm=6.8643e-01, inner_loss=6.7673e+00\n",
      "[eval] sample 17 iter 30 grad_norm=6.5532e-01, inner_loss=6.7222e+00\n",
      "[eval] sample 17 iter 40 grad_norm=6.2614e-01, inner_loss=6.6811e+00\n",
      "[eval] sample 18 iter 0 grad_norm=7.9920e-01, inner_loss=7.1409e+00\n",
      "[eval] sample 18 iter 10 grad_norm=7.6628e-01, inner_loss=7.0795e+00\n",
      "[eval] sample 18 iter 20 grad_norm=7.3534e-01, inner_loss=7.0230e+00\n",
      "[eval] sample 18 iter 30 grad_norm=7.0623e-01, inner_loss=6.9710e+00\n",
      "[eval] sample 18 iter 40 grad_norm=6.7880e-01, inner_loss=6.9229e+00\n",
      "[eval] sample 19 iter 0 grad_norm=5.2977e-01, inner_loss=6.4901e+00\n",
      "[eval] sample 19 iter 10 grad_norm=5.0772e-01, inner_loss=6.4631e+00\n",
      "[eval] sample 19 iter 20 grad_norm=4.8702e-01, inner_loss=6.4383e+00\n",
      "[eval] sample 19 iter 30 grad_norm=4.6758e-01, inner_loss=6.4155e+00\n",
      "[eval] sample 19 iter 40 grad_norm=4.4930e-01, inner_loss=6.3945e+00\n",
      "[eval] sample 20 iter 0 grad_norm=7.2217e-01, inner_loss=6.7171e+00\n",
      "[eval] sample 20 iter 10 grad_norm=6.8430e-01, inner_loss=6.6675e+00\n",
      "[eval] sample 20 iter 20 grad_norm=6.4909e-01, inner_loss=6.6230e+00\n",
      "[eval] sample 20 iter 30 grad_norm=6.1635e-01, inner_loss=6.5829e+00\n",
      "[eval] sample 20 iter 40 grad_norm=5.8589e-01, inner_loss=6.5466e+00\n",
      "[eval] sample 21 iter 0 grad_norm=6.3254e-01, inner_loss=6.7374e+00\n",
      "[eval] sample 21 iter 10 grad_norm=6.0608e-01, inner_loss=6.6989e+00\n",
      "[eval] sample 21 iter 20 grad_norm=5.8133e-01, inner_loss=6.6636e+00\n",
      "[eval] sample 21 iter 30 grad_norm=5.5817e-01, inner_loss=6.6311e+00\n",
      "[eval] sample 21 iter 40 grad_norm=5.3647e-01, inner_loss=6.6011e+00\n",
      "[eval] sample 22 iter 0 grad_norm=6.1716e-01, inner_loss=6.6943e+00\n",
      "[eval] sample 22 iter 10 grad_norm=5.9041e-01, inner_loss=6.6578e+00\n",
      "[eval] sample 22 iter 20 grad_norm=5.6536e-01, inner_loss=6.6244e+00\n",
      "[eval] sample 22 iter 30 grad_norm=5.4189e-01, inner_loss=6.5937e+00\n",
      "[eval] sample 22 iter 40 grad_norm=5.1988e-01, inner_loss=6.5654e+00\n",
      "[eval] sample 23 iter 0 grad_norm=6.1381e-01, inner_loss=6.6742e+00\n",
      "[eval] sample 23 iter 10 grad_norm=5.8791e-01, inner_loss=6.6380e+00\n",
      "[eval] sample 23 iter 20 grad_norm=5.6359e-01, inner_loss=6.6048e+00\n",
      "[eval] sample 23 iter 30 grad_norm=5.4074e-01, inner_loss=6.5742e+00\n",
      "[eval] sample 23 iter 40 grad_norm=5.1924e-01, inner_loss=6.5461e+00\n",
      "[eval] sample 24 iter 0 grad_norm=4.2239e-01, inner_loss=6.3297e+00\n",
      "[eval] sample 24 iter 10 grad_norm=4.0546e-01, inner_loss=6.3125e+00\n",
      "[eval] sample 24 iter 20 grad_norm=3.8955e-01, inner_loss=6.2967e+00\n",
      "[eval] sample 24 iter 30 grad_norm=3.7457e-01, inner_loss=6.2821e+00\n",
      "[eval] sample 24 iter 40 grad_norm=3.6046e-01, inner_loss=6.2686e+00\n",
      "[eval] sample 25 iter 0 grad_norm=1.1081e+00, inner_loss=7.4858e+00\n",
      "[eval] sample 25 iter 10 grad_norm=1.0528e+00, inner_loss=7.3688e+00\n",
      "[eval] sample 25 iter 20 grad_norm=1.0007e+00, inner_loss=7.2632e+00\n",
      "[eval] sample 25 iter 30 grad_norm=9.5173e-01, inner_loss=7.1677e+00\n",
      "[eval] sample 25 iter 40 grad_norm=9.0556e-01, inner_loss=7.0812e+00\n",
      "[eval] sample 26 iter 0 grad_norm=5.6826e-01, inner_loss=6.5499e+00\n",
      "[eval] sample 26 iter 10 grad_norm=5.4291e-01, inner_loss=6.5190e+00\n",
      "[eval] sample 26 iter 20 grad_norm=5.1912e-01, inner_loss=6.4908e+00\n",
      "[eval] sample 26 iter 30 grad_norm=4.9679e-01, inner_loss=6.4649e+00\n",
      "[eval] sample 26 iter 40 grad_norm=4.7581e-01, inner_loss=6.4412e+00\n",
      "[eval] sample 27 iter 0 grad_norm=6.6943e-01, inner_loss=6.7735e+00\n",
      "[eval] sample 27 iter 10 grad_norm=6.4175e-01, inner_loss=6.7304e+00\n",
      "[eval] sample 27 iter 20 grad_norm=6.1572e-01, inner_loss=6.6908e+00\n",
      "[eval] sample 27 iter 30 grad_norm=5.9120e-01, inner_loss=6.6544e+00\n",
      "[eval] sample 27 iter 40 grad_norm=5.6810e-01, inner_loss=6.6207e+00\n",
      "[eval] sample 28 iter 0 grad_norm=9.2034e-01, inner_loss=7.2206e+00\n",
      "[eval] sample 28 iter 10 grad_norm=8.7761e-01, inner_loss=7.1396e+00\n",
      "[eval] sample 28 iter 20 grad_norm=8.3742e-01, inner_loss=7.0659e+00\n",
      "[eval] sample 28 iter 30 grad_norm=7.9960e-01, inner_loss=6.9988e+00\n",
      "[eval] sample 28 iter 40 grad_norm=7.6399e-01, inner_loss=6.9376e+00\n",
      "[eval] sample 29 iter 0 grad_norm=5.6150e-01, inner_loss=6.4929e+00\n",
      "[eval] sample 29 iter 10 grad_norm=5.3563e-01, inner_loss=6.4627e+00\n",
      "[eval] sample 29 iter 20 grad_norm=5.1139e-01, inner_loss=6.4353e+00\n",
      "[eval] sample 29 iter 30 grad_norm=4.8866e-01, inner_loss=6.4102e+00\n",
      "[eval] sample 29 iter 40 grad_norm=4.6734e-01, inner_loss=6.3873e+00\n",
      "[eval] sample 30 iter 0 grad_norm=7.1297e-01, inner_loss=6.8016e+00\n",
      "[eval] sample 30 iter 10 grad_norm=6.7834e-01, inner_loss=6.7531e+00\n",
      "[eval] sample 30 iter 20 grad_norm=6.4605e-01, inner_loss=6.7091e+00\n",
      "[eval] sample 30 iter 30 grad_norm=6.1593e-01, inner_loss=6.6692e+00\n",
      "[eval] sample 30 iter 40 grad_norm=5.8782e-01, inner_loss=6.6329e+00\n",
      "[eval] sample 31 iter 0 grad_norm=4.9440e-01, inner_loss=6.3967e+00\n",
      "[eval] sample 31 iter 10 grad_norm=4.7192e-01, inner_loss=6.3733e+00\n",
      "[eval] sample 31 iter 20 grad_norm=4.5085e-01, inner_loss=6.3519e+00\n",
      "[eval] sample 31 iter 30 grad_norm=4.3109e-01, inner_loss=6.3325e+00\n",
      "[eval] sample 31 iter 40 grad_norm=4.1255e-01, inner_loss=6.3146e+00\n",
      "[eval] sample 32 iter 0 grad_norm=7.3761e-01, inner_loss=6.9277e+00\n",
      "[eval] sample 32 iter 10 grad_norm=7.0585e-01, inner_loss=6.8755e+00\n",
      "[eval] sample 32 iter 20 grad_norm=6.7610e-01, inner_loss=6.8276e+00\n",
      "[eval] sample 32 iter 30 grad_norm=6.4820e-01, inner_loss=6.7837e+00\n",
      "[eval] sample 32 iter 40 grad_norm=6.2200e-01, inner_loss=6.7433e+00\n",
      "[eval] sample 33 iter 0 grad_norm=7.2182e-01, inner_loss=6.8306e+00\n",
      "[eval] sample 33 iter 10 grad_norm=6.8805e-01, inner_loss=6.7808e+00\n",
      "[eval] sample 33 iter 20 grad_norm=6.5639e-01, inner_loss=6.7355e+00\n",
      "[eval] sample 33 iter 30 grad_norm=6.2669e-01, inner_loss=6.6942e+00\n",
      "[eval] sample 33 iter 40 grad_norm=5.9881e-01, inner_loss=6.6566e+00\n",
      "[eval] sample 34 iter 0 grad_norm=8.0535e-01, inner_loss=6.9395e+00\n",
      "[eval] sample 34 iter 10 grad_norm=7.6692e-01, inner_loss=6.8776e+00\n",
      "[eval] sample 34 iter 20 grad_norm=7.3092e-01, inner_loss=6.8214e+00\n",
      "[eval] sample 34 iter 30 grad_norm=6.9717e-01, inner_loss=6.7703e+00\n",
      "[eval] sample 34 iter 40 grad_norm=6.6552e-01, inner_loss=6.7238e+00\n",
      "[eval] sample 35 iter 0 grad_norm=8.0631e-01, inner_loss=7.0143e+00\n",
      "[eval] sample 35 iter 10 grad_norm=7.6950e-01, inner_loss=6.9521e+00\n",
      "[eval] sample 35 iter 20 grad_norm=7.3488e-01, inner_loss=6.8954e+00\n",
      "[eval] sample 35 iter 30 grad_norm=7.0231e-01, inner_loss=6.8437e+00\n",
      "[eval] sample 35 iter 40 grad_norm=6.7164e-01, inner_loss=6.7964e+00\n",
      "[eval] sample 36 iter 0 grad_norm=6.9301e-01, inner_loss=6.7301e+00\n",
      "[eval] sample 36 iter 10 grad_norm=6.5977e-01, inner_loss=6.6842e+00\n",
      "[eval] sample 36 iter 20 grad_norm=6.2859e-01, inner_loss=6.6427e+00\n",
      "[eval] sample 36 iter 30 grad_norm=5.9935e-01, inner_loss=6.6049e+00\n",
      "[eval] sample 36 iter 40 grad_norm=5.7190e-01, inner_loss=6.5705e+00\n",
      "[eval] sample 37 iter 0 grad_norm=6.0259e-01, inner_loss=6.5511e+00\n",
      "[eval] sample 37 iter 10 grad_norm=5.7435e-01, inner_loss=6.5164e+00\n",
      "[eval] sample 37 iter 20 grad_norm=5.4789e-01, inner_loss=6.4849e+00\n",
      "[eval] sample 37 iter 30 grad_norm=5.2310e-01, inner_loss=6.4561e+00\n",
      "[eval] sample 37 iter 40 grad_norm=4.9985e-01, inner_loss=6.4299e+00\n",
      "[eval] sample 38 iter 0 grad_norm=5.7016e-01, inner_loss=6.5451e+00\n",
      "[eval] sample 38 iter 10 grad_norm=5.4409e-01, inner_loss=6.5140e+00\n",
      "[eval] sample 38 iter 20 grad_norm=5.1970e-01, inner_loss=6.4857e+00\n",
      "[eval] sample 38 iter 30 grad_norm=4.9686e-01, inner_loss=6.4598e+00\n",
      "[eval] sample 38 iter 40 grad_norm=4.7545e-01, inner_loss=6.4361e+00\n",
      "[eval] sample 39 iter 0 grad_norm=7.2091e-01, inner_loss=6.6956e+00\n",
      "[eval] sample 39 iter 10 grad_norm=6.8531e-01, inner_loss=6.6461e+00\n",
      "[eval] sample 39 iter 20 grad_norm=6.5200e-01, inner_loss=6.6012e+00\n",
      "[eval] sample 39 iter 30 grad_norm=6.2080e-01, inner_loss=6.5607e+00\n",
      "[eval] sample 39 iter 40 grad_norm=5.9158e-01, inner_loss=6.5238e+00\n",
      "[eval] sample 40 iter 0 grad_norm=3.3069e-01, inner_loss=6.1867e+00\n",
      "[eval] sample 40 iter 10 grad_norm=3.1617e-01, inner_loss=6.1763e+00\n",
      "[eval] sample 40 iter 20 grad_norm=3.0255e-01, inner_loss=6.1667e+00\n",
      "[eval] sample 40 iter 30 grad_norm=2.8977e-01, inner_loss=6.1579e+00\n",
      "[eval] sample 40 iter 40 grad_norm=2.7777e-01, inner_loss=6.1498e+00\n",
      "[eval] sample 41 iter 0 grad_norm=5.8388e-01, inner_loss=6.5420e+00\n",
      "[eval] sample 41 iter 10 grad_norm=5.5764e-01, inner_loss=6.5094e+00\n",
      "[eval] sample 41 iter 20 grad_norm=5.3299e-01, inner_loss=6.4796e+00\n",
      "[eval] sample 41 iter 30 grad_norm=5.0983e-01, inner_loss=6.4523e+00\n",
      "[eval] sample 41 iter 40 grad_norm=4.8805e-01, inner_loss=6.4274e+00\n",
      "[eval] sample 42 iter 0 grad_norm=7.1330e-01, inner_loss=6.7989e+00\n",
      "[eval] sample 42 iter 10 grad_norm=6.8153e-01, inner_loss=6.7502e+00\n",
      "[eval] sample 42 iter 20 grad_norm=6.5175e-01, inner_loss=6.7056e+00\n",
      "[eval] sample 42 iter 30 grad_norm=6.2382e-01, inner_loss=6.6649e+00\n",
      "[eval] sample 42 iter 40 grad_norm=5.9759e-01, inner_loss=6.6275e+00\n",
      "[eval] sample 43 iter 0 grad_norm=5.4651e-01, inner_loss=6.4934e+00\n",
      "[eval] sample 43 iter 10 grad_norm=5.2209e-01, inner_loss=6.4647e+00\n",
      "[eval] sample 43 iter 20 grad_norm=4.9918e-01, inner_loss=6.4386e+00\n",
      "[eval] sample 43 iter 30 grad_norm=4.7766e-01, inner_loss=6.4147e+00\n",
      "[eval] sample 43 iter 40 grad_norm=4.5744e-01, inner_loss=6.3928e+00\n",
      "[eval] sample 44 iter 0 grad_norm=6.4742e-01, inner_loss=6.6716e+00\n",
      "[eval] sample 44 iter 10 grad_norm=6.1768e-01, inner_loss=6.6315e+00\n",
      "[eval] sample 44 iter 20 grad_norm=5.8987e-01, inner_loss=6.5950e+00\n",
      "[eval] sample 44 iter 30 grad_norm=5.6386e-01, inner_loss=6.5617e+00\n",
      "[eval] sample 44 iter 40 grad_norm=5.3951e-01, inner_loss=6.5312e+00\n",
      "[eval] sample 45 iter 0 grad_norm=5.6388e-01, inner_loss=6.6068e+00\n",
      "[eval] sample 45 iter 10 grad_norm=5.4222e-01, inner_loss=6.5761e+00\n",
      "[eval] sample 45 iter 20 grad_norm=5.2184e-01, inner_loss=6.5478e+00\n",
      "[eval] sample 45 iter 30 grad_norm=5.0262e-01, inner_loss=6.5215e+00\n",
      "[eval] sample 45 iter 40 grad_norm=4.8449e-01, inner_loss=6.4971e+00\n",
      "[eval] sample 46 iter 0 grad_norm=7.3661e-01, inner_loss=6.7238e+00\n",
      "[eval] sample 46 iter 10 grad_norm=7.0016e-01, inner_loss=6.6721e+00\n",
      "[eval] sample 46 iter 20 grad_norm=6.6604e-01, inner_loss=6.6253e+00\n",
      "[eval] sample 46 iter 30 grad_norm=6.3410e-01, inner_loss=6.5830e+00\n",
      "[eval] sample 46 iter 40 grad_norm=6.0416e-01, inner_loss=6.5446e+00\n",
      "[eval] sample 47 iter 0 grad_norm=4.5627e-01, inner_loss=6.3550e+00\n",
      "[eval] sample 47 iter 10 grad_norm=4.3407e-01, inner_loss=6.3351e+00\n",
      "[eval] sample 47 iter 20 grad_norm=4.1336e-01, inner_loss=6.3171e+00\n",
      "[eval] sample 47 iter 30 grad_norm=3.9405e-01, inner_loss=6.3008e+00\n",
      "[eval] sample 47 iter 40 grad_norm=3.7603e-01, inner_loss=6.2859e+00\n",
      "[eval] sample 48 iter 0 grad_norm=9.0796e-01, inner_loss=7.1044e+00\n",
      "[eval] sample 48 iter 10 grad_norm=8.6359e-01, inner_loss=7.0258e+00\n",
      "[eval] sample 48 iter 20 grad_norm=8.2210e-01, inner_loss=6.9546e+00\n",
      "[eval] sample 48 iter 30 grad_norm=7.8330e-01, inner_loss=6.8900e+00\n",
      "[eval] sample 48 iter 40 grad_norm=7.4697e-01, inner_loss=6.8313e+00\n",
      "[eval] sample 49 iter 0 grad_norm=7.2398e-01, inner_loss=6.8144e+00\n",
      "[eval] sample 49 iter 10 grad_norm=6.9083e-01, inner_loss=6.7643e+00\n",
      "[eval] sample 49 iter 20 grad_norm=6.5966e-01, inner_loss=6.7186e+00\n",
      "[eval] sample 49 iter 30 grad_norm=6.3032e-01, inner_loss=6.6769e+00\n",
      "[eval] sample 49 iter 40 grad_norm=6.0270e-01, inner_loss=6.6388e+00\n",
      "[eval] sample 50 iter 0 grad_norm=6.0972e-01, inner_loss=6.6997e+00\n",
      "[eval] sample 50 iter 10 grad_norm=5.8488e-01, inner_loss=6.6640e+00\n",
      "[eval] sample 50 iter 20 grad_norm=5.6162e-01, inner_loss=6.6310e+00\n",
      "[eval] sample 50 iter 30 grad_norm=5.3983e-01, inner_loss=6.6007e+00\n",
      "[eval] sample 50 iter 40 grad_norm=5.1937e-01, inner_loss=6.5726e+00\n",
      "[eval] sample 51 iter 0 grad_norm=8.4165e-01, inner_loss=7.2838e+00\n",
      "[eval] sample 51 iter 10 grad_norm=8.0727e-01, inner_loss=7.2157e+00\n",
      "[eval] sample 51 iter 20 grad_norm=7.7501e-01, inner_loss=7.1530e+00\n",
      "[eval] sample 51 iter 30 grad_norm=7.4470e-01, inner_loss=7.0952e+00\n",
      "[eval] sample 51 iter 40 grad_norm=7.1619e-01, inner_loss=7.0417e+00\n",
      "[eval] sample 52 iter 0 grad_norm=6.4004e-01, inner_loss=6.7593e+00\n",
      "[eval] sample 52 iter 10 grad_norm=6.1396e-01, inner_loss=6.7199e+00\n",
      "[eval] sample 52 iter 20 grad_norm=5.8944e-01, inner_loss=6.6837e+00\n",
      "[eval] sample 52 iter 30 grad_norm=5.6637e-01, inner_loss=6.6502e+00\n",
      "[eval] sample 52 iter 40 grad_norm=5.4463e-01, inner_loss=6.6193e+00\n",
      "[eval] sample 53 iter 0 grad_norm=4.8716e-01, inner_loss=6.4398e+00\n",
      "[eval] sample 53 iter 10 grad_norm=4.6738e-01, inner_loss=6.4170e+00\n",
      "[eval] sample 53 iter 20 grad_norm=4.4874e-01, inner_loss=6.3959e+00\n",
      "[eval] sample 53 iter 30 grad_norm=4.3115e-01, inner_loss=6.3765e+00\n",
      "[eval] sample 53 iter 40 grad_norm=4.1454e-01, inner_loss=6.3586e+00\n",
      "[eval] sample 54 iter 0 grad_norm=8.0175e-01, inner_loss=7.1661e+00\n",
      "[eval] sample 54 iter 10 grad_norm=7.7007e-01, inner_loss=7.1042e+00\n",
      "[eval] sample 54 iter 20 grad_norm=7.4022e-01, inner_loss=7.0471e+00\n",
      "[eval] sample 54 iter 30 grad_norm=7.1207e-01, inner_loss=6.9943e+00\n",
      "[eval] sample 54 iter 40 grad_norm=6.8549e-01, inner_loss=6.9454e+00\n",
      "[eval] sample 55 iter 0 grad_norm=7.7862e-01, inner_loss=7.0231e+00\n",
      "[eval] sample 55 iter 10 grad_norm=7.4674e-01, inner_loss=6.9649e+00\n",
      "[eval] sample 55 iter 20 grad_norm=7.1669e-01, inner_loss=6.9112e+00\n",
      "[eval] sample 55 iter 30 grad_norm=6.8833e-01, inner_loss=6.8618e+00\n",
      "[eval] sample 55 iter 40 grad_norm=6.6156e-01, inner_loss=6.8162e+00\n",
      "[eval] sample 56 iter 0 grad_norm=8.2478e-01, inner_loss=7.0145e+00\n",
      "[eval] sample 56 iter 10 grad_norm=7.8670e-01, inner_loss=6.9494e+00\n",
      "[eval] sample 56 iter 20 grad_norm=7.5088e-01, inner_loss=6.8902e+00\n",
      "[eval] sample 56 iter 30 grad_norm=7.1717e-01, inner_loss=6.8362e+00\n",
      "[eval] sample 56 iter 40 grad_norm=6.8542e-01, inner_loss=6.7869e+00\n",
      "[eval] sample 57 iter 0 grad_norm=4.7366e-01, inner_loss=6.3699e+00\n",
      "[eval] sample 57 iter 10 grad_norm=4.5229e-01, inner_loss=6.3485e+00\n",
      "[eval] sample 57 iter 20 grad_norm=4.3226e-01, inner_loss=6.3289e+00\n",
      "[eval] sample 57 iter 30 grad_norm=4.1347e-01, inner_loss=6.3109e+00\n",
      "[eval] sample 57 iter 40 grad_norm=3.9583e-01, inner_loss=6.2945e+00\n",
      "[eval] sample 58 iter 0 grad_norm=6.8730e-01, inner_loss=6.8380e+00\n",
      "[eval] sample 58 iter 10 grad_norm=6.5730e-01, inner_loss=6.7927e+00\n",
      "[eval] sample 58 iter 20 grad_norm=6.2921e-01, inner_loss=6.7513e+00\n",
      "[eval] sample 58 iter 30 grad_norm=6.0290e-01, inner_loss=6.7133e+00\n",
      "[eval] sample 58 iter 40 grad_norm=5.7822e-01, inner_loss=6.6783e+00\n",
      "[eval] sample 59 iter 0 grad_norm=4.4436e-01, inner_loss=6.3459e+00\n",
      "[eval] sample 59 iter 10 grad_norm=4.2465e-01, inner_loss=6.3270e+00\n",
      "[eval] sample 59 iter 20 grad_norm=4.0620e-01, inner_loss=6.3097e+00\n",
      "[eval] sample 59 iter 30 grad_norm=3.8891e-01, inner_loss=6.2939e+00\n",
      "[eval] sample 59 iter 40 grad_norm=3.7270e-01, inner_loss=6.2793e+00\n",
      "[eval] sample 60 iter 0 grad_norm=8.4662e-01, inner_loss=7.0246e+00\n",
      "[eval] sample 60 iter 10 grad_norm=8.0570e-01, inner_loss=6.9562e+00\n",
      "[eval] sample 60 iter 20 grad_norm=7.6738e-01, inner_loss=6.8942e+00\n",
      "[eval] sample 60 iter 30 grad_norm=7.3149e-01, inner_loss=6.8379e+00\n",
      "[eval] sample 60 iter 40 grad_norm=6.9785e-01, inner_loss=6.7867e+00\n",
      "[eval] sample 61 iter 0 grad_norm=7.4944e-01, inner_loss=6.9178e+00\n",
      "[eval] sample 61 iter 10 grad_norm=7.1552e-01, inner_loss=6.8640e+00\n",
      "[eval] sample 61 iter 20 grad_norm=6.8378e-01, inner_loss=6.8150e+00\n",
      "[eval] sample 61 iter 30 grad_norm=6.5406e-01, inner_loss=6.7701e+00\n",
      "[eval] sample 61 iter 40 grad_norm=6.2621e-01, inner_loss=6.7291e+00\n",
      "[eval] sample 62 iter 0 grad_norm=4.3772e-01, inner_loss=6.3582e+00\n",
      "[eval] sample 62 iter 10 grad_norm=4.1910e-01, inner_loss=6.3398e+00\n",
      "[eval] sample 62 iter 20 grad_norm=4.0167e-01, inner_loss=6.3229e+00\n",
      "[eval] sample 62 iter 30 grad_norm=3.8533e-01, inner_loss=6.3074e+00\n",
      "[eval] sample 62 iter 40 grad_norm=3.7000e-01, inner_loss=6.2931e+00\n",
      "[eval] sample 63 iter 0 grad_norm=6.4880e-01, inner_loss=6.7585e+00\n",
      "[eval] sample 63 iter 10 grad_norm=6.2193e-01, inner_loss=6.7181e+00\n",
      "[eval] sample 63 iter 20 grad_norm=5.9676e-01, inner_loss=6.6809e+00\n",
      "[eval] sample 63 iter 30 grad_norm=5.7317e-01, inner_loss=6.6466e+00\n",
      "[eval] sample 63 iter 40 grad_norm=5.5102e-01, inner_loss=6.6150e+00\n",
      "[eval] sample 64 iter 0 grad_norm=7.7109e-01, inner_loss=7.0202e+00\n",
      "[eval] sample 64 iter 10 grad_norm=7.3714e-01, inner_loss=6.9632e+00\n",
      "[eval] sample 64 iter 20 grad_norm=7.0538e-01, inner_loss=6.9111e+00\n",
      "[eval] sample 64 iter 30 grad_norm=6.7564e-01, inner_loss=6.8633e+00\n",
      "[eval] sample 64 iter 40 grad_norm=6.4777e-01, inner_loss=6.8194e+00\n",
      "[eval] sample 65 iter 0 grad_norm=4.3007e-01, inner_loss=6.3286e+00\n",
      "[eval] sample 65 iter 10 grad_norm=4.0896e-01, inner_loss=6.3110e+00\n",
      "[eval] sample 65 iter 20 grad_norm=3.8937e-01, inner_loss=6.2950e+00\n",
      "[eval] sample 65 iter 30 grad_norm=3.7118e-01, inner_loss=6.2805e+00\n",
      "[eval] sample 65 iter 40 grad_norm=3.5427e-01, inner_loss=6.2673e+00\n",
      "[eval] sample 66 iter 0 grad_norm=5.9246e-01, inner_loss=6.6624e+00\n",
      "[eval] sample 66 iter 10 grad_norm=5.6722e-01, inner_loss=6.6287e+00\n",
      "[eval] sample 66 iter 20 grad_norm=5.4355e-01, inner_loss=6.5978e+00\n",
      "[eval] sample 66 iter 30 grad_norm=5.2134e-01, inner_loss=6.5694e+00\n",
      "[eval] sample 66 iter 40 grad_norm=5.0049e-01, inner_loss=6.5432e+00\n",
      "[eval] sample 67 iter 0 grad_norm=6.7649e-01, inner_loss=6.7316e+00\n",
      "[eval] sample 67 iter 10 grad_norm=6.4575e-01, inner_loss=6.6878e+00\n",
      "[eval] sample 67 iter 20 grad_norm=6.1689e-01, inner_loss=6.6479e+00\n",
      "[eval] sample 67 iter 30 grad_norm=5.8977e-01, inner_loss=6.6114e+00\n",
      "[eval] sample 67 iter 40 grad_norm=5.6429e-01, inner_loss=6.5781e+00\n",
      "[eval] sample 68 iter 0 grad_norm=8.1688e-01, inner_loss=7.0689e+00\n",
      "[eval] sample 68 iter 10 grad_norm=7.7912e-01, inner_loss=7.0051e+00\n",
      "[eval] sample 68 iter 20 grad_norm=7.4403e-01, inner_loss=6.9470e+00\n",
      "[eval] sample 68 iter 30 grad_norm=7.1138e-01, inner_loss=6.8939e+00\n",
      "[eval] sample 68 iter 40 grad_norm=6.8098e-01, inner_loss=6.8454e+00\n",
      "[eval] sample 69 iter 0 grad_norm=8.1435e-01, inner_loss=7.0570e+00\n",
      "[eval] sample 69 iter 10 grad_norm=7.7708e-01, inner_loss=6.9935e+00\n",
      "[eval] sample 69 iter 20 grad_norm=7.4203e-01, inner_loss=6.9357e+00\n",
      "[eval] sample 69 iter 30 grad_norm=7.0905e-01, inner_loss=6.8830e+00\n",
      "[eval] sample 69 iter 40 grad_norm=6.7801e-01, inner_loss=6.8348e+00\n",
      "[eval] sample 70 iter 0 grad_norm=7.2958e-01, inner_loss=6.8425e+00\n",
      "[eval] sample 70 iter 10 grad_norm=6.9592e-01, inner_loss=6.7916e+00\n",
      "[eval] sample 70 iter 20 grad_norm=6.6437e-01, inner_loss=6.7452e+00\n",
      "[eval] sample 70 iter 30 grad_norm=6.3478e-01, inner_loss=6.7029e+00\n",
      "[eval] sample 70 iter 40 grad_norm=6.0701e-01, inner_loss=6.6643e+00\n",
      "[eval] sample 71 iter 0 grad_norm=1.0473e+00, inner_loss=7.5353e+00\n",
      "[eval] sample 71 iter 10 grad_norm=9.9708e-01, inner_loss=7.4306e+00\n",
      "[eval] sample 71 iter 20 grad_norm=9.4984e-01, inner_loss=7.3356e+00\n",
      "[eval] sample 71 iter 30 grad_norm=9.0542e-01, inner_loss=7.2494e+00\n",
      "[eval] sample 71 iter 40 grad_norm=8.6365e-01, inner_loss=7.1710e+00\n",
      "[eval] sample 72 iter 0 grad_norm=7.3222e-01, inner_loss=6.8178e+00\n",
      "[eval] sample 72 iter 10 grad_norm=6.9652e-01, inner_loss=6.7666e+00\n",
      "[eval] sample 72 iter 20 grad_norm=6.6315e-01, inner_loss=6.7203e+00\n",
      "[eval] sample 72 iter 30 grad_norm=6.3193e-01, inner_loss=6.6783e+00\n",
      "[eval] sample 72 iter 40 grad_norm=6.0272e-01, inner_loss=6.6401e+00\n",
      "[eval] sample 73 iter 0 grad_norm=7.7673e-01, inner_loss=6.8353e+00\n",
      "[eval] sample 73 iter 10 grad_norm=7.3535e-01, inner_loss=6.7780e+00\n",
      "[eval] sample 73 iter 20 grad_norm=6.9690e-01, inner_loss=6.7266e+00\n",
      "[eval] sample 73 iter 30 grad_norm=6.6116e-01, inner_loss=6.6804e+00\n",
      "[eval] sample 73 iter 40 grad_norm=6.2793e-01, inner_loss=6.6388e+00\n",
      "[eval] sample 74 iter 0 grad_norm=6.3493e-01, inner_loss=6.5894e+00\n",
      "[eval] sample 74 iter 10 grad_norm=6.0466e-01, inner_loss=6.5509e+00\n",
      "[eval] sample 74 iter 20 grad_norm=5.7633e-01, inner_loss=6.5159e+00\n",
      "[eval] sample 74 iter 30 grad_norm=5.4979e-01, inner_loss=6.4842e+00\n",
      "[eval] sample 74 iter 40 grad_norm=5.2491e-01, inner_loss=6.4552e+00\n",
      "[eval] sample 75 iter 0 grad_norm=6.3127e-01, inner_loss=6.6920e+00\n",
      "[eval] sample 75 iter 10 grad_norm=6.0226e-01, inner_loss=6.6539e+00\n",
      "[eval] sample 75 iter 20 grad_norm=5.7524e-01, inner_loss=6.6192e+00\n",
      "[eval] sample 75 iter 30 grad_norm=5.5004e-01, inner_loss=6.5875e+00\n",
      "[eval] sample 75 iter 40 grad_norm=5.2654e-01, inner_loss=6.5584e+00\n",
      "[eval] sample 76 iter 0 grad_norm=5.5525e-01, inner_loss=6.5493e+00\n",
      "[eval] sample 76 iter 10 grad_norm=5.3181e-01, inner_loss=6.5197e+00\n",
      "[eval] sample 76 iter 20 grad_norm=5.0983e-01, inner_loss=6.4926e+00\n",
      "[eval] sample 76 iter 30 grad_norm=4.8919e-01, inner_loss=6.4676e+00\n",
      "[eval] sample 76 iter 40 grad_norm=4.6978e-01, inner_loss=6.4445e+00\n",
      "[eval] sample 77 iter 0 grad_norm=5.8539e-01, inner_loss=6.5703e+00\n",
      "[eval] sample 77 iter 10 grad_norm=5.6036e-01, inner_loss=6.5374e+00\n",
      "[eval] sample 77 iter 20 grad_norm=5.3672e-01, inner_loss=6.5073e+00\n",
      "[eval] sample 77 iter 30 grad_norm=5.1437e-01, inner_loss=6.4796e+00\n",
      "[eval] sample 77 iter 40 grad_norm=4.9325e-01, inner_loss=6.4542e+00\n",
      "[eval] sample 78 iter 0 grad_norm=6.0358e-01, inner_loss=6.5157e+00\n",
      "[eval] sample 78 iter 10 grad_norm=5.7116e-01, inner_loss=6.4811e+00\n",
      "[eval] sample 78 iter 20 grad_norm=5.4100e-01, inner_loss=6.4501e+00\n",
      "[eval] sample 78 iter 30 grad_norm=5.1295e-01, inner_loss=6.4223e+00\n",
      "[eval] sample 78 iter 40 grad_norm=4.8684e-01, inner_loss=6.3972e+00\n",
      "[eval] sample 79 iter 0 grad_norm=9.6277e-01, inner_loss=7.1590e+00\n",
      "[eval] sample 79 iter 10 grad_norm=9.1366e-01, inner_loss=7.0707e+00\n",
      "[eval] sample 79 iter 20 grad_norm=8.6772e-01, inner_loss=6.9912e+00\n",
      "[eval] sample 79 iter 30 grad_norm=8.2472e-01, inner_loss=6.9195e+00\n",
      "[eval] sample 79 iter 40 grad_norm=7.8445e-01, inner_loss=6.8546e+00\n",
      "[eval] sample 80 iter 0 grad_norm=6.8850e-01, inner_loss=6.7910e+00\n",
      "[eval] sample 80 iter 10 grad_norm=6.5785e-01, inner_loss=6.7456e+00\n",
      "[eval] sample 80 iter 20 grad_norm=6.2918e-01, inner_loss=6.7041e+00\n",
      "[eval] sample 80 iter 30 grad_norm=6.0234e-01, inner_loss=6.6661e+00\n",
      "[eval] sample 80 iter 40 grad_norm=5.7720e-01, inner_loss=6.6312e+00\n",
      "[eval] sample 81 iter 0 grad_norm=7.5205e-01, inner_loss=6.8464e+00\n",
      "[eval] sample 81 iter 10 grad_norm=7.1693e-01, inner_loss=6.7923e+00\n",
      "[eval] sample 81 iter 20 grad_norm=6.8399e-01, inner_loss=6.7432e+00\n",
      "[eval] sample 81 iter 30 grad_norm=6.5310e-01, inner_loss=6.6984e+00\n",
      "[eval] sample 81 iter 40 grad_norm=6.2409e-01, inner_loss=6.6575e+00\n",
      "[eval] sample 82 iter 0 grad_norm=9.0621e-01, inner_loss=7.1280e+00\n",
      "[eval] sample 82 iter 10 grad_norm=8.6178e-01, inner_loss=7.0497e+00\n",
      "[eval] sample 82 iter 20 grad_norm=8.2014e-01, inner_loss=6.9788e+00\n",
      "[eval] sample 82 iter 30 grad_norm=7.8109e-01, inner_loss=6.9146e+00\n",
      "[eval] sample 82 iter 40 grad_norm=7.4444e-01, inner_loss=6.8563e+00\n",
      "[eval] sample 83 iter 0 grad_norm=6.4156e-01, inner_loss=6.6537e+00\n",
      "[eval] sample 83 iter 10 grad_norm=6.1268e-01, inner_loss=6.6143e+00\n",
      "[eval] sample 83 iter 20 grad_norm=5.8556e-01, inner_loss=6.5783e+00\n",
      "[eval] sample 83 iter 30 grad_norm=5.6008e-01, inner_loss=6.5454e+00\n",
      "[eval] sample 83 iter 40 grad_norm=5.3611e-01, inner_loss=6.5153e+00\n",
      "[eval] sample 84 iter 0 grad_norm=6.1228e-01, inner_loss=6.7540e+00\n",
      "[eval] sample 84 iter 10 grad_norm=5.8603e-01, inner_loss=6.7181e+00\n",
      "[eval] sample 84 iter 20 grad_norm=5.6161e-01, inner_loss=6.6851e+00\n",
      "[eval] sample 84 iter 30 grad_norm=5.3887e-01, inner_loss=6.6548e+00\n",
      "[eval] sample 84 iter 40 grad_norm=5.1766e-01, inner_loss=6.6268e+00\n",
      "[eval] sample 85 iter 0 grad_norm=8.9619e-01, inner_loss=7.0821e+00\n",
      "[eval] sample 85 iter 10 grad_norm=8.5240e-01, inner_loss=7.0055e+00\n",
      "[eval] sample 85 iter 20 grad_norm=8.1138e-01, inner_loss=6.9361e+00\n",
      "[eval] sample 85 iter 30 grad_norm=7.7294e-01, inner_loss=6.8733e+00\n",
      "[eval] sample 85 iter 40 grad_norm=7.3689e-01, inner_loss=6.8161e+00\n",
      "[eval] sample 86 iter 0 grad_norm=8.1414e-01, inner_loss=6.9005e+00\n",
      "[eval] sample 86 iter 10 grad_norm=7.7403e-01, inner_loss=6.8373e+00\n",
      "[eval] sample 86 iter 20 grad_norm=7.3652e-01, inner_loss=6.7801e+00\n",
      "[eval] sample 86 iter 30 grad_norm=7.0142e-01, inner_loss=6.7283e+00\n",
      "[eval] sample 86 iter 40 grad_norm=6.6856e-01, inner_loss=6.6813e+00\n",
      "[eval] sample 87 iter 0 grad_norm=7.2713e-01, inner_loss=6.8340e+00\n",
      "[eval] sample 87 iter 10 grad_norm=6.9329e-01, inner_loss=6.7835e+00\n",
      "[eval] sample 87 iter 20 grad_norm=6.6163e-01, inner_loss=6.7375e+00\n",
      "[eval] sample 87 iter 30 grad_norm=6.3200e-01, inner_loss=6.6956e+00\n",
      "[eval] sample 87 iter 40 grad_norm=6.0423e-01, inner_loss=6.6573e+00\n",
      "[eval] sample 88 iter 0 grad_norm=7.2215e-01, inner_loss=6.8509e+00\n",
      "[eval] sample 88 iter 10 grad_norm=6.8841e-01, inner_loss=6.8011e+00\n",
      "[eval] sample 88 iter 20 grad_norm=6.5684e-01, inner_loss=6.7557e+00\n",
      "[eval] sample 88 iter 30 grad_norm=6.2728e-01, inner_loss=6.7144e+00\n",
      "[eval] sample 88 iter 40 grad_norm=5.9958e-01, inner_loss=6.6767e+00\n",
      "[eval] sample 89 iter 0 grad_norm=6.7375e-01, inner_loss=6.6309e+00\n",
      "[eval] sample 89 iter 10 grad_norm=6.4089e-01, inner_loss=6.5876e+00\n",
      "[eval] sample 89 iter 20 grad_norm=6.1015e-01, inner_loss=6.5484e+00\n",
      "[eval] sample 89 iter 30 grad_norm=5.8136e-01, inner_loss=6.5128e+00\n",
      "[eval] sample 89 iter 40 grad_norm=5.5439e-01, inner_loss=6.4805e+00\n",
      "[eval] sample 90 iter 0 grad_norm=6.6556e-01, inner_loss=6.7719e+00\n",
      "[eval] sample 90 iter 10 grad_norm=6.3481e-01, inner_loss=6.7296e+00\n",
      "[eval] sample 90 iter 20 grad_norm=6.0610e-01, inner_loss=6.6910e+00\n",
      "[eval] sample 90 iter 30 grad_norm=5.7927e-01, inner_loss=6.6558e+00\n",
      "[eval] sample 90 iter 40 grad_norm=5.5419e-01, inner_loss=6.6236e+00\n",
      "[eval] sample 91 iter 0 grad_norm=5.7759e-01, inner_loss=6.5933e+00\n",
      "[eval] sample 91 iter 10 grad_norm=5.5032e-01, inner_loss=6.5614e+00\n",
      "[eval] sample 91 iter 20 grad_norm=5.2498e-01, inner_loss=6.5325e+00\n",
      "[eval] sample 91 iter 30 grad_norm=5.0141e-01, inner_loss=6.5061e+00\n",
      "[eval] sample 91 iter 40 grad_norm=4.7948e-01, inner_loss=6.4820e+00\n",
      "[eval] sample 92 iter 0 grad_norm=4.0853e-01, inner_loss=6.3039e+00\n",
      "[eval] sample 92 iter 10 grad_norm=3.8946e-01, inner_loss=6.2879e+00\n",
      "[eval] sample 92 iter 20 grad_norm=3.7171e-01, inner_loss=6.2734e+00\n",
      "[eval] sample 92 iter 30 grad_norm=3.5519e-01, inner_loss=6.2602e+00\n",
      "[eval] sample 92 iter 40 grad_norm=3.3980e-01, inner_loss=6.2481e+00\n",
      "[eval] sample 93 iter 0 grad_norm=8.8399e-01, inner_loss=6.9721e+00\n",
      "[eval] sample 93 iter 10 grad_norm=8.3675e-01, inner_loss=6.8979e+00\n",
      "[eval] sample 93 iter 20 grad_norm=7.9275e-01, inner_loss=6.8313e+00\n",
      "[eval] sample 93 iter 30 grad_norm=7.5173e-01, inner_loss=6.7716e+00\n",
      "[eval] sample 93 iter 40 grad_norm=7.1347e-01, inner_loss=6.7178e+00\n",
      "[eval] sample 94 iter 0 grad_norm=7.8884e-01, inner_loss=6.7529e+00\n",
      "[eval] sample 94 iter 10 grad_norm=7.4749e-01, inner_loss=6.6937e+00\n",
      "[eval] sample 94 iter 20 grad_norm=7.0885e-01, inner_loss=6.6406e+00\n",
      "[eval] sample 94 iter 30 grad_norm=6.7274e-01, inner_loss=6.5927e+00\n",
      "[eval] sample 94 iter 40 grad_norm=6.3896e-01, inner_loss=6.5496e+00\n",
      "[eval] sample 95 iter 0 grad_norm=7.7335e-01, inner_loss=6.8970e+00\n",
      "[eval] sample 95 iter 10 grad_norm=7.3461e-01, inner_loss=6.8400e+00\n",
      "[eval] sample 95 iter 20 grad_norm=6.9865e-01, inner_loss=6.7885e+00\n",
      "[eval] sample 95 iter 30 grad_norm=6.6523e-01, inner_loss=6.7419e+00\n",
      "[eval] sample 95 iter 40 grad_norm=6.3416e-01, inner_loss=6.6996e+00\n",
      "[eval] sample 96 iter 0 grad_norm=5.2418e-01, inner_loss=6.4282e+00\n",
      "[eval] sample 96 iter 10 grad_norm=4.9991e-01, inner_loss=6.4019e+00\n",
      "[eval] sample 96 iter 20 grad_norm=4.7719e-01, inner_loss=6.3780e+00\n",
      "[eval] sample 96 iter 30 grad_norm=4.5588e-01, inner_loss=6.3562e+00\n",
      "[eval] sample 96 iter 40 grad_norm=4.3590e-01, inner_loss=6.3363e+00\n",
      "[eval] sample 97 iter 0 grad_norm=7.6646e-01, inner_loss=6.9058e+00\n",
      "[eval] sample 97 iter 10 grad_norm=7.2970e-01, inner_loss=6.8497e+00\n",
      "[eval] sample 97 iter 20 grad_norm=6.9544e-01, inner_loss=6.7988e+00\n",
      "[eval] sample 97 iter 30 grad_norm=6.6349e-01, inner_loss=6.7526e+00\n",
      "[eval] sample 97 iter 40 grad_norm=6.3367e-01, inner_loss=6.7104e+00\n",
      "[eval] sample 98 iter 0 grad_norm=6.6402e-01, inner_loss=6.6684e+00\n",
      "[eval] sample 98 iter 10 grad_norm=6.3170e-01, inner_loss=6.6263e+00\n",
      "[eval] sample 98 iter 20 grad_norm=6.0146e-01, inner_loss=6.5882e+00\n",
      "[eval] sample 98 iter 30 grad_norm=5.7315e-01, inner_loss=6.5537e+00\n",
      "[eval] sample 98 iter 40 grad_norm=5.4665e-01, inner_loss=6.5223e+00\n",
      "[eval] sample 99 iter 0 grad_norm=8.0339e-01, inner_loss=7.0584e+00\n",
      "[eval] sample 99 iter 10 grad_norm=7.6951e-01, inner_loss=6.9965e+00\n",
      "[eval] sample 99 iter 20 grad_norm=7.3767e-01, inner_loss=6.9396e+00\n",
      "[eval] sample 99 iter 30 grad_norm=7.0774e-01, inner_loss=6.8872e+00\n",
      "[eval] sample 99 iter 40 grad_norm=6.7957e-01, inner_loss=6.8390e+00\n",
      "[eval] sample 100 iter 0 grad_norm=4.5377e-01, inner_loss=6.4094e+00\n",
      "[eval] sample 100 iter 10 grad_norm=4.3552e-01, inner_loss=6.3896e+00\n",
      "[eval] sample 100 iter 20 grad_norm=4.1841e-01, inner_loss=6.3713e+00\n",
      "[eval] sample 100 iter 30 grad_norm=4.0236e-01, inner_loss=6.3544e+00\n",
      "[eval] sample 100 iter 40 grad_norm=3.8728e-01, inner_loss=6.3388e+00\n",
      "[eval] sample 101 iter 0 grad_norm=7.6578e-01, inner_loss=6.9283e+00\n",
      "[eval] sample 101 iter 10 grad_norm=7.3069e-01, inner_loss=6.8722e+00\n",
      "[eval] sample 101 iter 20 grad_norm=6.9771e-01, inner_loss=6.8211e+00\n",
      "[eval] sample 101 iter 30 grad_norm=6.6671e-01, inner_loss=6.7745e+00\n",
      "[eval] sample 101 iter 40 grad_norm=6.3754e-01, inner_loss=6.7319e+00\n",
      "[eval] sample 102 iter 0 grad_norm=7.5375e-01, inner_loss=6.8972e+00\n",
      "[eval] sample 102 iter 10 grad_norm=7.1838e-01, inner_loss=6.8429e+00\n",
      "[eval] sample 102 iter 20 grad_norm=6.8521e-01, inner_loss=6.7935e+00\n",
      "[eval] sample 102 iter 30 grad_norm=6.5409e-01, inner_loss=6.7486e+00\n",
      "[eval] sample 102 iter 40 grad_norm=6.2488e-01, inner_loss=6.7076e+00\n",
      "[eval] sample 103 iter 0 grad_norm=7.5368e-01, inner_loss=6.8718e+00\n",
      "[eval] sample 103 iter 10 grad_norm=7.1846e-01, inner_loss=6.8175e+00\n",
      "[eval] sample 103 iter 20 grad_norm=6.8539e-01, inner_loss=6.7681e+00\n",
      "[eval] sample 103 iter 30 grad_norm=6.5431e-01, inner_loss=6.7231e+00\n",
      "[eval] sample 103 iter 40 grad_norm=6.2510e-01, inner_loss=6.6821e+00\n",
      "[eval] sample 104 iter 0 grad_norm=6.6111e-01, inner_loss=6.7772e+00\n",
      "[eval] sample 104 iter 10 grad_norm=6.3383e-01, inner_loss=6.7352e+00\n",
      "[eval] sample 104 iter 20 grad_norm=6.0818e-01, inner_loss=6.6966e+00\n",
      "[eval] sample 104 iter 30 grad_norm=5.8404e-01, inner_loss=6.6610e+00\n",
      "[eval] sample 104 iter 40 grad_norm=5.6130e-01, inner_loss=6.6281e+00\n",
      "[eval] sample 105 iter 0 grad_norm=5.9645e-01, inner_loss=6.6605e+00\n",
      "[eval] sample 105 iter 10 grad_norm=5.7269e-01, inner_loss=6.6262e+00\n",
      "[eval] sample 105 iter 20 grad_norm=5.5035e-01, inner_loss=6.5946e+00\n",
      "[eval] sample 105 iter 30 grad_norm=5.2931e-01, inner_loss=6.5655e+00\n",
      "[eval] sample 105 iter 40 grad_norm=5.0948e-01, inner_loss=6.5384e+00\n",
      "[eval] sample 106 iter 0 grad_norm=7.5499e-01, inner_loss=6.8963e+00\n",
      "[eval] sample 106 iter 10 grad_norm=7.1969e-01, inner_loss=6.8418e+00\n",
      "[eval] sample 106 iter 20 grad_norm=6.8652e-01, inner_loss=6.7923e+00\n",
      "[eval] sample 106 iter 30 grad_norm=6.5533e-01, inner_loss=6.7472e+00\n",
      "[eval] sample 106 iter 40 grad_norm=6.2600e-01, inner_loss=6.7060e+00\n",
      "[eval] sample 107 iter 0 grad_norm=5.9117e-01, inner_loss=6.5286e+00\n",
      "[eval] sample 107 iter 10 grad_norm=5.6353e-01, inner_loss=6.4952e+00\n",
      "[eval] sample 107 iter 20 grad_norm=5.3763e-01, inner_loss=6.4648e+00\n",
      "[eval] sample 107 iter 30 grad_norm=5.1335e-01, inner_loss=6.4372e+00\n",
      "[eval] sample 107 iter 40 grad_norm=4.9057e-01, inner_loss=6.4119e+00\n",
      "[eval] sample 108 iter 0 grad_norm=6.5033e-01, inner_loss=6.6946e+00\n",
      "[eval] sample 108 iter 10 grad_norm=6.1920e-01, inner_loss=6.6542e+00\n",
      "[eval] sample 108 iter 20 grad_norm=5.9021e-01, inner_loss=6.6176e+00\n",
      "[eval] sample 108 iter 30 grad_norm=5.6319e-01, inner_loss=6.5842e+00\n",
      "[eval] sample 108 iter 40 grad_norm=5.3800e-01, inner_loss=6.5539e+00\n",
      "[eval] sample 109 iter 0 grad_norm=5.4977e-01, inner_loss=6.5916e+00\n",
      "[eval] sample 109 iter 10 grad_norm=5.2781e-01, inner_loss=6.5625e+00\n",
      "[eval] sample 109 iter 20 grad_norm=5.0723e-01, inner_loss=6.5357e+00\n",
      "[eval] sample 109 iter 30 grad_norm=4.8791e-01, inner_loss=6.5109e+00\n",
      "[eval] sample 109 iter 40 grad_norm=4.6975e-01, inner_loss=6.4879e+00\n",
      "[eval] sample 110 iter 0 grad_norm=6.3038e-01, inner_loss=6.7195e+00\n",
      "[eval] sample 110 iter 10 grad_norm=6.0015e-01, inner_loss=6.6816e+00\n",
      "[eval] sample 110 iter 20 grad_norm=5.7213e-01, inner_loss=6.6472e+00\n",
      "[eval] sample 110 iter 30 grad_norm=5.4614e-01, inner_loss=6.6158e+00\n",
      "[eval] sample 110 iter 40 grad_norm=5.2201e-01, inner_loss=6.5873e+00\n",
      "[eval] sample 111 iter 0 grad_norm=6.7274e-01, inner_loss=6.6677e+00\n",
      "[eval] sample 111 iter 10 grad_norm=6.4016e-01, inner_loss=6.6245e+00\n",
      "[eval] sample 111 iter 20 grad_norm=6.0971e-01, inner_loss=6.5853e+00\n",
      "[eval] sample 111 iter 30 grad_norm=5.8122e-01, inner_loss=6.5498e+00\n",
      "[eval] sample 111 iter 40 grad_norm=5.5456e-01, inner_loss=6.5175e+00\n",
      "[eval] sample 112 iter 0 grad_norm=6.0311e-01, inner_loss=6.6041e+00\n",
      "[eval] sample 112 iter 10 grad_norm=5.7582e-01, inner_loss=6.5692e+00\n",
      "[eval] sample 112 iter 20 grad_norm=5.5030e-01, inner_loss=6.5375e+00\n",
      "[eval] sample 112 iter 30 grad_norm=5.2642e-01, inner_loss=6.5084e+00\n",
      "[eval] sample 112 iter 40 grad_norm=5.0405e-01, inner_loss=6.4818e+00\n",
      "[eval] sample 113 iter 0 grad_norm=6.4846e-01, inner_loss=6.7632e+00\n",
      "[eval] sample 113 iter 10 grad_norm=6.2081e-01, inner_loss=6.7229e+00\n",
      "[eval] sample 113 iter 20 grad_norm=5.9501e-01, inner_loss=6.6858e+00\n",
      "[eval] sample 113 iter 30 grad_norm=5.7092e-01, inner_loss=6.6518e+00\n",
      "[eval] sample 113 iter 40 grad_norm=5.4838e-01, inner_loss=6.6204e+00\n",
      "[eval] sample 114 iter 0 grad_norm=6.3446e-01, inner_loss=6.5984e+00\n",
      "[eval] sample 114 iter 10 grad_norm=6.0489e-01, inner_loss=6.5599e+00\n",
      "[eval] sample 114 iter 20 grad_norm=5.7720e-01, inner_loss=6.5249e+00\n",
      "[eval] sample 114 iter 30 grad_norm=5.5124e-01, inner_loss=6.4930e+00\n",
      "[eval] sample 114 iter 40 grad_norm=5.2688e-01, inner_loss=6.4639e+00\n",
      "[eval] sample 115 iter 0 grad_norm=5.1581e-01, inner_loss=6.5367e+00\n",
      "[eval] sample 115 iter 10 grad_norm=4.9380e-01, inner_loss=6.5112e+00\n",
      "[eval] sample 115 iter 20 grad_norm=4.7334e-01, inner_loss=6.4877e+00\n",
      "[eval] sample 115 iter 30 grad_norm=4.5429e-01, inner_loss=6.4662e+00\n",
      "[eval] sample 115 iter 40 grad_norm=4.3654e-01, inner_loss=6.4463e+00\n",
      "[eval] sample 116 iter 0 grad_norm=4.9954e-01, inner_loss=6.4568e+00\n",
      "[eval] sample 116 iter 10 grad_norm=4.7774e-01, inner_loss=6.4329e+00\n",
      "[eval] sample 116 iter 20 grad_norm=4.5742e-01, inner_loss=6.4110e+00\n",
      "[eval] sample 116 iter 30 grad_norm=4.3848e-01, inner_loss=6.3909e+00\n",
      "[eval] sample 116 iter 40 grad_norm=4.2079e-01, inner_loss=6.3724e+00\n",
      "[eval] sample 117 iter 0 grad_norm=7.5656e-01, inner_loss=6.8559e+00\n",
      "[eval] sample 117 iter 10 grad_norm=7.2065e-01, inner_loss=6.8013e+00\n",
      "[eval] sample 117 iter 20 grad_norm=6.8702e-01, inner_loss=6.7516e+00\n",
      "[eval] sample 117 iter 30 grad_norm=6.5551e-01, inner_loss=6.7065e+00\n",
      "[eval] sample 117 iter 40 grad_norm=6.2596e-01, inner_loss=6.6653e+00\n",
      "[eval] sample 118 iter 0 grad_norm=5.0316e-01, inner_loss=6.4979e+00\n",
      "[eval] sample 118 iter 10 grad_norm=4.8294e-01, inner_loss=6.4735e+00\n",
      "[eval] sample 118 iter 20 grad_norm=4.6402e-01, inner_loss=6.4511e+00\n",
      "[eval] sample 118 iter 30 grad_norm=4.4628e-01, inner_loss=6.4303e+00\n",
      "[eval] sample 118 iter 40 grad_norm=4.2964e-01, inner_loss=6.4111e+00\n",
      "[eval] sample 119 iter 0 grad_norm=8.3065e-01, inner_loss=7.0160e+00\n",
      "[eval] sample 119 iter 10 grad_norm=7.9318e-01, inner_loss=6.9499e+00\n",
      "[eval] sample 119 iter 20 grad_norm=7.5790e-01, inner_loss=6.8896e+00\n",
      "[eval] sample 119 iter 30 grad_norm=7.2464e-01, inner_loss=6.8346e+00\n",
      "[eval] sample 119 iter 40 grad_norm=6.9328e-01, inner_loss=6.7842e+00\n",
      "[eval] sample 120 iter 0 grad_norm=8.9691e-01, inner_loss=7.3280e+00\n",
      "[eval] sample 120 iter 10 grad_norm=8.5955e-01, inner_loss=7.2507e+00\n",
      "[eval] sample 120 iter 20 grad_norm=8.2447e-01, inner_loss=7.1797e+00\n",
      "[eval] sample 120 iter 30 grad_norm=7.9150e-01, inner_loss=7.1143e+00\n",
      "[eval] sample 120 iter 40 grad_norm=7.6046e-01, inner_loss=7.0540e+00\n",
      "[eval] sample 121 iter 0 grad_norm=6.3390e-01, inner_loss=6.6702e+00\n",
      "[eval] sample 121 iter 10 grad_norm=6.0727e-01, inner_loss=6.6316e+00\n",
      "[eval] sample 121 iter 20 grad_norm=5.8220e-01, inner_loss=6.5962e+00\n",
      "[eval] sample 121 iter 30 grad_norm=5.5858e-01, inner_loss=6.5636e+00\n",
      "[eval] sample 121 iter 40 grad_norm=5.3629e-01, inner_loss=6.5335e+00\n",
      "[eval] sample 122 iter 0 grad_norm=5.3974e-01, inner_loss=6.4779e+00\n",
      "[eval] sample 122 iter 10 grad_norm=5.1530e-01, inner_loss=6.4500e+00\n",
      "[eval] sample 122 iter 20 grad_norm=4.9239e-01, inner_loss=6.4245e+00\n",
      "[eval] sample 122 iter 30 grad_norm=4.7089e-01, inner_loss=6.4013e+00\n",
      "[eval] sample 122 iter 40 grad_norm=4.5072e-01, inner_loss=6.3800e+00\n",
      "[eval] sample 123 iter 0 grad_norm=5.6531e-01, inner_loss=6.6406e+00\n",
      "[eval] sample 123 iter 10 grad_norm=5.4070e-01, inner_loss=6.6099e+00\n",
      "[eval] sample 123 iter 20 grad_norm=5.1778e-01, inner_loss=6.5819e+00\n",
      "[eval] sample 123 iter 30 grad_norm=4.9639e-01, inner_loss=6.5561e+00\n",
      "[eval] sample 123 iter 40 grad_norm=4.7643e-01, inner_loss=6.5324e+00\n",
      "[eval] sample 124 iter 0 grad_norm=7.6196e-01, inner_loss=6.8607e+00\n",
      "[eval] sample 124 iter 10 grad_norm=7.2560e-01, inner_loss=6.8052e+00\n",
      "[eval] sample 124 iter 20 grad_norm=6.9156e-01, inner_loss=6.7549e+00\n",
      "[eval] sample 124 iter 30 grad_norm=6.5968e-01, inner_loss=6.7092e+00\n",
      "[eval] sample 124 iter 40 grad_norm=6.2980e-01, inner_loss=6.6675e+00\n",
      "[eval] sample 125 iter 0 grad_norm=5.3686e-01, inner_loss=6.5451e+00\n",
      "[eval] sample 125 iter 10 grad_norm=5.1383e-01, inner_loss=6.5174e+00\n",
      "[eval] sample 125 iter 20 grad_norm=4.9234e-01, inner_loss=6.4921e+00\n",
      "[eval] sample 125 iter 30 grad_norm=4.7229e-01, inner_loss=6.4688e+00\n",
      "[eval] sample 125 iter 40 grad_norm=4.5355e-01, inner_loss=6.4473e+00\n",
      "[eval] sample 126 iter 0 grad_norm=8.0339e-01, inner_loss=7.0283e+00\n",
      "[eval] sample 126 iter 10 grad_norm=7.6701e-01, inner_loss=6.9665e+00\n",
      "[eval] sample 126 iter 20 grad_norm=7.3283e-01, inner_loss=6.9101e+00\n",
      "[eval] sample 126 iter 30 grad_norm=7.0068e-01, inner_loss=6.8587e+00\n",
      "[eval] sample 126 iter 40 grad_norm=6.7043e-01, inner_loss=6.8116e+00\n",
      "[eval] sample 127 iter 0 grad_norm=7.6431e-01, inner_loss=6.9649e+00\n",
      "[eval] sample 127 iter 10 grad_norm=7.3018e-01, inner_loss=6.9090e+00\n",
      "[eval] sample 127 iter 20 grad_norm=6.9820e-01, inner_loss=6.8579e+00\n",
      "[eval] sample 127 iter 30 grad_norm=6.6824e-01, inner_loss=6.8111e+00\n",
      "[eval] sample 127 iter 40 grad_norm=6.4013e-01, inner_loss=6.7682e+00\n",
      "[eval] sample 128 iter 0 grad_norm=7.7248e-01, inner_loss=7.0511e+00\n",
      "[eval] sample 128 iter 10 grad_norm=7.3683e-01, inner_loss=6.9941e+00\n",
      "[eval] sample 128 iter 20 grad_norm=7.0375e-01, inner_loss=6.9421e+00\n",
      "[eval] sample 128 iter 30 grad_norm=6.7303e-01, inner_loss=6.8946e+00\n",
      "[eval] sample 128 iter 40 grad_norm=6.4446e-01, inner_loss=6.8512e+00\n",
      "[eval] sample 129 iter 0 grad_norm=6.8579e-01, inner_loss=6.7953e+00\n",
      "[eval] sample 129 iter 10 grad_norm=6.5669e-01, inner_loss=6.7502e+00\n",
      "[eval] sample 129 iter 20 grad_norm=6.2931e-01, inner_loss=6.7087e+00\n",
      "[eval] sample 129 iter 30 grad_norm=6.0354e-01, inner_loss=6.6707e+00\n",
      "[eval] sample 129 iter 40 grad_norm=5.7925e-01, inner_loss=6.6356e+00\n",
      "[eval] sample 130 iter 0 grad_norm=8.0847e-01, inner_loss=6.9428e+00\n",
      "[eval] sample 130 iter 10 grad_norm=7.6921e-01, inner_loss=6.8804e+00\n",
      "[eval] sample 130 iter 20 grad_norm=7.3254e-01, inner_loss=6.8239e+00\n",
      "[eval] sample 130 iter 30 grad_norm=6.9828e-01, inner_loss=6.7727e+00\n",
      "[eval] sample 130 iter 40 grad_norm=6.6624e-01, inner_loss=6.7260e+00\n",
      "[eval] sample 131 iter 0 grad_norm=8.6003e-01, inner_loss=7.0292e+00\n",
      "[eval] sample 131 iter 10 grad_norm=8.1748e-01, inner_loss=6.9587e+00\n",
      "[eval] sample 131 iter 20 grad_norm=7.7772e-01, inner_loss=6.8949e+00\n",
      "[eval] sample 131 iter 30 grad_norm=7.4052e-01, inner_loss=6.8372e+00\n",
      "[eval] sample 131 iter 40 grad_norm=7.0571e-01, inner_loss=6.7848e+00\n",
      "[eval] sample 132 iter 0 grad_norm=9.1333e-01, inner_loss=7.2856e+00\n",
      "[eval] sample 132 iter 10 grad_norm=8.7248e-01, inner_loss=7.2057e+00\n",
      "[eval] sample 132 iter 20 grad_norm=8.3417e-01, inner_loss=7.1327e+00\n",
      "[eval] sample 132 iter 30 grad_norm=7.9820e-01, inner_loss=7.0660e+00\n",
      "[eval] sample 132 iter 40 grad_norm=7.6441e-01, inner_loss=7.0048e+00\n",
      "[eval] sample 133 iter 0 grad_norm=8.6015e-01, inner_loss=7.0536e+00\n",
      "[eval] sample 133 iter 10 grad_norm=8.1920e-01, inner_loss=6.9830e+00\n",
      "[eval] sample 133 iter 20 grad_norm=7.8080e-01, inner_loss=6.9188e+00\n",
      "[eval] sample 133 iter 30 grad_norm=7.4476e-01, inner_loss=6.8605e+00\n",
      "[eval] sample 133 iter 40 grad_norm=7.1093e-01, inner_loss=6.8075e+00\n",
      "[eval] sample 134 iter 0 grad_norm=6.4260e-01, inner_loss=6.7952e+00\n",
      "[eval] sample 134 iter 10 grad_norm=6.1376e-01, inner_loss=6.7557e+00\n",
      "[eval] sample 134 iter 20 grad_norm=5.8701e-01, inner_loss=6.7196e+00\n",
      "[eval] sample 134 iter 30 grad_norm=5.6219e-01, inner_loss=6.6865e+00\n",
      "[eval] sample 134 iter 40 grad_norm=5.3913e-01, inner_loss=6.6561e+00\n",
      "[eval] sample 135 iter 0 grad_norm=7.1423e-01, inner_loss=6.7067e+00\n",
      "[eval] sample 135 iter 10 grad_norm=6.7952e-01, inner_loss=6.6580e+00\n",
      "[eval] sample 135 iter 20 grad_norm=6.4704e-01, inner_loss=6.6139e+00\n",
      "[eval] sample 135 iter 30 grad_norm=6.1661e-01, inner_loss=6.5739e+00\n",
      "[eval] sample 135 iter 40 grad_norm=5.8808e-01, inner_loss=6.5376e+00\n",
      "[eval] sample 136 iter 0 grad_norm=7.4385e-01, inner_loss=6.9227e+00\n",
      "[eval] sample 136 iter 10 grad_norm=7.1255e-01, inner_loss=6.8696e+00\n",
      "[eval] sample 136 iter 20 grad_norm=6.8302e-01, inner_loss=6.8208e+00\n",
      "[eval] sample 136 iter 30 grad_norm=6.5514e-01, inner_loss=6.7759e+00\n",
      "[eval] sample 136 iter 40 grad_norm=6.2881e-01, inner_loss=6.7347e+00\n",
      "[eval] sample 137 iter 0 grad_norm=7.5674e-01, inner_loss=6.7361e+00\n",
      "[eval] sample 137 iter 10 grad_norm=7.1859e-01, inner_loss=6.6816e+00\n",
      "[eval] sample 137 iter 20 grad_norm=6.8289e-01, inner_loss=6.6324e+00\n",
      "[eval] sample 137 iter 30 grad_norm=6.4948e-01, inner_loss=6.5879e+00\n",
      "[eval] sample 137 iter 40 grad_norm=6.1820e-01, inner_loss=6.5477e+00\n",
      "[eval] sample 138 iter 0 grad_norm=6.9014e-01, inner_loss=6.8351e+00\n",
      "[eval] sample 138 iter 10 grad_norm=6.5853e-01, inner_loss=6.7895e+00\n",
      "[eval] sample 138 iter 20 grad_norm=6.2895e-01, inner_loss=6.7480e+00\n",
      "[eval] sample 138 iter 30 grad_norm=6.0126e-01, inner_loss=6.7101e+00\n",
      "[eval] sample 138 iter 40 grad_norm=5.7532e-01, inner_loss=6.6754e+00\n",
      "[eval] sample 139 iter 0 grad_norm=7.2005e-01, inner_loss=6.8466e+00\n",
      "[eval] sample 139 iter 10 grad_norm=6.8866e-01, inner_loss=6.7969e+00\n",
      "[eval] sample 139 iter 20 grad_norm=6.5925e-01, inner_loss=6.7513e+00\n",
      "[eval] sample 139 iter 30 grad_norm=6.3166e-01, inner_loss=6.7096e+00\n",
      "[eval] sample 139 iter 40 grad_norm=6.0575e-01, inner_loss=6.6713e+00\n",
      "[eval] sample 140 iter 0 grad_norm=4.4484e-01, inner_loss=6.3496e+00\n",
      "[eval] sample 140 iter 10 grad_norm=4.2406e-01, inner_loss=6.3307e+00\n",
      "[eval] sample 140 iter 20 grad_norm=4.0469e-01, inner_loss=6.3135e+00\n",
      "[eval] sample 140 iter 30 grad_norm=3.8662e-01, inner_loss=6.2978e+00\n",
      "[eval] sample 140 iter 40 grad_norm=3.6976e-01, inner_loss=6.2835e+00\n",
      "[eval] sample 141 iter 0 grad_norm=6.5759e-01, inner_loss=6.7278e+00\n",
      "[eval] sample 141 iter 10 grad_norm=6.2773e-01, inner_loss=6.6864e+00\n",
      "[eval] sample 141 iter 20 grad_norm=5.9975e-01, inner_loss=6.6487e+00\n",
      "[eval] sample 141 iter 30 grad_norm=5.7352e-01, inner_loss=6.6142e+00\n",
      "[eval] sample 141 iter 40 grad_norm=5.4892e-01, inner_loss=6.5827e+00\n",
      "[eval] sample 142 iter 0 grad_norm=6.4030e-01, inner_loss=6.7464e+00\n",
      "[eval] sample 142 iter 10 grad_norm=6.1357e-01, inner_loss=6.7070e+00\n",
      "[eval] sample 142 iter 20 grad_norm=5.8847e-01, inner_loss=6.6708e+00\n",
      "[eval] sample 142 iter 30 grad_norm=5.6487e-01, inner_loss=6.6375e+00\n",
      "[eval] sample 142 iter 40 grad_norm=5.4267e-01, inner_loss=6.6068e+00\n",
      "[eval] sample 143 iter 0 grad_norm=5.2139e-01, inner_loss=6.4342e+00\n",
      "[eval] sample 143 iter 10 grad_norm=4.9758e-01, inner_loss=6.4082e+00\n",
      "[eval] sample 143 iter 20 grad_norm=4.7526e-01, inner_loss=6.3844e+00\n",
      "[eval] sample 143 iter 30 grad_norm=4.5433e-01, inner_loss=6.3628e+00\n",
      "[eval] sample 143 iter 40 grad_norm=4.3469e-01, inner_loss=6.3430e+00\n",
      "[eval] sample 144 iter 0 grad_norm=6.7880e-01, inner_loss=6.8011e+00\n",
      "[eval] sample 144 iter 10 grad_norm=6.4966e-01, inner_loss=6.7569e+00\n",
      "[eval] sample 144 iter 20 grad_norm=6.2241e-01, inner_loss=6.7164e+00\n",
      "[eval] sample 144 iter 30 grad_norm=5.9691e-01, inner_loss=6.6791e+00\n",
      "[eval] sample 144 iter 40 grad_norm=5.7302e-01, inner_loss=6.6449e+00\n",
      "[eval] sample 145 iter 0 grad_norm=5.0259e-01, inner_loss=6.4090e+00\n",
      "[eval] sample 145 iter 10 grad_norm=4.7976e-01, inner_loss=6.3848e+00\n",
      "[eval] sample 145 iter 20 grad_norm=4.5835e-01, inner_loss=6.3628e+00\n",
      "[eval] sample 145 iter 30 grad_norm=4.3828e-01, inner_loss=6.3426e+00\n",
      "[eval] sample 145 iter 40 grad_norm=4.1943e-01, inner_loss=6.3242e+00\n",
      "[eval] sample 146 iter 0 grad_norm=6.7168e-01, inner_loss=6.8365e+00\n",
      "[eval] sample 146 iter 10 grad_norm=6.4015e-01, inner_loss=6.7933e+00\n",
      "[eval] sample 146 iter 20 grad_norm=6.1090e-01, inner_loss=6.7541e+00\n",
      "[eval] sample 146 iter 30 grad_norm=5.8374e-01, inner_loss=6.7184e+00\n",
      "[eval] sample 146 iter 40 grad_norm=5.5850e-01, inner_loss=6.6857e+00\n",
      "[eval] sample 147 iter 0 grad_norm=8.7196e-01, inner_loss=7.1356e+00\n",
      "[eval] sample 147 iter 10 grad_norm=8.2983e-01, inner_loss=7.0631e+00\n",
      "[eval] sample 147 iter 20 grad_norm=7.9040e-01, inner_loss=6.9973e+00\n",
      "[eval] sample 147 iter 30 grad_norm=7.5348e-01, inner_loss=6.9376e+00\n",
      "[eval] sample 147 iter 40 grad_norm=7.1889e-01, inner_loss=6.8833e+00\n",
      "[eval] sample 148 iter 0 grad_norm=7.2487e-01, inner_loss=6.8568e+00\n",
      "[eval] sample 148 iter 10 grad_norm=6.9375e-01, inner_loss=6.8064e+00\n",
      "[eval] sample 148 iter 20 grad_norm=6.6437e-01, inner_loss=6.7602e+00\n",
      "[eval] sample 148 iter 30 grad_norm=6.3662e-01, inner_loss=6.7178e+00\n",
      "[eval] sample 148 iter 40 grad_norm=6.1040e-01, inner_loss=6.6789e+00\n",
      "[eval] sample 149 iter 0 grad_norm=4.4000e-01, inner_loss=6.3595e+00\n",
      "[eval] sample 149 iter 10 grad_norm=4.1895e-01, inner_loss=6.3410e+00\n",
      "[eval] sample 149 iter 20 grad_norm=3.9937e-01, inner_loss=6.3243e+00\n",
      "[eval] sample 149 iter 30 grad_norm=3.8116e-01, inner_loss=6.3090e+00\n",
      "[eval] sample 149 iter 40 grad_norm=3.6420e-01, inner_loss=6.2951e+00\n",
      "[eval] sample 150 iter 0 grad_norm=6.8622e-01, inner_loss=6.8209e+00\n",
      "[eval] sample 150 iter 10 grad_norm=6.5841e-01, inner_loss=6.7757e+00\n",
      "[eval] sample 150 iter 20 grad_norm=6.3223e-01, inner_loss=6.7339e+00\n",
      "[eval] sample 150 iter 30 grad_norm=6.0756e-01, inner_loss=6.6955e+00\n",
      "[eval] sample 150 iter 40 grad_norm=5.8429e-01, inner_loss=6.6599e+00\n",
      "[eval] sample 151 iter 0 grad_norm=7.7908e-01, inner_loss=7.0298e+00\n",
      "[eval] sample 151 iter 10 grad_norm=7.4781e-01, inner_loss=6.9714e+00\n",
      "[eval] sample 151 iter 20 grad_norm=7.1828e-01, inner_loss=6.9175e+00\n",
      "[eval] sample 151 iter 30 grad_norm=6.9037e-01, inner_loss=6.8679e+00\n",
      "[eval] sample 151 iter 40 grad_norm=6.6395e-01, inner_loss=6.8219e+00\n",
      "[eval] sample 152 iter 0 grad_norm=5.8014e-01, inner_loss=6.6342e+00\n",
      "[eval] sample 152 iter 10 grad_norm=5.5573e-01, inner_loss=6.6019e+00\n",
      "[eval] sample 152 iter 20 grad_norm=5.3291e-01, inner_loss=6.5722e+00\n",
      "[eval] sample 152 iter 30 grad_norm=5.1156e-01, inner_loss=6.5449e+00\n",
      "[eval] sample 152 iter 40 grad_norm=4.9156e-01, inner_loss=6.5197e+00\n",
      "[eval] sample 153 iter 0 grad_norm=5.6900e-01, inner_loss=6.5832e+00\n",
      "[eval] sample 153 iter 10 grad_norm=5.4544e-01, inner_loss=6.5521e+00\n",
      "[eval] sample 153 iter 20 grad_norm=5.2331e-01, inner_loss=6.5235e+00\n",
      "[eval] sample 153 iter 30 grad_norm=5.0250e-01, inner_loss=6.4971e+00\n",
      "[eval] sample 153 iter 40 grad_norm=4.8291e-01, inner_loss=6.4728e+00\n",
      "[eval] sample 154 iter 0 grad_norm=4.6553e-01, inner_loss=6.3559e+00\n",
      "[eval] sample 154 iter 10 grad_norm=4.4451e-01, inner_loss=6.3352e+00\n",
      "[eval] sample 154 iter 20 grad_norm=4.2482e-01, inner_loss=6.3163e+00\n",
      "[eval] sample 154 iter 30 grad_norm=4.0634e-01, inner_loss=6.2990e+00\n",
      "[eval] sample 154 iter 40 grad_norm=3.8900e-01, inner_loss=6.2831e+00\n",
      "[eval] sample 155 iter 0 grad_norm=8.3853e-01, inner_loss=7.0755e+00\n",
      "[eval] sample 155 iter 10 grad_norm=8.0173e-01, inner_loss=7.0082e+00\n",
      "[eval] sample 155 iter 20 grad_norm=7.6711e-01, inner_loss=6.9465e+00\n",
      "[eval] sample 155 iter 30 grad_norm=7.3452e-01, inner_loss=6.8900e+00\n",
      "[eval] sample 155 iter 40 grad_norm=7.0380e-01, inner_loss=6.8382e+00\n",
      "[eval] sample 156 iter 0 grad_norm=6.5375e-01, inner_loss=6.6525e+00\n",
      "[eval] sample 156 iter 10 grad_norm=6.2224e-01, inner_loss=6.6117e+00\n",
      "[eval] sample 156 iter 20 grad_norm=5.9275e-01, inner_loss=6.5748e+00\n",
      "[eval] sample 156 iter 30 grad_norm=5.6513e-01, inner_loss=6.5412e+00\n",
      "[eval] sample 156 iter 40 grad_norm=5.3925e-01, inner_loss=6.5106e+00\n",
      "[eval] sample 157 iter 0 grad_norm=7.2908e-01, inner_loss=6.8563e+00\n",
      "[eval] sample 157 iter 10 grad_norm=6.9780e-01, inner_loss=6.8053e+00\n",
      "[eval] sample 157 iter 20 grad_norm=6.6827e-01, inner_loss=6.7585e+00\n",
      "[eval] sample 157 iter 30 grad_norm=6.4036e-01, inner_loss=6.7156e+00\n",
      "[eval] sample 157 iter 40 grad_norm=6.1397e-01, inner_loss=6.6762e+00\n",
      "[eval] sample 158 iter 0 grad_norm=7.4931e-01, inner_loss=6.8582e+00\n",
      "[eval] sample 158 iter 10 grad_norm=7.1461e-01, inner_loss=6.8046e+00\n",
      "[eval] sample 158 iter 20 grad_norm=6.8206e-01, inner_loss=6.7557e+00\n",
      "[eval] sample 158 iter 30 grad_norm=6.5150e-01, inner_loss=6.7111e+00\n",
      "[eval] sample 158 iter 40 grad_norm=6.2279e-01, inner_loss=6.6705e+00\n",
      "[eval] sample 159 iter 0 grad_norm=4.6240e-01, inner_loss=6.3769e+00\n",
      "[eval] sample 159 iter 10 grad_norm=4.4093e-01, inner_loss=6.3565e+00\n",
      "[eval] sample 159 iter 20 grad_norm=4.2097e-01, inner_loss=6.3379e+00\n",
      "[eval] sample 159 iter 30 grad_norm=4.0240e-01, inner_loss=6.3209e+00\n",
      "[eval] sample 159 iter 40 grad_norm=3.8511e-01, inner_loss=6.3054e+00\n",
      "[eval] sample 160 iter 0 grad_norm=7.1192e-01, inner_loss=6.8558e+00\n",
      "[eval] sample 160 iter 10 grad_norm=6.8068e-01, inner_loss=6.8072e+00\n",
      "[eval] sample 160 iter 20 grad_norm=6.5148e-01, inner_loss=6.7628e+00\n",
      "[eval] sample 160 iter 30 grad_norm=6.2416e-01, inner_loss=6.7220e+00\n",
      "[eval] sample 160 iter 40 grad_norm=5.9857e-01, inner_loss=6.6846e+00\n",
      "[eval] sample 161 iter 0 grad_norm=6.9890e-01, inner_loss=6.7752e+00\n",
      "[eval] sample 161 iter 10 grad_norm=6.6641e-01, inner_loss=6.7285e+00\n",
      "[eval] sample 161 iter 20 grad_norm=6.3589e-01, inner_loss=6.6860e+00\n",
      "[eval] sample 161 iter 30 grad_norm=6.0720e-01, inner_loss=6.6473e+00\n",
      "[eval] sample 161 iter 40 grad_norm=5.8021e-01, inner_loss=6.6120e+00\n",
      "[eval] sample 162 iter 0 grad_norm=9.2225e-01, inner_loss=7.3171e+00\n",
      "[eval] sample 162 iter 10 grad_norm=8.8175e-01, inner_loss=7.2356e+00\n",
      "[eval] sample 162 iter 20 grad_norm=8.4371e-01, inner_loss=7.1610e+00\n",
      "[eval] sample 162 iter 30 grad_norm=8.0793e-01, inner_loss=7.0927e+00\n",
      "[eval] sample 162 iter 40 grad_norm=7.7427e-01, inner_loss=7.0300e+00\n",
      "[eval] sample 163 iter 0 grad_norm=8.4282e-01, inner_loss=7.0308e+00\n",
      "[eval] sample 163 iter 10 grad_norm=8.0235e-01, inner_loss=6.9630e+00\n",
      "[eval] sample 163 iter 20 grad_norm=7.6441e-01, inner_loss=6.9015e+00\n",
      "[eval] sample 163 iter 30 grad_norm=7.2883e-01, inner_loss=6.8457e+00\n",
      "[eval] sample 163 iter 40 grad_norm=6.9544e-01, inner_loss=6.7949e+00\n",
      "[eval] sample 164 iter 0 grad_norm=5.9807e-01, inner_loss=6.5784e+00\n",
      "[eval] sample 164 iter 10 grad_norm=5.6903e-01, inner_loss=6.5443e+00\n",
      "[eval] sample 164 iter 20 grad_norm=5.4192e-01, inner_loss=6.5134e+00\n",
      "[eval] sample 164 iter 30 grad_norm=5.1661e-01, inner_loss=6.4853e+00\n",
      "[eval] sample 164 iter 40 grad_norm=4.9296e-01, inner_loss=6.4598e+00\n",
      "[eval] sample 165 iter 0 grad_norm=8.2798e-01, inner_loss=7.1969e+00\n",
      "[eval] sample 165 iter 10 grad_norm=7.9521e-01, inner_loss=7.1309e+00\n",
      "[eval] sample 165 iter 20 grad_norm=7.6429e-01, inner_loss=7.0700e+00\n",
      "[eval] sample 165 iter 30 grad_norm=7.3508e-01, inner_loss=7.0137e+00\n",
      "[eval] sample 165 iter 40 grad_norm=7.0746e-01, inner_loss=6.9616e+00\n",
      "[eval] sample 166 iter 0 grad_norm=5.4593e-01, inner_loss=6.6080e+00\n",
      "[eval] sample 166 iter 10 grad_norm=5.2416e-01, inner_loss=6.5794e+00\n",
      "[eval] sample 166 iter 20 grad_norm=5.0381e-01, inner_loss=6.5529e+00\n",
      "[eval] sample 166 iter 30 grad_norm=4.8476e-01, inner_loss=6.5284e+00\n",
      "[eval] sample 166 iter 40 grad_norm=4.6691e-01, inner_loss=6.5058e+00\n",
      "[eval] sample 167 iter 0 grad_norm=4.6609e-01, inner_loss=6.4830e+00\n",
      "[eval] sample 167 iter 10 grad_norm=4.4624e-01, inner_loss=6.4621e+00\n",
      "[eval] sample 167 iter 20 grad_norm=4.2787e-01, inner_loss=6.4430e+00\n",
      "[eval] sample 167 iter 30 grad_norm=4.1086e-01, inner_loss=6.4254e+00\n",
      "[eval] sample 167 iter 40 grad_norm=3.9507e-01, inner_loss=6.4091e+00\n",
      "[eval] sample 168 iter 0 grad_norm=6.2315e-01, inner_loss=6.5702e+00\n",
      "[eval] sample 168 iter 10 grad_norm=5.9375e-01, inner_loss=6.5331e+00\n",
      "[eval] sample 168 iter 20 grad_norm=5.6620e-01, inner_loss=6.4994e+00\n",
      "[eval] sample 168 iter 30 grad_norm=5.4036e-01, inner_loss=6.4687e+00\n",
      "[eval] sample 168 iter 40 grad_norm=5.1611e-01, inner_loss=6.4408e+00\n",
      "[eval] sample 169 iter 0 grad_norm=6.0476e-01, inner_loss=6.6484e+00\n",
      "[eval] sample 169 iter 10 grad_norm=5.7953e-01, inner_loss=6.6133e+00\n",
      "[eval] sample 169 iter 20 grad_norm=5.5591e-01, inner_loss=6.5810e+00\n",
      "[eval] sample 169 iter 30 grad_norm=5.3377e-01, inner_loss=6.5513e+00\n",
      "[eval] sample 169 iter 40 grad_norm=5.1299e-01, inner_loss=6.5238e+00\n",
      "[eval] sample 170 iter 0 grad_norm=5.2534e-01, inner_loss=6.5365e+00\n",
      "[eval] sample 170 iter 10 grad_norm=5.0375e-01, inner_loss=6.5100e+00\n",
      "[eval] sample 170 iter 20 grad_norm=4.8358e-01, inner_loss=6.4856e+00\n",
      "[eval] sample 170 iter 30 grad_norm=4.6473e-01, inner_loss=6.4631e+00\n",
      "[eval] sample 170 iter 40 grad_norm=4.4707e-01, inner_loss=6.4423e+00\n",
      "[eval] sample 171 iter 0 grad_norm=8.8941e-01, inner_loss=6.9823e+00\n",
      "[eval] sample 171 iter 10 grad_norm=8.4313e-01, inner_loss=6.9071e+00\n",
      "[eval] sample 171 iter 20 grad_norm=7.9993e-01, inner_loss=6.8395e+00\n",
      "[eval] sample 171 iter 30 grad_norm=7.5957e-01, inner_loss=6.7785e+00\n",
      "[eval] sample 171 iter 40 grad_norm=7.2186e-01, inner_loss=6.7235e+00\n",
      "[eval] sample 172 iter 0 grad_norm=7.8956e-01, inner_loss=7.0765e+00\n",
      "[eval] sample 172 iter 10 grad_norm=7.5531e-01, inner_loss=7.0167e+00\n",
      "[eval] sample 172 iter 20 grad_norm=7.2320e-01, inner_loss=6.9620e+00\n",
      "[eval] sample 172 iter 30 grad_norm=6.9307e-01, inner_loss=6.9117e+00\n",
      "[eval] sample 172 iter 40 grad_norm=6.6477e-01, inner_loss=6.8656e+00\n",
      "[eval] sample 173 iter 0 grad_norm=5.3508e-01, inner_loss=6.5472e+00\n",
      "[eval] sample 173 iter 10 grad_norm=5.1323e-01, inner_loss=6.5197e+00\n",
      "[eval] sample 173 iter 20 grad_norm=4.9268e-01, inner_loss=6.4943e+00\n",
      "[eval] sample 173 iter 30 grad_norm=4.7334e-01, inner_loss=6.4709e+00\n",
      "[eval] sample 173 iter 40 grad_norm=4.5511e-01, inner_loss=6.4494e+00\n",
      "[eval] sample 174 iter 0 grad_norm=6.5612e-01, inner_loss=6.6612e+00\n",
      "[eval] sample 174 iter 10 grad_norm=6.2388e-01, inner_loss=6.6202e+00\n",
      "[eval] sample 174 iter 20 grad_norm=5.9377e-01, inner_loss=6.5830e+00\n",
      "[eval] sample 174 iter 30 grad_norm=5.6563e-01, inner_loss=6.5494e+00\n",
      "[eval] sample 174 iter 40 grad_norm=5.3932e-01, inner_loss=6.5188e+00\n",
      "[eval] sample 175 iter 0 grad_norm=3.3435e-01, inner_loss=6.1917e+00\n",
      "[eval] sample 175 iter 10 grad_norm=3.1937e-01, inner_loss=6.1810e+00\n",
      "[eval] sample 175 iter 20 grad_norm=3.0536e-01, inner_loss=6.1713e+00\n",
      "[eval] sample 175 iter 30 grad_norm=2.9223e-01, inner_loss=6.1623e+00\n",
      "[eval] sample 175 iter 40 grad_norm=2.7992e-01, inner_loss=6.1541e+00\n",
      "[eval] sample 176 iter 0 grad_norm=3.7126e-01, inner_loss=6.2259e+00\n",
      "[eval] sample 176 iter 10 grad_norm=3.5458e-01, inner_loss=6.2127e+00\n",
      "[eval] sample 176 iter 20 grad_norm=3.3895e-01, inner_loss=6.2007e+00\n",
      "[eval] sample 176 iter 30 grad_norm=3.2429e-01, inner_loss=6.1896e+00\n",
      "[eval] sample 176 iter 40 grad_norm=3.1052e-01, inner_loss=6.1795e+00\n",
      "[eval] sample 177 iter 0 grad_norm=5.9203e-01, inner_loss=6.7544e+00\n",
      "[eval] sample 177 iter 10 grad_norm=5.6743e-01, inner_loss=6.7207e+00\n",
      "[eval] sample 177 iter 20 grad_norm=5.4465e-01, inner_loss=6.6897e+00\n",
      "[eval] sample 177 iter 30 grad_norm=5.2352e-01, inner_loss=6.6612e+00\n",
      "[eval] sample 177 iter 40 grad_norm=5.0388e-01, inner_loss=6.6347e+00\n",
      "[eval] sample 178 iter 0 grad_norm=5.9462e-01, inner_loss=6.5433e+00\n",
      "[eval] sample 178 iter 10 grad_norm=5.6703e-01, inner_loss=6.5095e+00\n",
      "[eval] sample 178 iter 20 grad_norm=5.4117e-01, inner_loss=6.4787e+00\n",
      "[eval] sample 178 iter 30 grad_norm=5.1692e-01, inner_loss=6.4507e+00\n",
      "[eval] sample 178 iter 40 grad_norm=4.9417e-01, inner_loss=6.4251e+00\n",
      "[eval] sample 179 iter 0 grad_norm=7.1910e-01, inner_loss=6.9094e+00\n",
      "[eval] sample 179 iter 10 grad_norm=6.8557e-01, inner_loss=6.8599e+00\n",
      "[eval] sample 179 iter 20 grad_norm=6.5446e-01, inner_loss=6.8150e+00\n",
      "[eval] sample 179 iter 30 grad_norm=6.2556e-01, inner_loss=6.7739e+00\n",
      "[eval] sample 179 iter 40 grad_norm=5.9871e-01, inner_loss=6.7364e+00\n",
      "[eval] sample 180 iter 0 grad_norm=5.3933e-01, inner_loss=6.4670e+00\n",
      "[eval] sample 180 iter 10 grad_norm=5.1468e-01, inner_loss=6.4391e+00\n",
      "[eval] sample 180 iter 20 grad_norm=4.9159e-01, inner_loss=6.4138e+00\n",
      "[eval] sample 180 iter 30 grad_norm=4.6995e-01, inner_loss=6.3906e+00\n",
      "[eval] sample 180 iter 40 grad_norm=4.4964e-01, inner_loss=6.3694e+00\n",
      "[eval] sample 181 iter 0 grad_norm=6.9432e-01, inner_loss=6.9379e+00\n",
      "[eval] sample 181 iter 10 grad_norm=6.6508e-01, inner_loss=6.8916e+00\n",
      "[eval] sample 181 iter 20 grad_norm=6.3792e-01, inner_loss=6.8491e+00\n",
      "[eval] sample 181 iter 30 grad_norm=6.1266e-01, inner_loss=6.8099e+00\n",
      "[eval] sample 181 iter 40 grad_norm=5.8914e-01, inner_loss=6.7738e+00\n",
      "[eval] sample 182 iter 0 grad_norm=6.2529e-01, inner_loss=6.6353e+00\n",
      "[eval] sample 182 iter 10 grad_norm=5.9654e-01, inner_loss=6.5979e+00\n",
      "[eval] sample 182 iter 20 grad_norm=5.6963e-01, inner_loss=6.5639e+00\n",
      "[eval] sample 182 iter 30 grad_norm=5.4442e-01, inner_loss=6.5328e+00\n",
      "[eval] sample 182 iter 40 grad_norm=5.2079e-01, inner_loss=6.5043e+00\n",
      "[eval] sample 183 iter 0 grad_norm=9.7660e-01, inner_loss=7.4550e+00\n",
      "[eval] sample 183 iter 10 grad_norm=9.3155e-01, inner_loss=7.3638e+00\n",
      "[eval] sample 183 iter 20 grad_norm=8.8922e-01, inner_loss=7.2807e+00\n",
      "[eval] sample 183 iter 30 grad_norm=8.4943e-01, inner_loss=7.2050e+00\n",
      "[eval] sample 183 iter 40 grad_norm=8.1200e-01, inner_loss=7.1359e+00\n",
      "[eval] sample 184 iter 0 grad_norm=5.9097e-01, inner_loss=6.6921e+00\n",
      "[eval] sample 184 iter 10 grad_norm=5.6638e-01, inner_loss=6.6586e+00\n",
      "[eval] sample 184 iter 20 grad_norm=5.4351e-01, inner_loss=6.6277e+00\n",
      "[eval] sample 184 iter 30 grad_norm=5.2219e-01, inner_loss=6.5993e+00\n",
      "[eval] sample 184 iter 40 grad_norm=5.0229e-01, inner_loss=6.5730e+00\n",
      "[eval] sample 185 iter 0 grad_norm=6.2152e-01, inner_loss=6.7175e+00\n",
      "[eval] sample 185 iter 10 grad_norm=5.9625e-01, inner_loss=6.6804e+00\n",
      "[eval] sample 185 iter 20 grad_norm=5.7257e-01, inner_loss=6.6462e+00\n",
      "[eval] sample 185 iter 30 grad_norm=5.5036e-01, inner_loss=6.6146e+00\n",
      "[eval] sample 185 iter 40 grad_norm=5.2948e-01, inner_loss=6.5854e+00\n",
      "[eval] sample 186 iter 0 grad_norm=9.2581e-01, inner_loss=7.2212e+00\n",
      "[eval] sample 186 iter 10 grad_norm=8.8138e-01, inner_loss=7.1394e+00\n",
      "[eval] sample 186 iter 20 grad_norm=8.3987e-01, inner_loss=7.0652e+00\n",
      "[eval] sample 186 iter 30 grad_norm=8.0104e-01, inner_loss=6.9977e+00\n",
      "[eval] sample 186 iter 40 grad_norm=7.6471e-01, inner_loss=6.9363e+00\n",
      "[eval] sample 187 iter 0 grad_norm=7.5573e-01, inner_loss=7.0680e+00\n",
      "[eval] sample 187 iter 10 grad_norm=7.2771e-01, inner_loss=7.0129e+00\n",
      "[eval] sample 187 iter 20 grad_norm=7.0128e-01, inner_loss=6.9618e+00\n",
      "[eval] sample 187 iter 30 grad_norm=6.7631e-01, inner_loss=6.9143e+00\n",
      "[eval] sample 187 iter 40 grad_norm=6.5268e-01, inner_loss=6.8700e+00\n",
      "[eval] sample 188 iter 0 grad_norm=9.7230e-01, inner_loss=7.3834e+00\n",
      "[eval] sample 188 iter 10 grad_norm=9.2717e-01, inner_loss=7.2931e+00\n",
      "[eval] sample 188 iter 20 grad_norm=8.8474e-01, inner_loss=7.2108e+00\n",
      "[eval] sample 188 iter 30 grad_norm=8.4481e-01, inner_loss=7.1359e+00\n",
      "[eval] sample 188 iter 40 grad_norm=8.0723e-01, inner_loss=7.0675e+00\n",
      "[eval] sample 189 iter 0 grad_norm=4.2503e-01, inner_loss=6.3002e+00\n",
      "[eval] sample 189 iter 10 grad_norm=4.0600e-01, inner_loss=6.2829e+00\n",
      "[eval] sample 189 iter 20 grad_norm=3.8816e-01, inner_loss=6.2671e+00\n",
      "[eval] sample 189 iter 30 grad_norm=3.7142e-01, inner_loss=6.2527e+00\n",
      "[eval] sample 189 iter 40 grad_norm=3.5570e-01, inner_loss=6.2394e+00\n",
      "[eval] sample 190 iter 0 grad_norm=7.5891e-01, inner_loss=6.6975e+00\n",
      "[eval] sample 190 iter 10 grad_norm=7.1920e-01, inner_loss=6.6428e+00\n",
      "[eval] sample 190 iter 20 grad_norm=6.8208e-01, inner_loss=6.5936e+00\n",
      "[eval] sample 190 iter 30 grad_norm=6.4737e-01, inner_loss=6.5493e+00\n",
      "[eval] sample 190 iter 40 grad_norm=6.1490e-01, inner_loss=6.5094e+00\n",
      "[eval] sample 191 iter 0 grad_norm=5.6440e-01, inner_loss=6.4929e+00\n",
      "[eval] sample 191 iter 10 grad_norm=5.3822e-01, inner_loss=6.4624e+00\n",
      "[eval] sample 191 iter 20 grad_norm=5.1370e-01, inner_loss=6.4347e+00\n",
      "[eval] sample 191 iter 30 grad_norm=4.9071e-01, inner_loss=6.4094e+00\n",
      "[eval] sample 191 iter 40 grad_norm=4.6913e-01, inner_loss=6.3864e+00\n",
      "[eval] sample 192 iter 0 grad_norm=7.8622e-01, inner_loss=7.0649e+00\n",
      "[eval] sample 192 iter 10 grad_norm=7.5408e-01, inner_loss=7.0055e+00\n",
      "[eval] sample 192 iter 20 grad_norm=7.2379e-01, inner_loss=6.9508e+00\n",
      "[eval] sample 192 iter 30 grad_norm=6.9522e-01, inner_loss=6.9003e+00\n",
      "[eval] sample 192 iter 40 grad_norm=6.6824e-01, inner_loss=6.8538e+00\n",
      "[eval] sample 193 iter 0 grad_norm=5.9204e-01, inner_loss=6.5610e+00\n",
      "[eval] sample 193 iter 10 grad_norm=5.6477e-01, inner_loss=6.5275e+00\n",
      "[eval] sample 193 iter 20 grad_norm=5.3931e-01, inner_loss=6.4969e+00\n",
      "[eval] sample 193 iter 30 grad_norm=5.1551e-01, inner_loss=6.4691e+00\n",
      "[eval] sample 193 iter 40 grad_norm=4.9326e-01, inner_loss=6.4436e+00\n",
      "[eval] sample 194 iter 0 grad_norm=8.4149e-01, inner_loss=7.0780e+00\n",
      "[eval] sample 194 iter 10 grad_norm=8.0345e-01, inner_loss=7.0103e+00\n",
      "[eval] sample 194 iter 20 grad_norm=7.6762e-01, inner_loss=6.9484e+00\n",
      "[eval] sample 194 iter 30 grad_norm=7.3386e-01, inner_loss=6.8920e+00\n",
      "[eval] sample 194 iter 40 grad_norm=7.0204e-01, inner_loss=6.8403e+00\n",
      "[eval] sample 195 iter 0 grad_norm=4.0910e-01, inner_loss=6.2914e+00\n",
      "[eval] sample 195 iter 10 grad_norm=3.9104e-01, inner_loss=6.2754e+00\n",
      "[eval] sample 195 iter 20 grad_norm=3.7412e-01, inner_loss=6.2607e+00\n",
      "[eval] sample 195 iter 30 grad_norm=3.5824e-01, inner_loss=6.2473e+00\n",
      "[eval] sample 195 iter 40 grad_norm=3.4334e-01, inner_loss=6.2350e+00\n",
      "[eval] sample 196 iter 0 grad_norm=7.4028e-01, inner_loss=6.7070e+00\n",
      "[eval] sample 196 iter 10 grad_norm=7.0301e-01, inner_loss=6.6548e+00\n",
      "[eval] sample 196 iter 20 grad_norm=6.6814e-01, inner_loss=6.6077e+00\n",
      "[eval] sample 196 iter 30 grad_norm=6.3550e-01, inner_loss=6.5651e+00\n",
      "[eval] sample 196 iter 40 grad_norm=6.0493e-01, inner_loss=6.5265e+00\n",
      "[eval] sample 197 iter 0 grad_norm=7.7780e-01, inner_loss=6.8177e+00\n",
      "[eval] sample 197 iter 10 grad_norm=7.3624e-01, inner_loss=6.7602e+00\n",
      "[eval] sample 197 iter 20 grad_norm=6.9760e-01, inner_loss=6.7087e+00\n",
      "[eval] sample 197 iter 30 grad_norm=6.6167e-01, inner_loss=6.6624e+00\n",
      "[eval] sample 197 iter 40 grad_norm=6.2824e-01, inner_loss=6.6207e+00\n",
      "[eval] sample 198 iter 0 grad_norm=5.9432e-01, inner_loss=6.6679e+00\n",
      "[eval] sample 198 iter 10 grad_norm=5.7020e-01, inner_loss=6.6339e+00\n",
      "[eval] sample 198 iter 20 grad_norm=5.4764e-01, inner_loss=6.6026e+00\n",
      "[eval] sample 198 iter 30 grad_norm=5.2650e-01, inner_loss=6.5738e+00\n",
      "[eval] sample 198 iter 40 grad_norm=5.0667e-01, inner_loss=6.5470e+00\n",
      "[eval] sample 199 iter 0 grad_norm=9.2026e-01, inner_loss=7.0669e+00\n",
      "[eval] sample 199 iter 10 grad_norm=8.7209e-01, inner_loss=6.9864e+00\n",
      "[eval] sample 199 iter 20 grad_norm=8.2713e-01, inner_loss=6.9140e+00\n",
      "[eval] sample 199 iter 30 grad_norm=7.8514e-01, inner_loss=6.8489e+00\n",
      "[eval] sample 199 iter 40 grad_norm=7.4589e-01, inner_loss=6.7901e+00\n",
      "\n",
      "--- Classification Performance (Evaluation) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        17\n",
      "           1       1.00      0.79      0.88        28\n",
      "           2       0.64      0.88      0.74        16\n",
      "           3       0.50      0.88      0.64        16\n",
      "           4       0.67      1.00      0.80        28\n",
      "           5       0.92      0.55      0.69        20\n",
      "           6       1.00      0.70      0.82        20\n",
      "           7       0.94      0.62      0.75        24\n",
      "           8       0.50      0.80      0.62        10\n",
      "           9       0.83      0.48      0.61        21\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.80      0.76      0.75       200\n",
      "weighted avg       0.83      0.76      0.76       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukta\\AppData\\Local\\Temp\\ipykernel_19036\\4281910160.py:438: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"\\n[HOAG] Sample {sample_count}: label={int(label.cpu().numpy())}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HOAG] Sample 601: label=8\n",
      "[HOAG] theta now: [-4.857665   -0.01696531]\n",
      "\n",
      "[HOAG] Sample 602: label=4\n",
      "[HOAG] theta now: [-4.857758   -0.01686367]\n",
      "\n",
      "[HOAG] Sample 603: label=2\n",
      "[HOAG] theta now: [-4.857986   -0.01659759]\n",
      "\n",
      "[HOAG] Sample 604: label=5\n",
      "[HOAG] theta now: [-4.859836   -0.01480141]\n",
      "\n",
      "[HOAG] Sample 605: label=7\n",
      "[HOAG] theta now: [-4.860453 -0.014207]\n",
      "\n",
      "[HOAG] Sample 606: label=1\n",
      "[HOAG] theta now: [-4.8655925  -0.00924613]\n",
      "\n",
      "[HOAG] Sample 607: label=9\n",
      "[HOAG] theta now: [-4.8646135  -0.01021503]\n",
      "\n",
      "[HOAG] Sample 608: label=7\n",
      "[HOAG] theta now: [-4.8645167  -0.00931553]\n",
      "\n",
      "[HOAG] Sample 609: label=8\n",
      "[HOAG] theta now: [-4.8666735  -0.00717965]\n",
      "\n",
      "[HOAG] Sample 610: label=6\n",
      "[HOAG] theta now: [-4.8700795e+00 -3.7579699e-03]\n",
      "\n",
      "[HOAG] Sample 611: label=7\n",
      "[HOAG] theta now: [-4.8732657e+00 -6.8776077e-04]\n",
      "\n",
      "[HOAG] Sample 612: label=0\n",
      "[HOAG] theta now: [-4.8761134e+00  2.1830846e-03]\n",
      "\n",
      "[HOAG] Sample 613: label=6\n",
      "[HOAG] theta now: [-4.8740954e+00  2.9776862e-04]\n",
      "\n",
      "[HOAG] Sample 614: label=8\n",
      "[HOAG] theta now: [-4.8739491e+00  1.1377466e-05]\n",
      "\n",
      "[HOAG] Sample 615: label=3\n",
      "[HOAG] theta now: [-4.873655e+00 -8.689166e-04]\n",
      "\n",
      "[HOAG] Sample 616: label=8\n",
      "[HOAG] theta now: [-4.875131e+00  5.723596e-04]\n",
      "\n",
      "[HOAG] Sample 617: label=2\n",
      "[HOAG] theta now: [-4.8760381e+00  1.4512059e-03]\n",
      "\n",
      "[HOAG] Sample 618: label=6\n",
      "[HOAG] theta now: [-4.8791871e+00  4.3542553e-03]\n",
      "\n",
      "[HOAG] Sample 619: label=9\n",
      "[HOAG] theta now: [-4.8806787   0.00571946]\n",
      "\n",
      "[HOAG] Sample 620: label=2\n",
      "[HOAG] theta now: [-4.880759    0.00580263]\n",
      "\n",
      "[HOAG] Sample 621: label=8\n",
      "[HOAG] theta now: [-4.882826    0.00785221]\n",
      "\n",
      "[HOAG] Sample 622: label=4\n",
      "[HOAG] theta now: [-4.8834395   0.00845511]\n",
      "\n",
      "[HOAG] Sample 623: label=3\n",
      "[HOAG] theta now: [-4.88419     0.00914764]\n",
      "\n",
      "[HOAG] Sample 624: label=3\n",
      "[HOAG] theta now: [-4.8855      0.01046005]\n",
      "\n",
      "[HOAG] Sample 625: label=9\n",
      "[HOAG] theta now: [-4.887508    0.01238959]\n",
      "\n",
      "[HOAG] Sample 626: label=3\n",
      "[HOAG] theta now: [-4.888513    0.01335003]\n",
      "\n",
      "[HOAG] Sample 627: label=1\n",
      "[HOAG] theta now: [-4.89329     0.01791889]\n",
      "\n",
      "[HOAG] Sample 628: label=1\n",
      "[HOAG] theta now: [-4.8979607   0.02245204]\n",
      "\n",
      "[HOAG] Sample 629: label=1\n",
      "[HOAG] theta now: [-4.9002976   0.02467552]\n",
      "\n",
      "[HOAG] Sample 630: label=8\n",
      "[HOAG] theta now: [-4.899789    0.02393319]\n",
      "\n",
      "[HOAG] Sample 631: label=5\n",
      "[HOAG] theta now: [-4.9023204   0.02632746]\n",
      "\n",
      "[HOAG] Sample 632: label=3\n",
      "[HOAG] theta now: [-4.90269     0.02669783]\n",
      "\n",
      "[HOAG] Sample 633: label=9\n",
      "[HOAG] theta now: [-4.904263    0.02827724]\n",
      "\n",
      "[HOAG] Sample 634: label=0\n",
      "[HOAG] theta now: [-4.905473    0.02945107]\n",
      "\n",
      "[HOAG] Sample 635: label=8\n",
      "[HOAG] theta now: [-4.906693    0.03057874]\n",
      "\n",
      "[HOAG] Sample 636: label=9\n",
      "[HOAG] theta now: [-4.9089255   0.03278048]\n",
      "\n",
      "[HOAG] Sample 637: label=8\n",
      "[HOAG] theta now: [-4.9092717   0.03313994]\n",
      "\n",
      "[HOAG] Sample 638: label=9\n",
      "[HOAG] theta now: [-4.911673    0.03556224]\n",
      "\n",
      "[HOAG] Sample 639: label=8\n",
      "[HOAG] theta now: [-4.913343    0.03717504]\n",
      "\n",
      "[HOAG] Sample 640: label=4\n",
      "[HOAG] theta now: [-4.9123154   0.03617529]\n",
      "\n",
      "[HOAG] Sample 641: label=3\n",
      "[HOAG] theta now: [-4.915313    0.03900344]\n",
      "\n",
      "[HOAG] Sample 642: label=5\n",
      "[HOAG] theta now: [-4.9173193   0.04093343]\n",
      "\n",
      "[HOAG] Sample 643: label=8\n",
      "[HOAG] theta now: [-4.9186554   0.04218966]\n",
      "\n",
      "[HOAG] Sample 644: label=4\n",
      "[HOAG] theta now: [-4.9192476   0.04287422]\n",
      "\n",
      "[HOAG] Sample 645: label=3\n",
      "[HOAG] theta now: [-4.920121    0.04373547]\n",
      "\n",
      "[HOAG] Sample 646: label=0\n",
      "[HOAG] theta now: [-4.920368    0.04396112]\n",
      "\n",
      "[HOAG] Sample 647: label=3\n",
      "[HOAG] theta now: [-4.91869     0.04174209]\n",
      "\n",
      "[HOAG] Sample 648: label=5\n",
      "[HOAG] theta now: [-4.917378    0.04046946]\n",
      "\n",
      "[HOAG] Sample 649: label=0\n",
      "[HOAG] theta now: [-4.9190664   0.04213166]\n",
      "\n",
      "[HOAG] Sample 650: label=1\n",
      "[HOAG] theta now: [-4.9206223  0.0438666]\n",
      "\n",
      "[HOAG] Sample 651: label=4\n",
      "[HOAG] theta now: [-4.921748    0.04478951]\n",
      "\n",
      "[HOAG] Sample 652: label=0\n",
      "[HOAG] theta now: [-4.923299    0.04629963]\n",
      "\n",
      "[HOAG] Sample 653: label=1\n",
      "[HOAG] theta now: [-4.926325    0.04912027]\n",
      "\n",
      "[HOAG] Sample 654: label=5\n",
      "[HOAG] theta now: [-4.928363    0.05120094]\n",
      "\n",
      "[HOAG] Sample 655: label=7\n",
      "[HOAG] theta now: [-4.9295053   0.05226496]\n",
      "\n",
      "[HOAG] Sample 656: label=9\n",
      "[HOAG] theta now: [-4.9308405  0.0535853]\n",
      "\n",
      "[HOAG] Sample 657: label=8\n",
      "[HOAG] theta now: [-4.932813    0.05552111]\n",
      "\n",
      "[HOAG] Sample 658: label=2\n",
      "[HOAG] theta now: [-4.932866    0.05558148]\n",
      "\n",
      "[HOAG] Sample 659: label=5\n",
      "[HOAG] theta now: [-4.9333043   0.05604026]\n",
      "\n",
      "[HOAG] Sample 660: label=3\n",
      "[HOAG] theta now: [-4.931585    0.05421015]\n",
      "\n",
      "[HOAG] Sample 661: label=1\n",
      "[HOAG] theta now: [-4.9328403   0.05542624]\n",
      "\n",
      "[HOAG] Sample 662: label=0\n",
      "[HOAG] theta now: [-4.9332423   0.05580931]\n",
      "\n",
      "[HOAG] Sample 663: label=0\n",
      "[HOAG] theta now: [-4.9341636   0.05670531]\n",
      "\n",
      "[HOAG] Sample 664: label=4\n",
      "[HOAG] theta now: [-4.9347234  0.0571683]\n",
      "\n",
      "[HOAG] Sample 665: label=7\n",
      "[HOAG] theta now: [-4.936286    0.05835071]\n",
      "\n",
      "[HOAG] Sample 666: label=0\n",
      "[HOAG] theta now: [-4.9365234  0.0585818]\n",
      "\n",
      "[HOAG] Sample 667: label=0\n",
      "[HOAG] theta now: [-4.938302    0.06039777]\n",
      "\n",
      "[HOAG] Sample 668: label=1\n",
      "[HOAG] theta now: [-4.940783    0.06281003]\n",
      "\n",
      "[HOAG] Sample 669: label=2\n",
      "[HOAG] theta now: [-4.941006   0.0630535]\n",
      "\n",
      "[HOAG] Sample 670: label=7\n",
      "[HOAG] theta now: [-4.9411592   0.06320994]\n",
      "\n",
      "[HOAG] Sample 671: label=3\n",
      "[HOAG] theta now: [-4.94257     0.06459218]\n",
      "\n",
      "[HOAG] Sample 672: label=6\n",
      "[HOAG] theta now: [-4.946296    0.06829912]\n",
      "\n",
      "[HOAG] Sample 673: label=2\n",
      "[HOAG] theta now: [-4.94791     0.06959258]\n",
      "\n",
      "[HOAG] Sample 674: label=5\n",
      "[HOAG] theta now: [-4.947226    0.06884994]\n",
      "\n",
      "[HOAG] Sample 675: label=3\n",
      "[HOAG] theta now: [-4.948716    0.07035161]\n",
      "\n",
      "[HOAG] Sample 676: label=8\n",
      "[HOAG] theta now: [-4.9500666  0.0716692]\n",
      "\n",
      "[HOAG] Sample 677: label=4\n",
      "[HOAG] theta now: [-4.9501805   0.07179664]\n",
      "\n",
      "[HOAG] Sample 678: label=0\n",
      "[HOAG] theta now: [-4.950342    0.07196218]\n",
      "\n",
      "[HOAG] Sample 679: label=4\n",
      "[HOAG] theta now: [-4.951737    0.07333902]\n",
      "\n",
      "[HOAG] Sample 680: label=9\n",
      "[HOAG] theta now: [-4.9528546   0.07451233]\n",
      "\n",
      "[HOAG] Sample 681: label=5\n",
      "[HOAG] theta now: [-4.9531283   0.07468085]\n",
      "\n",
      "[HOAG] Sample 682: label=0\n",
      "[HOAG] theta now: [-4.956327    0.07781153]\n",
      "\n",
      "[HOAG] Sample 683: label=4\n",
      "[HOAG] theta now: [-4.9568667   0.07835539]\n",
      "\n",
      "[HOAG] Sample 684: label=0\n",
      "[HOAG] theta now: [-4.958493    0.07992868]\n",
      "\n",
      "[HOAG] Sample 685: label=8\n",
      "[HOAG] theta now: [-4.9592905   0.08074021]\n",
      "\n",
      "[HOAG] Sample 686: label=0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import kornia\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PRETRAIN_EPOCHS = 15\n",
    "PRETRAIN_BATCH = 256\n",
    "PRETRAIN_LR = 1e-3\n",
    "\n",
    "NUM_EPOCHS = 2               \n",
    "SUBSET_SIZE = 500            \n",
    "BATCH_SIZE = 1                 \n",
    "HOAG_N_OUTER_STEPS = 5     \n",
    "HOAG_MAX_INNER_STEPS = 100    \n",
    "INNER_LR = 0.01\n",
    "OUTER_LR = 0.02\n",
    "CG_MAX_ITERS = 60\n",
    "CG_TOL = 1e-5\n",
    "\n",
    "GRAD_CLIP_VALUE = 2.0\n",
    "THETA_CLAMP = (-10.0, 10.0)    \n",
    "EPSILON_MIN = 1e-6\n",
    "\n",
    "\n",
    "THETA_INIT = torch.tensor([0.0, -3.0], device=DEVICE)  \n",
    "\n",
    "EVAL_MAX_SAMPLES = 500  \n",
    "\n",
    "\n",
    "def is_finite(t):\n",
    "    return torch.isfinite(t).all().item()\n",
    "\n",
    "def clamp_theta(theta):\n",
    "    return torch.clamp(theta, THETA_CLAMP[0], THETA_CLAMP[1])\n",
    "\n",
    "raw_k = kornia.filters.get_gaussian_kernel2d((7, 7), (1.5, 1.5))\n",
    "kernel = torch.as_tensor(raw_k, dtype=torch.float32, device=DEVICE)\n",
    "while kernel.dim() > 2:\n",
    "    kernel = kernel.squeeze(0)\n",
    "kernel = kernel / kernel.sum()\n",
    "BLUR_KERNEL = kernel.unsqueeze(0).unsqueeze(0).to(DEVICE)  # (1,1,kH,kW)\n",
    "print(\"BLUR_KERNEL shape:\", BLUR_KERNEL.shape)\n",
    "\n",
    "class BlurredMNIST(Dataset):\n",
    "    def __init__(self, root, train=True, blur_sigma=1.5):\n",
    "        self.mnist = datasets.MNIST(root=root, train=train, download=True, transform=transforms.ToTensor())\n",
    "        self.blur_sigma = blur_sigma\n",
    "\n",
    "    def __len__(self): return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, label = self.mnist[idx]  # x: [1,H,W]\n",
    "        y_np = gaussian_filter(x.squeeze(0).numpy(), sigma=self.blur_sigma).astype(np.float32)\n",
    "        y = torch.from_numpy(y_np).unsqueeze(0)\n",
    "        # return: blurry input y, sharp image x, label\n",
    "        return y.to(DEVICE), x.to(DEVICE), torch.tensor(label, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(1, 48, 3, padding=1)\n",
    "        self.c2 = nn.Conv2d(48, 96, 3, padding=1)\n",
    "        self.c3 = nn.Conv2d(96, 192, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(192 * 3 * 3, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.c1(x)))\n",
    "        x = self.pool(F.relu(self.c2(x)))\n",
    "        x = self.pool(F.relu(self.c3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def pretrain_classifier(classifier, pretrain_loader, epochs=PRETRAIN_EPOCHS, lr=PRETRAIN_LR):\n",
    "    classifier = classifier.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    classifier.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        for imgs, labels in pretrain_loader:\n",
    "            imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = classifier(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), GRAD_CLIP_VALUE)\n",
    "            optimizer.step()\n",
    "            running_loss += float(loss.item()) * imgs.size(0)\n",
    "            count += imgs.size(0)\n",
    "        scheduler.step()\n",
    "        avg_loss = running_loss / max(1, count)\n",
    "        print(f\"[Pretrain] Epoch {epoch+1}/{epochs} avg_loss={avg_loss:.5f} lr={scheduler.get_last_lr()[0]:.5f}\")\n",
    "    classifier.eval()\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def total_variation_regularizer(x, theta):\n",
    "    theta_clamped = clamp_theta(theta)\n",
    "    scale = torch.exp(theta_clamped[0])\n",
    "    eps = (torch.exp(theta_clamped[1])**2) + EPSILON_MIN\n",
    "    grad = kornia.filters.spatial_gradient(x, mode='diff')  # (B,C,2,H,W)\n",
    "    gh = grad[:, :, 0, :, :]; gw = grad[:, :, 1, :, :]\n",
    "    magsq = gh**2 + gw**2 + eps\n",
    "    tv = torch.sum(torch.sqrt(magsq))\n",
    "    reg = scale * tv\n",
    "    return reg\n",
    "\n",
    "\n",
    "def inner_loss_func(x, theta, fixed_params, y):\n",
    "    # x: [B,1,H,W], y: [B,1,H,W]\n",
    "    kH, kW = BLUR_KERNEL.shape[-2], BLUR_KERNEL.shape[-1]\n",
    "    pred = F.conv2d(x, BLUR_KERNEL, padding=(kH//2, kW//2))\n",
    "    recon = 0.5 * torch.sum((y - pred)**2)\n",
    "    reg = total_variation_regularizer(x, theta)\n",
    "    loss = recon + reg\n",
    "    return loss\n",
    "\n",
    "\n",
    "def outer_loss_func(x_hat, fixed_params, val_data):\n",
    "    classifier = fixed_params['classifier']\n",
    "    classifier.eval()\n",
    "    d_label = val_data\n",
    "    logits = classifier(x_hat) \n",
    "    if d_label.dim() == 0:\n",
    "        d_label = d_label.unsqueeze(0)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(logits, d_label)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def hessian_vector_product(h_scalar, w, v):\n",
    "    grad_h = autograd.grad(h_scalar, w, create_graph=True, retain_graph=True)[0]\n",
    "    grad_h_v = torch.dot(grad_h.flatten(), v.flatten())\n",
    "    hvp = autograd.grad(grad_h_v, w, retain_graph=True)[0]\n",
    "    return hvp\n",
    "\n",
    "def conjugate_gradient(A_func, b, max_iters=CG_MAX_ITERS, tol=CG_TOL):\n",
    "    x = torch.zeros_like(b)\n",
    "    r = b.clone()\n",
    "    p = r.clone()\n",
    "    rsold = torch.dot(r.flatten(), r.flatten())\n",
    "    if rsold == 0:\n",
    "        return x\n",
    "    for i in range(max_iters):\n",
    "        Ap = A_func(p)\n",
    "        denom = torch.dot(p.flatten(), Ap.flatten()) + 1e-12\n",
    "        if not torch.isfinite(denom) or denom == 0:\n",
    "            break\n",
    "        alpha = rsold / denom\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        rsnew = torch.dot(r.flatten(), r.flatten())\n",
    "        if torch.sqrt(rsnew) < tol:\n",
    "            break\n",
    "        p = r + (rsnew/rsold) * p\n",
    "        rsold = rsnew\n",
    "    return x\n",
    "\n",
    "\n",
    "def hoag_optimize_theta(inner_loss_func, outer_loss_func, w_init, theta_init, fixed_params, y_train, d_val,\n",
    "                        n_outer_steps=HOAG_N_OUTER_STEPS, inner_lr=INNER_LR, outer_lr=OUTER_LR,\n",
    "                        max_inner_steps=HOAG_MAX_INNER_STEPS):\n",
    "\n",
    "    theta = theta_init.detach().clone().requires_grad_(True)\n",
    "    w = w_init.detach().clone().requires_grad_(True)\n",
    "\n",
    "    history = []\n",
    "    for outer_k in range(n_outer_steps):\n",
    "        w_k = w.detach().clone().requires_grad_(True)\n",
    "        for inner_i in range(max_inner_steps):\n",
    "            inner_loss = inner_loss_func(w_k, theta, fixed_params, y_train)\n",
    "            grad_w = autograd.grad(inner_loss, w_k, create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                gnorm = float(torch.norm(grad_w))\n",
    "                if not math.isfinite(gnorm) or gnorm > 1e8:\n",
    "                    print(f\"[HOAG] inner grad abnormal (norm={gnorm}), stopping inner.\")\n",
    "                    break\n",
    "                w_k -= inner_lr * grad_w\n",
    "            w_k = w_k.detach().requires_grad_(True)\n",
    "            # early stop\n",
    "            if gnorm < 1e-4:\n",
    "                break\n",
    "\n",
    "        w = w_k.detach().requires_grad_(True)\n",
    "\n",
    "        outer_loss = outer_loss_func(w, fixed_params, d_val)\n",
    "        b = autograd.grad(outer_loss, w, retain_graph=True)[0]\n",
    "\n",
    "        # Solve H q = b using CG (H from inner_loss)\n",
    "        w_cg = w.detach().requires_grad_(True)\n",
    "        def A_hvp(v):\n",
    "            theta_cg = theta.detach()\n",
    "            h = inner_loss_func(w_cg, theta_cg, fixed_params, y_train)\n",
    "            return hessian_vector_product(h, w_cg, v)\n",
    "        q = conjugate_gradient(A_hvp, b, max_iters=CG_MAX_ITERS, tol=CG_TOL)\n",
    "\n",
    "        w_clean = w.detach().requires_grad_(True)\n",
    "        theta_clean = theta.detach().requires_grad_(True)\n",
    "        inner_scalar = inner_loss_func(w_clean, theta_clean, fixed_params, y_train)\n",
    "        grad_w_h = autograd.grad(inner_scalar, w_clean, create_graph=True)[0]  \n",
    "        cross = autograd.grad(grad_w_h, theta_clean, grad_outputs=q, retain_graph=True)[0]\n",
    "\n",
    "        hypergrad = -cross \n",
    "\n",
    "        if not is_finite_tensor(hypergrad := hypergrad):\n",
    "            print(\"[HOAG] hypergrad non-finite; skipping theta update\")\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                theta = theta - outer_lr * hypergrad\n",
    "                theta = theta.detach()\n",
    "                theta = torch.clamp(theta, THETA_CLAMP[0], THETA_CLAMP[1]).detach().requires_grad_(True)\n",
    "\n",
    "        w = w.detach().requires_grad_(True)\n",
    "        history.append({\n",
    "            'outer_iter': outer_k + 1,\n",
    "            'theta': theta.detach().cpu().numpy().tolist(),\n",
    "            'outer_loss': float(outer_loss.detach().cpu().item())\n",
    "        })\n",
    "    return theta.detach(), w.detach(), history\n",
    "\n",
    "def is_finite_tensor(t):\n",
    "    return isinstance(t, torch.Tensor) and torch.isfinite(t).all().item()\n",
    "\n",
    "\n",
    "def evaluate(theta_vec, fixed_params, test_loader, n_classes=10, max_samples=500, inner_steps=50, inner_lr=0.01):\n",
    "  \n",
    "    classifier_net = fixed_params['classifier'].to(DEVICE)\n",
    "    classifier_net.eval()\n",
    "\n",
    "    theta_eval = theta_vec.clone().detach()\n",
    "    theta_eval = torch.clamp(theta_eval, min=THETA_CLAMP[0], max=THETA_CLAMP[1]).to(DEVICE)\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (y_data, x_true, d_target) in enumerate(test_loader):\n",
    "        if batch_idx >= max_samples:\n",
    "            break\n",
    "\n",
    "        w_hat = y_data.clone().detach().to(DEVICE)\n",
    "        for it in range(inner_steps):\n",
    "            w_hat = w_hat.detach().requires_grad_(True)\n",
    "\n",
    "            inner_loss = inner_loss_func(w_hat, theta_eval, fixed_params, y_data)\n",
    "            grad_w = torch.autograd.grad(inner_loss, w_hat, create_graph=False)[0]\n",
    "            if it % 10 == 0:\n",
    "                try:\n",
    "                    gnorm = float(torch.norm(grad_w).cpu().item())\n",
    "                except Exception:\n",
    "                    gnorm = float('nan')\n",
    "                print(f\"[eval] sample {batch_idx} iter {it} grad_norm={gnorm:.4e}, inner_loss={float(inner_loss.cpu().item()):.4e}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if not torch.isfinite(grad_w).all():\n",
    "                    print(f\"[eval] Warning: non-finite grad at sample {batch_idx}, iter {it}; breaking inner loop.\")\n",
    "                    break\n",
    "                w_hat = (w_hat - inner_lr * grad_w).detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = classifier_net(w_hat)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_labels.extend(d_target.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    print(\"\\n Classification Performance (Evaluation)\")\n",
    "    try:\n",
    "        report = classification_report(all_labels, all_preds, zero_division=0)\n",
    "        print(report)\n",
    "    except Exception as e:\n",
    "        print(\"Error computing classification report:\", e)\n",
    "\n",
    "    try:\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        cm_df = pd.DataFrame(cm, index=range(n_classes), columns=range(n_classes))\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Error plotting confusion matrix:\", e)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Datasets\n",
    "    full_train_sharp = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transforms.ToTensor())\n",
    "    \n",
    "    if SUBSET_SIZE is not None and isinstance(SUBSET_SIZE, int):\n",
    "        idx = torch.randperm(len(full_train_sharp))[:SUBSET_SIZE]\n",
    "        sharp_pretrain = Subset(full_train_sharp, idx)\n",
    "    else:\n",
    "        sharp_pretrain = full_train_sharp\n",
    "\n",
    "    pretrain_loader = DataLoader(sharp_pretrain, batch_size=PRETRAIN_BATCH, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    blurred_train = BlurredMNIST(DATA_DIR, train=True, blur_sigma=1.5)\n",
    "    if SUBSET_SIZE is not None and isinstance(SUBSET_SIZE, int):\n",
    "        blurred_train = Subset(blurred_train, idx)\n",
    "    train_loader = DataLoader(blurred_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    blurred_test = BlurredMNIST(DATA_DIR, train=False, blur_sigma=1.5)\n",
    "    test_loader = DataLoader(blurred_test, batch_size=1, shuffle=False)\n",
    "\n",
    "    classifier = Classifier(n_classes=10).to(DEVICE)\n",
    "    classifier = pretrain_classifier(classifier, pretrain_loader, epochs=PRETRAIN_EPOCHS, lr=PRETRAIN_LR)\n",
    "    classifier.eval()\n",
    "    fixed_params = {'classifier': classifier, 'phi': [p.clone().detach() for p in classifier.parameters()]}\n",
    "\n",
    "    theta = THETA_INIT.clone().to(DEVICE).detach().requires_grad_(True)\n",
    "    print(\"Initial theta:\", theta.detach().cpu().numpy())\n",
    "\n",
    "    sample_count = 0\n",
    "    best_theta = theta.clone().detach()\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\" HOAG epoch {epoch+1}/{NUM_EPOCHS} \")\n",
    "        for batch_idx, (y, x_sharp, label) in enumerate(train_loader):\n",
    "            sample_count += 1\n",
    "            print(f\"\\n[HOAG] Sample {sample_count}: label={int(label.cpu().numpy())}\")\n",
    "\n",
    "            w_init = y.clone().detach().requires_grad_(True)\n",
    "            theta, w_final, hist = hoag_optimize_theta(\n",
    "                inner_loss_func=inner_loss_func,\n",
    "                outer_loss_func=outer_loss_func,\n",
    "                w_init=w_init,\n",
    "                theta_init=theta,\n",
    "                fixed_params=fixed_params,\n",
    "                y_train=y,\n",
    "                d_val=label,\n",
    "                n_outer_steps=HOAG_N_OUTER_STEPS,\n",
    "                inner_lr=INNER_LR,\n",
    "                outer_lr=OUTER_LR,\n",
    "                max_inner_steps=HOAG_MAX_INNER_STEPS\n",
    "            )\n",
    "\n",
    "            print(\"[HOAG] theta now:\", theta.detach().cpu().numpy())\n",
    "\n",
    "            if sample_count % 200 == 0:\n",
    "                ckpt = {\n",
    "                    'theta': theta.detach().cpu().numpy().tolist(),\n",
    "                    'classifier_state': [p.clone().detach().cpu() for p in fixed_params['phi']]\n",
    "                }\n",
    "                torch.save(ckpt, os.path.join(CHECKPOINT_DIR, f\"ckpt_sample{sample_count}.pt\"))\n",
    "    \n",
    "            if sample_count % 200 == 0:\n",
    "                print(\"[HOAG] quick eval after sample\", sample_count)\n",
    "                evaluate(theta, fixed_params, test_loader, max_samples=200)\n",
    "\n",
    "    #evaluation\n",
    "    print(\"\\n Final evaluation \")\n",
    "    evaluate(theta, fixed_params, test_loader, max_samples=EVAL_MAX_SAMPLES)\n",
    "    print(\"Final theta:\", theta.detach().cpu().numpy())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
